radiant textuality
literature after the world wide web

Jerome mcgann

,

palgrave

*

RADIANT TEXTUALITY: LITERATURE AFTER THE WORLD WIDE WEB

© Jerome McGann, 2001
All rights reserved. No part of this book may be used or reproduced in any manner whatsoever without written permission except in the case of brief quotations embodied in critical articles or reviews. First published 2001 by PALGRAVETM 175 Fifth Avenue, New York, N.Y.10010 and Houndmills, Basingstoke, Hampshire RG21 6XS. Companies and representatives throughout the world

PALGRAVE is the new global publishing imprint of St. Martin's Press LLC Scholarly and Reference Division and Palgrave Publishers Ltd (formerly Macmillan Press Ltd). ISBN 0-312-29352-6 hardback Library of Congress Cataloging-in-Publication Data McGann, Jerome J. Radiant textuality : literature after the World Wide Web / Jerome McGann. p.cm. Includes bibliographical references and index. ISBN 0-312-29352-6 1. Criticism-Data processing. 2. Criticism, Textual-Data processing. 3. Hypertext systems. I. Tide. PN98.E4 M39 2001 801'.959'0285-dc21 A catalogue record for this book is available from the British Library. Design by Westchester Book Composition First edition: November 2001 10 9 8 7 6 5 4 3 2 1 Printed in the United States of America.

2001021795

There is a crack in every thing, That's how the lightgets in. -Leonard Cohen, UAnthem" (1993)

Contents

Acknowledgements Preface Note 0" tile Text
Introduction. Beginning Again: Humanities and Digital Culture, 1993-2000
Part I. Hideous Progeny, Rough Beasts: 1993-1995 1. The Alice Fallacy; or, Only God Can Make a Tree 2. The Rationale of Hypertext 3. Editing as a Theoretical Pursuit Appendix to Chapter 3 Part H. Imagining What You Don't Know: 1995-1999 4. Deformance and Interpretation (with Lisa Samuels) Appendix to Chapter 4 5. Rethinking Textuality Part HI. Quantum Poetics: 1999-2000 6. Visible and Invisible Books in N-Dimensional Space Appendix to Chapter 6 7. Dialogue and Interpretation at the Interface of Man and Machine

ix xi
xvi

29 53 75 88

105 131 137

167 187 193

viii

Contents
209 232

Conclusion. Beginning Again and Again: "The Ivanhoe Game" Appendix to the Conclusion

Notes Bibliography

249 259

Preface

Knowledge of the world means dissolving the solidity of the world. -Italo Calvino, Six Memos for the Next Millenium
n one sense the story running through this book is a very old story. We sometimes see it as the story of Faust and Margaret, and it comes again as Beauty and the Beast or as any of that wondrous fairy tale's mutations. A hundred years ago Henry Adams recognized its emergence in a historical tension he named the Dynamo and the Virgin. The Computer and the Book-their relation has much in common with those three legends. For the book was once upon a time the very emblem of Faustian power. As late as 1870 Emily Dickinson could think that "There is no Frigate like a book." The thought charms us now precisely in its quaintness, since current imaginative voyagings are everywhere traversing digital space. And so bibliographical lamentations begin to arise, "au sont les livres d'antan?" This book is a commentary on that question, and the commentary is organized around two ideas about humanities-based digital instruments. The first is that understanding the structure of digital space requires a disciplined aesthetic intelligence. Because our most developed models for that kind of intelligence are textual models, we would be foolish indeed not to study those models in the closest possible ways. Our minds think in textual codes. Because the most advanced forms of textual codings are what we call "poetical:' the study and application of digital codings summons us to new investigations into our textual inheritance. To date that summons has been slow to develop, which brings me to the second idea that organizes this book. Digital technology used by humani-

I

xii

Preface

ties scholars has focused almost exclusively on methods of sorting, accessing, and disseminating large bodies of materials, and on certain specialized problems in computational stylistics and linguistics. In this respect the work rarely engages those questions about interpretation and self-aware reflection that are the central concerns for most humanities scholars and educators. Digital technology has remained instrumental in serving the technical and precritical occupations of librarians and archivists and editors. But the general field of humanities education and scholarship will not take the use of digital technology seriollsly until one demonstrates how its tools improve the ways we , explore and explain aesthetic works-until, tl,at is, they expand our interpretational procedures. A close relation holds between the book and computer. For textual and digital forms alike, however, this historical continuity has brought questions and problems that have not been studied at all well precisely because the genetic relation between the two media has been too much taken for granted, as if it were simple to see and understand. The situation is emblemized in the dichotomy of enthusiasm and skepticism that marks so much of the current discussion-indeed, that organizes the discussion along two sides. We have to break away from questions like "will the computer replace the book?" So much more interesting are the intellectual opportunities that open at a revelatory historical moment such as we are passing through. These opportunities come with special privileges for certain key disciplines-now, for engineering, for the sciences, for certain areas of philosophy (studies in logic), and the social sciences (cognitive modeling). But unapparent as it may at first seem, scholarship devoted to aesthetic materials has never been more needed than at this historical moment. That necessity leaped to one's attention in 1993 with the coming of the World Wide Web (W3). Until that epochal moment, digital technology had moved at the margins of literary and humanistic studies. The tools were taken up largely by some linguists and form-critical scholars, and by specialists interested in problems of storing and archiving scholarly (textual) materials. Even word-processing tools came slowly into the hands of humanities scholars. We forget that ten years ago-I am writing this sentence in late February 200o-the number of humanities scholars who used any computerized tools at all was relatively small. A discontinuous historical event occurred during those ten years, and in the course of its unfolding emerged W3, the digital environment that organizes and commands the subjects of this book. To the speed and ubiquity of digital intercourse and transaction have been added interface and mul-

Preface

xiii

timedia, and that, as the poet said, "has made all the difference." Our sense of language will never be the same. Or rather, perhaps, our sense of it-in every sense-has been renewed, restored to something like the richness that it possessed in the Middle Ages, and that is still available in the works descending to us from that remarkable period-pre-eminently in its greatest invention, the medieval church and cathedral. From Santa Sophia to St. Mark's to Monreale, and across all of Europe and England, the doors of human perception were flung open in those amazing multimedia environments. And not only in Europe. Scattered across the globe from China to New Guinea to Egypt to the Nazca desert in Peru are the remains of human inventions of similar and even more amazing complexity. Next to them, even our most recent and advanced virtual reality tools and constructions seem primitive indeed. However toddling they appear, contemporary instruments of hyperand multimedia constitute a profane resurrection of those once-sacred models of communication. To get a clear grasp of their historical emergence one would have to return to the middle and late nineteenth century, when so much of what is apparent today was being forecast: in mathematics and physics, in logic, in the emergence of photography. My own special field of interest, textuality, underwent a great renewal at the same moment. [n England, the work of John Ruskin, D. G. Rossetti, and William Morris catalyzed a complex set of historical forces into the Arts and Crafts movement and, more particularly, into the Renaissance of the Book. In the rediscovered "Grotesque" art of the Middle Ages was heard-the metaphor is deliberately mixed-the first premonition of the famous proverb that would define the coming of the digital age a century later: the medium is the message. This book is a report on some early attempts to understand how that proverb might be read by people interested in humanities education. It is based in certain ideas about language and semiotic systems that recur throughout history-ideas that may seem not to match with many common formulations. In my view, however. the problem here lies in the formulations, not in the actual fact of the matter (so to speak). Recall that even before we began creating formal systems of visual signs--systems that generate this very sentence-object you are now reading-the language we use is woven from audible and visible elements. And as the syntax of that last sentence is designed to suggest, this textual condition of ours is constructed as a play of incommensurable elements, of which temporality is one. Linguistic units are not self-identical, as even the briefest reflective glance at a dictionary will show. Indeed, they don't even

XlV

Preface

occupy fIxed positions within a given textual space-the specialized space of this reading-text, for example--since a variety of overlapping and incommensurable planes transact all textual spaces. Textual space and textual time are n-dimensional simply because they locate embodied actions and events. Computational systems are not designed like the first sentence of the previous paragraph. They are designed to negotiate disambiguated, fully commensurable signifying structures. "Indeed! And so why should machines of that kind hold any positive interest for humanities scholars, whose attention is always focused on human ambiguities and incommensurables?" "Indeed! But why not also ask: How shall these machines be made to operate in a world that functions through such ambiguities and inc ommensurables?" Both of those questions have set the terms for the work of this book. Anyone who works with texts in disciplined ways, and especially those interested in their rhetorical and aesthetic properties, understands very well the incommensurability of textual forms. How to gain some clarity and control over our textual condition has been a perpetual human concern and is a central concern of this book as well. It is organized to show how the work at the University of Virginia's Institute for Advanced Technology in the Humanities (lATH) from 1993 to 2000 led to the practical implementation of catastrophe and quantum models for the critical investigation of aesthetic forms. Suggestive as the ideas of quantum mechanics have been for many humanities scholars, the scale of quantum effects has seemed far removed from the apparent scale of textual and semiotic phenomena. The latter involve macroscopic events, the former submicroscopicindeed, quantum effects are, in the view of many, not objective events at all but simply types of measurements and calculations executed for certain practical ends. It was Roger Penrose, I think, who fIrst argued most effectively against this view. He proposes that "the phenomenon of consciousness is something that cannot be understood in entirely classical terms u and that "a quantum world [might] be required so that thinking, perceiving creatures, such as ourselves, can be constructed from its substance" (Penrose 226). The empirical data of consciousness are texts and semiotic phenomena of all types-"autopoetic u phenomena, in the terms of Humberto Maturana and Francesco Varela. This book will argue that our" classical models for investigating such data are less precise than they might be and that quantum dynamical models should be imagined and can be built. The
U

Preface

xv

book focuses on the historical circumstances that forced this argument into being. It traces the development of certain experiments with textual materials to their unforeseen but, I would now say, necessary consequences: most importantly, the practical illustrations and proposals for new models of critical and interpretational study. One final comment may be helpful. This book's commitment to a "quantum poetics" may call to mind, for Modernist scholars at any rate, Daniel Albright's stimulating and elegant study of certain strains of twentieth-century writing, Quantum Poetics (1997). Albright's book investigates "the appropriation of scientific metaphors by poets" (1) whose work emerged at the same time as the great figures of early-twentieth-century science. Albright argues that these writers exploited certain scientific figures in their imaginative work. My argument is quite different: that quantum and topological models of analysis are applicable to imaginative writing tout court, that these models are more adequate, more comprehensive, and more enlightening than the traditional models we inherit from Plato and Aristotle to Kant and Marx. "Quantum poetics" in this study does not signify certain figures and tropes that stimulated the practices of a certain group of historically located writers. On the contrary, it comprises a set of critical methods and procedures that are meant to be pursued and then applied in a general way to the study of imaginative work. The final discussion of "The Ivanhoe Game" illustrates the difference very clearly. "The Ivanhoe Game" models a new form of critical method. Its applicability is of a general kind-as much for Yeats and Pound as for Keats and Byron, for Shakespeare or Dante, for Ovid, Lucretius, the Bible. It is a model that we propose to build in a new kind of textual environment-a digital one. Finally, it is only a model-one model. We propose to build it in the hope that it may stimulate others to develop and build more adequate critical tools.

Note on the Text

he idea for this book took shape in 1998 to 2000, when most of it was written. The five chapters comprising Parts I and II were drafted earlier, however. Three of those were written between 1993 and 1995 as a related series of critical reflections on The Rossetti Archive and its initial theoretical goals. The two very different chapters tided "The Alice Fallacy" and "Deformance and Interpretation" were written in 1993 and 1996 (respectively). Framing the other three conceptually as well as historically, they define the interpretational issues that had been running through the work we undertook with The Rossetti Archive. The full elaboration of this book's arguments only emerged very late, however, when those interpretational topics and problems had been rehearsed and pursued. This happened in 1998 to 2000 when the rest of the book was written. At that point-the spring of 2000, when "The Ivanhoe Game" was conceived in conversations with Johanna Drucker and John Unsworth-I saw how important "The Alice Fallacy" had been for the development of the arguments being made in this book. I then revised and recast the chapters originally written from 1993 to 1995, and I wrote the introduction, "Beginning Again," as well as the series of critical reflections that introduce the different parts of the book. I played several iterations of "The Ivanhoe Game" with Johanna Drucker and some graduate and undergraduate students (May to November 2000) and finished the book by writing up several accounts of those events, including the last chapter of this book, "Beginning Again and Again."

T

Introduction

Beginning Again: Humanities and Digital Culture, 1993-2000

we're like the man who climbed on a chair and declared he was a little closer to the moon. -Hubert Dreyfus, What Computers Can't Do
umanities computing is beginning again. It passed through one coherent period of historical development and, more recently, through an exploratory interlude of considerable importance. This book examines that interlude, the years 1993 to 2000, from the perspective of a project I undertook at exactly the same moment (fortuitously as it happened): The Complete Writings and Pictures of Dante Gabriel Rossetti. A Hypermedia Research Archive ("The Rossetti Archive"}.1 Working on the archive during those years, I began to see more clearly the kinds of change that are coming to literary and humanistic studies. These changes will bring to the center of scholarly procedures theoretical models that have been perceived until now as odd, idiosyncratic, nonnormal. Before 1993 the computerized future of our humanistic inheritance was apparent to a relatively small group of librarians and archival scholars and to very few other people in literary and cultural studies. I am speaking here not of loose and speculative cybernetic conceptions and imaginings, which have been widespread for some 15 years or so, but of practical and concrete understandings of the momentous changes that lay in store for our libraries and other archival depositories. Now, however, in 2000, the community of humanities scholars at large has also begun to see that future with greater clarity and to feel the pressure of its demands. In 1993 when projects like The Rossetti Archive sought funding for their work, the applications failed. Then the scholarly community was not prepared to

H

2

Radiant Textuality

judge either the need or the adequacy of such projects. The situation in 2000 is different, for many educators now understand that our inherited .....archive of materials in libraries and museums will have to be re-edited Twith information technology (IT) tools. We see as well the kind of massive reorganization that will have to be carried out in these depositories in order to store, connect, and conveniently access their holdings. All this work is already well underway. But also now in 2000 some are being pushed further by the inertia of the new tools being placed at our disposal. Ideas about textuality that were once taken as speculative or even imaginary now appear to be the only ones that have any practical relation to the digital environments we occupy every day. So that now all of aesthetic, literary, and humane studies appear brinked for major changes in the ways they will be studied, analyzed, and interpreted. 2 This book tells a story of how we got to where we are now. The story describes how certain theoretical views of textuality once considered weird, impractical, and unserious discovered their moment of realization in the digital world of the late twentieth century. I first taught the works of Lautreamont,Jarry, and Roussel at the University of Chicago in the late 1960s and early 1970s in courses I then called "The Literature of Excess:' Authors of their kind treated documents as scenes of predse imaginative possibilities. None approached their work in the spirit of a romantic hermeneutics: that is to say, under a horizon where multiple meanings are generated by readers working in and through texts imagined on an analogy with.the Bible. Those kinds of text appear to us as massively authoritative and deeply mysterious, requiring devotional study to uncover their secret meanings. Reconnecting with certain performative and rhetorical traditions, however, writers like larry laid a groundwork for post-romantic procedural writing. They began to make clear once again the constructed character of textuality-the fact that texts and documents are fields open to decisive and rule-governed manipulations. In this view of the matter, texts and documents are not primarily understood as containers or even vehicles of meaning. Rather, they are sets of instantiated rules and algorithms for generating and controlling themselves and for constructing further sets of transmissional possibilities. 3 How I came to write that previous paragraph constitutes the story being told in this book.
Points of Departure

In the fall of 1993 we began work on The Rossetti Archive and in July 2000 saw the public release of its first research installment: an online hyperme-

Beginning Again: Humanities and Digital Culture, 1993-2000

3

dia construction of some 10,000 image and text fIles organized for use (and experiment) by students and scholars with many disciplinary interests. (When the archive is completed it will contain more than twice that number of files.) During those initial seven years the archive-along with the whole field of humanities computing--was swept in directions no one foresaw in 1992. The project was consciously begun as a pragmatically-based theoretical undertaking-in fact, an experiment in Ian Hacking's sense4-to explore the nature of textuality: in particular, book and paper-based textualities, as well as the editorial methods for marking and interpreting these kinds of texts. The archive was built under the auspices of the University of Virginia's Institute for Advanced Technology in the Humanities (lATH), which was founded the same year as The Rossetti Archive was begun. The humanities computing work sponsored by IATH-a large array of research projects in texts, media, images, and information-can now be seen to mark the end of a fIrst and distinct phase in the history of humanities computing. Our experience in building The Rossetti Archive is an epitome of what happened at lATH between 1993 and 1999, when humanities computing began to move in very new directions.5 A brief historical note here will be helpful. The use of IT in humanities disciplines began in the late 1940s with Father Roberto Busa SJ, whose work on the corpus of St. Thomas Aquinas set the terms in which humanities computing would operate successfully for more than 40 years. 6 Two lines of work dominate the period: first, the creation of databases of humanities materials-almost exclusively textual materials-for various types of automated retrieval, search, and analysis; second, the design and construction of statistical models for studying language formalities of many kinds, ranging from social and historical linguistics to the study of literary forms. Viewed from the perspective of a humanities scholar's interests, this work has had its greatest impact on the library, which began its extraordinary digital reconstitution during this period.? Because the library locates the center, if not the very soul, of arts and humanities studies, this transformation carries enormous consequences for humanities students and educators. Hand catalogues have virtually disappeared and libraries everywhere are offering larger, more varied, and more integrated bodies of electronically organized and connected materials. While everyone is directly affected by these changes in the library at the general access level, and to an increasing degree in the area of reference works of different kinds, few people, including very few humanities schol-

4

Radiant Textuality

ars, have been touched by more specialized work with stylometrics, cladistics, and tools for automated collation and author-attribution. This situation is especially clear in the United States, where New Criticism and its theoretical aftermath exiled nearly all kinds of statistical, editorial, and textual work to the periphery of humanities studies. Because humanities computing in its first phase was so closely linked to computationallinguistics, on one hand, and to textual/editorial studies, on the other, the central lines of work in literary and cultural studies between 1950 and t 990 remained virtually untouched by developments in humanities computing. To the degree that IT attracted the attention of humanities scholars, the interest was largely theoretical, engaging the subjects of media and culture in either speculative and relatively abstract ways or journalistic treatments. That situation has kept most humanities scholars in a state of invincible ignorance of one of the most remarkable achievements of this early phase of humanities computing: the design and development of systems for the structural description (or "marking") of textual materials. The parent of these developments is SGML (Standard Generalized Markup Language), which is a rigorously articulated logic for marking the structural parts and relations of textual documents (or bodies of material fashioned on a model of textual documents). So far as the humanities are concerned, the signal event was the development of TEl (Text Encoding Initiative). TEl is a specialized markup derivative of SGML, one designed to facilitate computer implementations of traditional humanities texts (literature, history, philosophy). By 1993, when lATH was founded, TEl was establishing itself as a professional standard for text encoding of humanities materials. 8 These dates and events are important because of what happened in the larger world of IT between 1993 and 1994: the definitive appearance of the W3. 9 It is important to remember--not an easy thing to do at this distance-that the coming of W3 seemed to most scholars involved in humanities computing at that time as a trivial event so far as they were concerned. A hypermedia environment established on a global scale, W3 ought to have fed immediately into a number of long-standing theoretical interests in decentered and reader-oriented textualities. The scholarly meetings and journals devoted to humanities computing show with unmistakable clarity, however, that few people in those communities registered the importance of W3. Disinterest was perhaps to be expected from computational scholars, but even the hypertext community barely noticed this truly epochal event before 1995. 10 Hypertext was a playground clearly founded by the enthusiastic descendants of those earlier twentieth-century move-

Beginning Again: Humanities and Digital Culture, 1993-2000

5

ments called New Bibliography and New Criticism. While those children played around in their hypertextual fields, "serious" humanities computing remained located in library and archival technology, on computational analyses of various kinds, and on the closely related fields of textual editing and textual markup. And meanwhile W3 arrived on its own. The period from 1993 to 1999 gains its peculiar shape and significance largely because of the crisis W3 brought to humanities computing. Critical discussion of hypertext and hypermedia explodes throughout cultural and literary studies, with interest now fueled in practical ways by various persons, including scholars like Jay Bolter and George Landow, who launched online hypermedia constructions of many kinds. Before W3, anyone interested in building computerized humanities tools or environments would have had to learn at least elementary programming. W3 ended that situation by making HTML (Hypertext Markup Language) the language of W3 documents. Developed by Tim Berners-Lee, HTML was a brilliantly simplified subset of SGML, whose basic rules could be mastered in a few hours. As a consequence, W3 quickly burgeoned, with people throughout the world putting up terabytes of web pages and documents of all kinds. Into that expanding universe of textuality moved a small army of literary students and scholars to create an array of sites designed for various scholarly and pedagogical audiences. Nearly all of these materials were viewed with varying degrees of skepticism or scorn by "the humanities computing community." And with good enough reason since that community had been trying for decades to develop rigorous analytic tools within a traditional milieu of work controlled by careful standards and peer review. These new materials, by contrast, usually appeared from nowhere, the brainchildren of some spontaneous overflow of powerful feeling in a particular person here or there, even a particular scholar. Idiosyncrasy ruled the World Wild West, including its humanities subset. W3 encouraged people to make and send forth digital things on their own initiative and in their own ways. The upside of these events was the coming of a large and diverse population of new people into digital fields previously occupied by small and tightly connected groups. More significantly, they came to build things with digital tools rather than simply to reflect abstractly on the new technologies. This general situation was replicated in humanities disciplines at large. In addition, because W3 was from the beginning of its public life strongly visual rather than textual in character, humanist scholars and students brought a multidisciplinary and multimedia set of interests to the sites they were building and visiting.

6

Radiant Textuality
Humanities Comp"ting at the University of Virginia: 1992-1993

Insofar as humanities computing existed at the University of Virginia in 1993, it was located at the periphery-in an initiative taken by librarians at the Alderman Library to found an Electronic Text Center. Because few faculty were involved in this initiative, and none direcdy, it began as a speculative institutional venture-a kind of bet made by the library that the center would attract interest and use by the faculty. This center, which flourishes today, was to be an instrument for creating and disseminating electronic texts of various kinds and in many disciplines for the use of students and scholars in class and in research work. The center began its work at a minimal level, all but invisibly to the campus at large, in 1992. It is now the largest disseminator of online humanities texts in the world. Later that same year IBM approached UVA's computer science (CS) department with an offer of $1 million in equipment for educational use over a three-year period. Two CS faculty members, Alan Batson and Bill Wulf, contacted two humanities professors, Ed Ayers and myself, to see if IBM's offer might be useful to people in the arts and sciences division of the university. A small committee was formed of dlese four people plus Kendon Stubbs (the Associate Librarian and chief architect of the library's Electronic Text, or E-Text, Center) and two other CS people. Out of that committee was formed what would become the Institute for Advanced Technology in the Humanities (lATH). Because lATH came into being fortuitously, its shape and focus evolved through a randomized state of affairs. To see this let me reset the scene at UVA in late 1992:
(1) The library's E-Text Center was begun as an independent initiative. (2) The grant from mM had not been sought by the library or by anyone in humanities. (3) Ed Ayers and myself were only casually acquainted and neither of us knew, before the establishment of the committee that created lATH, that we each had some interests in humanities computing. (4) The CS faculty who initiated the committee did not have in mind any clear plan for what to do with the IBM offer, nor did they have any close (let alone working) relations with Ed Ayers or myself. When I joined the committee I knew no one on it other than Ed Ayers. (5) Kendon Stubbs joined the committee only after it was initially formed, and at my suggestion (because I had learned from him about the recent founding of the E-Text Center).

Beginning Again: Humanities and Digital Ciliture, 1993-2000

7

I give that list to emphasize the relatively atomized state of affairs when the committee was formed. That loose situation would prove an asset, for it ensured that the committee didn't begin its work in the context of a coherent institutional history or a strong set of prevenient ideas about humanities computing.

The Idea of LATH
The question to be answered by the committee was this: "What should be done with IBM's offer?" Ed Ayers and myself were invited to join the committee because the CS faculty, to whom IBM had made the offer, thought the equipment might be put to best use in the arts and sciences division rather than in the engineering school. As a result of this remarkable act of intramural collegiality (and imagination), this CS-run committee was charging itself only in relation to humanities educational needs. The object was to use the IBM offer to initiate a major change in humanities education at UVA. The overwhelming initial answer to the central question was that the equipment should be made available as soon as possible to all arts and sciences departments for as long as possible. One person, Alan Batson, held out against that position. He argued that to move in this way would be to replicate a known history of 30 years of failure. A genuine engagement between humanities education and computer technology would not get beyond word processing if this model were adopted, Batson argued: "Throwing IT resources at people who have no special interest in them or desire to exploit them doesn't work. We know this because whenever we've done it during the past 30 years the results have been minimal at best." (Those are not exactly his words, but as Thucydides said of his History's reported conversations, I'm giving the substance of what he said.) Batson's model was different: to seek out projects with demonstrable intellectual importance for humanities scholarship and to fund those projects as completely as possible with the technical resources the projects need. His rationale: "Educational change at the level of the university is driven by the active research work of the faculty. Changes in pedagogy and classroom dynamics follow from research." After an intense meeting in which Batson held his position against the rest of the committee, his view prevailed. Further meetings refined and modified Batson's general model. The idea of lATH thus became formulated in the following set of charges:

8

Radiant Textuality
(1) Each year offer fellowships to UVA faculty who submit humanities research proposals to lATH. These should not be proposals for IT teaching initiatives but for scholarly research projects that use IT tools. Successful applicants become fellows of lATH for one year. They are given a one-year release from teaching plus complete technical support for their projects. (2) Try to ensure a diverse, interdisciplinary set of research fellows (rather than a set of closely related projects). (3) Require that the department of the successful fellowship applicants contribute materially to the fellow's work-specifically, by supplying the fellow with one or two graduate students to work on the research project and helping, if possible, with securing release time from teaching.

Two important ideas organize this plan for lATH. First of all, the plan assumes Batson's view that the educational work of a university is driven by its research activities. This idea does not imply that pedagogy is a secondary or less important university function-quite the contrary. But in a university environment students have to expect that their courses and classrooms will be organized in terms of the most up-to-date and adventurous scholarly work-work generated from research agendas that establish the standards and touchstones for a field. In the ideal university setting, a dynamic relation operates between the scholars' research work and the classrooms where it is tested, explored, and modified. Thus one of our key expectations in founding lATH was that its research projects would become gravity centers drawing the attention of other faculty and the interest and work of students. The graduate assistants of the research fellows, it was believed, would themselves become gravity centers affecting other graduate students and undergraduates. In this way IT resources would begin to be exploited in all of the university's educational activities, in the instructional and in the research work of faculty and students alike. Second, the plan for lATH assumed that IT tools would only be taken up by humanities faculty who had an active interest in using these tools in their primary areas of scholarly work. Simply giving equipment to faculty and offering technical support would have a minimal effect, as the dismal history of such efforts in the past has demonstrated. A steep learning curve defines the shape of one's involvement with these tools. Learning to use them is in one respect not unlike learning a new language. You may gain a certain minimal competence fairly quickly, but if your goal is more ambi-

Beginning Again: Humanities and Digital Culture, 1993-2000

9

tious-in this case, to exploit these tools for advanced research work-a deep and long-term investment is required. The problem with developing serious work in humanities computing is complicated by two additional factors. IT tools are in such a volatile stage of development that to use them well one has to remain vigilant about the current state of a wide range of technical resources. This takes time, real effort, and, perhaps most of all, a collective environment. Given the institutional structure of higher education, indexed by the tenure system and its measures of scholarly work, scholars--even tenured scholars-may reasonably conclude that their interests are not served by these tools. It is a fact that right now one can function most efficiendy as a university scholar and teacher by working within the paper-based system we inherit. (This moment, this "now," is quickly passing away.) In face of such a situation, lATH was founded as a resource for people who had already made a commitment to humanities computing, a commitment defined practically by an actual project with demonstrable scholarly importance. There were to be no outright gifts in the arrangement. Everyone involved in a fellow's appointment to lATH would have to make some material commitment to the work. The hope, the goal, of this plan was a transformation of humanities education at UVA. It didn't take five years before we knew that we had succeeded far beyond what we had expected or even, speaking for myself, what we had imagined as possible. In five years the two initial research projects proliferated into more than two dozen. These included projects begun by graduate students as well as regular faculty and library staff. Faculty from more and more university departments became lATH fellows and enriched the institute's work: projects in music, art history, linguistics, architecture, urban planning, religion, archaeology, and so forth. Important work being done by scholars outside UVA gravitated to lATH because of its resources and lively intellectual scene. The hope that the institute's research orientation would catalyze important pedagogical initiatives was also realized. After a few years lATH moved to support certain teaching-oriented initiatives that were driven by serious research agendas. In addition, the institute worked hard to help its fellows exploit the classroom potential of its research projects-a potential that extended well beyond the university to include K12 education as well. As a consequence of all this activity, in 1995 the university established its Teaching Technology Initiative (TTl), a program organized to provide IT resources and technical help to faculty and

10

Radiant Textuality

teaching staff. Similar resources were being made available through the library's E-Text Center. A crucial factor in UVA's involvement with humanities computing was the close liaison that was fostered from the start between lATH and the library. Nothing illustrates the depth of that liaison more than the library's decision to clear out more than 2,000 square feet of its floor space to make room for lATH's faculty and staff offices. This close working relationship expanded the university's research activities in remarkable and innovative ways. Some of the most important theoretical work in humanities scholarship is now being undertaken by faculty, graduate students, and library staff working in collaborative groups. Finally, the remarkable success of lATH resulted in major part because its work from the outset was consciously developed in relation to W3. When John Unsworth was appointed as director in 1993, his first move was to ensure that the institute's projects were designed for web dissemination. Pursuing that direction in 1993 was to move against nearly every current in humanities computing scholarship, which was dominated by "standalone" ideas and technologies (epitomized in the early and short-sighted choice of CD-ROM as the venue for carrying humanities texts and hypertexts). In this situation we see once again the cultural influence of book paradigms on the new digital environments. Or, one should rather say, a certain view of books and book culture-a view defined, as I've already noted, by ideas drawn from New Bibliography and New Criticism. The convergence in 1993 of digital technology and W3 changed the shape of things, and not least the shape of humanities computing.
The Rossetti Archive and the Theory of Scholarly Editing

Under Unsworth's direction, then, the institute's work shifted in various ways from its initial conception and charges. Projects conceived by nonUVA scholars were invited to come to the institute if they brought their own funding, and certain interesting pedagogical projects were taken on board after several years. Most significandy, Unsworth invited important IT projects, especially web-based projects, to locate themselves, or instances of themselves, on the lATH server. On the technical side, a major challenge for the institute and its fellows was to pursue long-term, large-scale humanities computing research projects with an almost ascetic rejection of the surface effects and short-term gains offered by proprietary software and proprietary data standards. In an apparendy paradoxical way, lATH's W3 commitment drove its projects to

Beginning Again: Humanities and Digital Culture, 1993-2000

11

make rigorous logical design a fundamental goal. This pursuit reflected a dedication to portability and the abstraction that enables it-even if it also entailed doing without good tools for creating or disseminating the scholarly work in the short run. As it happened, that commitment was to induce a profound shift in the principal focus and goals of The Rossetti Archive-moving it, in fact, from an editorial project per se to a machine for ing the nature of textuality in more general and theoretical ways. The character of the two initial lATH projects-The Rossetti Archive and Ayers's frillley of the Shadow-would exert a continuing influence on the direction of lATH's work in general. Both projects operated with large datasets of textual and visual materials. In addition, the texts in these projects were often handled both as alphanumeric data and as digital images. The image-based approach to the data was especially marked in The Rossetti Archive because, of course, Rossetti was not only a painter and visual artist; he was a poet who wrote under a horizon of book design and book illustration. But what precisely was involved in The Rossetti Archive's image-based approach to its materials? I can pose this question now because the hindsight of seven years has exposed how loosely and unselfconsciously we undertook our work with digital images. To unpack the import of that question is to begin exposing all the issues and problems that are the subjects of this book. The Rossetti Archive was conceived within the context of a technological tradition that stretches across more than two millennia. I speak of the period when scroll, book, and other textual instruments were developed as tools for communication, information storage, and critical reflection. Perhaps the most sophisticated of these machines were the ones invented and refined by so-called textual scholars: text machines-the best known being the book-for preserving and studying forms of cultural memory, including texts themselves. The Rossetti Archive was undertaken as a practical effort to design a model for scholarly editing that would have wide applicability and that would synthesize the functions of the two chief models for such works: the critical edition (for analyzing the historical relations of a complex set of descendant texts with a view toward locating accumulated linguistic error); and the facsimile edition (a rigorously faithful reproduction of a particular text, usually a rare work, for scholarly access and study). The purpose of marrying these two kinds of scholarly instruments was based in a theory of textuality that was seriously underdeveloped 20 years ago. The theory holds two positions: first, that the apparitions of text-its paratexts, bibliographical codes, and all visual features-are as important in the

12

Radiant Textuality

text's signifying programs as the linguistic elements; second, that the social intercourse of texts-the context of their relations-must be conceived an essential part of the "text itself" if one means to gain an adequate critical grasp of the textual situation. 11 That view of texts and the textual condition explains why the initial conception of The Rossetti Archive took shape well before we began our actual work on the project. In fact it came around 1983, when I was teaching at California Institute of Technology. That year I published A Critique of Modern Textual Criticism, which was the first in a series of works aimed at dislocating certain theories of textuality that dominated scholars' conceptions of their two principal disciplinary tasks: textual editing and textual interpretation. That same year I was introduced to UNIX computing systems and to hypermedia. With the convergence of these twain I knew that when circumstances were right I would undertake building a computerized hypermedia model for scholarly editing. Building the archive would articulate a powerful argument for the view of textuality I wanted to promote. The chance arrived when lATH arrived. We spent the year from 1992 to 1993 theorizing the methodology of the project and designing its logical structure. Then in 1993 we built the first small demonstration model of The Rossetti Archive, which at that time I described in the following general terms:
Like the work of Blake, Burns, and other important artists and writers, Dante Gabriel Rossetti's work is difficult to access or to edit for access. Expressive forms that work in or with visual and auditional materials do not lend themselves to the paper-based formats of traditional scholarship. Under such conditions, a more flexible medium is required. TI,e Rossetti Archive has been developed in response to this situation. The scholarly models it builds have a particular applicability to artists and writers who seek to exploit and explore the expressive potential of more than one medium. We have been especially interested in developing critical tools for studying visual materials, as well as textual materials with a significant "visible" component. Concentrating on the linguistic codes of textualities, readers and even scholars regularly give scant attention to the physique of texts. But all texts deploy a more or less complex series of bibliographical codes, and page design-if not page ornament and graphic illustration-is a rich scene of textual expression. Computerized tools that deploy hypermedia networks and digitization have the means to study visual materials and the visibilities of language in ways that have not been possible before. This archive was built to harness

Beginning Again: Humanities and Digital Culture, 1993-2000
those capabilities, and Dante Gabriel Rossetti was chosen because the diversity of his work puts the goals of such a project to a serious test. Rossetti's work was executed in two different media, visual and textual, and his work in each is intimately-and often explicitly-interconnected. The relations are clearest, perhaps, in those works where he made pictures for poems or other texts he had already written-like "The Blessed Damozel"-or in works where he made texts to accompany or comment upon pictures he had executed-for example, the sonnets he wrote as extensions of the meaning of his first important painting, Tire Girlhood of Mary Virgin. That basic complexity in Rossetti's work gets deepened and elaborated because of the centrality of Rossetti's work in recovering the poetic culture of the "Early Italian Poets" of the twelfth to the fourteenth centuries. The connections between Rossetti's so-called original work, both written and pictorial, and his translations of Dante and his circle are pervasive. Finally, Rossetti's work habits were such that these structural complexities of his art and writing get vastly extended. Rossetti was an obsessive reviser of his written work, and these revisions were carried out at every level of the writing: He worked and reworked words, phrases, passages, and he rearranged "finished" units into dizzying sets of variant organizational units. The difficulties come into sharp relief as soon as one considers any of Rossetti's works: say, the 1870 Poems; or "The House of Life," which was a subunit in that volume; or the introductory "Sonnet" to "The House of Life," which first appeared as part of the sonnet-sequence of "The House of Life" only in 1881. Rossetti followed the same kind of revisionary habits when he was painting and drawing. All these features of Rossetti's work pose a complex and hitherto unsolvable editorial problem. One cannot properly study or appreciate Rossetti's work without having access to all of it. Even an introductory selection presents serious difficulties, because one needs to combine two media together and one also needs to present the materials so that the complex relations of all the parts are preserved. One easily understands why Rossetti's work has never been comprehensively edited and why the separate parts of his work are themselves available for general study only in the most limited ways. Virginia Surtees's standard catalogue of The Paintings and Drawings of Dante Gabriel Rossetti is excellent but quite incomplete. And to this day the "standard" edition of the writings is the 1911 The H1Jrks of Dante Gabriel Rossetti, edited by Rossetti's brother, William Michael. The writings have never been critically edited. As a result. their nervous structural features can only be encountered in scholarly periodicals and monographs.

13

14

Radiant Textuality
A hypermedia computerized environment allows one to overleap these problems, which are a function of editing that has to be carried out in the framework of the book. In The Rossetti Archive all the works are available for study in facsimiles of their original documentary forms. This means that the user has access to all his original manuscripts, printed texts, drawings, designs, and paintings. Since Rossetti designed his own books, one can appreciate the importance of reading his work in its original documentary states. And since the archive preserves these original materials in full color facsimile as necessary, one can see the great advance computerization makes in this case over Surtees's catalogue (which reproduces Rossetti's images in black and white). Furthermore, computerization allows the editor to connect all of Rossetti's documents to each other so that their relationships can be examined and better understood. Finally, these authorial materials are embedded in a context of related documents, historical and critical, that help to illuminate the primary materials as well as their cultural context. The archive has incorporated, for example, various contemporary materials that are important for understanding Rossetti, pre-Raphaelitism, and the world in which they emerged and developed. The archive has included The Germ, William Michael Rossetti's early biography of his brother, as well as William Michael's 1911 edition of the works, H. C. Marillier's and Frederick Stephens's commentaries on the art, and other crucial contemporary documents (Swinburne, Buchanan, Pater, etc.). Also included is a large corpus of the photographs by which Rossetti's work was disseminated. As the archive is further developed, this body of material will be expanded. It is all marked for full electronic search and analysis. It is also supplemented by the present editor's critical essays, notes, and commentaries. The latter, of course, draw upon the considerable corpus of scholarship and criticism that has evolved over the past century on Rossetti, his circle, and their general historical milieu. (http://jefferson.village. virginia.edu/rossetti/introduction.html)

One can easily see, from this later vantage, how well that description reflects the state of humanities computing in 1993, when the TEl implementation of SGML markup was beginning to take serious hold, when hypermedia models were gaining widespread attention, and when W3 was scattering text and image constructions of many kinds across the globe.

Beginning Again: Humanities and Digital Culture, 1993-2000

15

Beginning Again
Because The Rossetti Archive was conceived and pursued, early on, as much as a kind of thought experiment in the theory of texts as an editorial project per se, it kept a constant focus on reflexive attention. I refer not only to the standard and highly pragmatic critical processes that regulate the design and building of any kind of tool or instrument. Of course we were constantly constructing the archive, testing what we had implemented, modifying what we had done, and then rescaling the level of implementation. Beyond those critical operations, however, the archive held our interest as a theoretical instrument for investigating the nature of textuality as such. This inertia in the project broke out as a series of related texts. I wrote these reflexive pieces between 1993 and 2000 as expositions and critiques of our work, and all but one, the dialogue on "The Alice Fallacy," were originally published as online research reports. Writing the first set of these pieces between 1993 and 1996 brought a new level of clarity to what we were doing; and one of these, "Imagining What You Don't Know: The Theoretical Goals of The Rossetti Archive:' marked a turning point in the project as a whole. It argues that to make anything is also to make a speculative foray into a concealed but wished for unknown. The thing made is not the achievement of one's desire; it is the shadow of that desire, the sign of what the poet spoke of as "something longed for, never seen." Writing that essay ushered the project of the archive to a new level of operation. It also initiated the project of this book: that is to say, the decision to draft careful written records of the critical stages in the making of the archive. The work of those writings has been recomposed into the parts of this book, which is organized around a double vanishing point. In one perspective appears a set of related but independent explorations into the characteristics of different kinds of textualities. In another, one follows a kind of metanarrative or critical history projecting a map of future scholarly operations. We begin to explore that relatively unentered territory in the final section of the book, where the project of The Rossetti Archive mutates into an entirely different set of critical and scholarly demands. These demands arise naturally-this is now clear, as it was not clear in 1993-from the way in which the project was first conceived. As a model constructed to reflect on its own process of development, the archive proved acutely sensitive in two directions at the same time: to changes taking place in the encompassing field of digital media, and to the traditional

16

Radiant Textuality

needs of humanities scholars working out of paper-based models of textuality. Because W3 browsers had just become available, John Unsworth urged me to build the first model of the archive in HTML for web dissemination. Doing that was essentially an act of handicraft, for in 1993 we were primarily involved in discussions about how to design an SGML structure for all of The Rossetti Archive's materials, visual as well as textual. The latter was to be a complex logical structure-in contrast to the HTML-marked demonstration model. One wants to hold this initial situation clearly in mind, for the contradiction between the web demo model, a simple visual interface built in HTML, and the archive itself, a set of logical relations and determinants conceived in SGML, would surface repeatedly in all our work. Briefly, then: As we built the archive we kept encountering variations on a pair of difficulties. Both are functions of the special character of '/J:umanities materials, which are not primarily informational materials. They are made for reflective and imaginative purposes-in Rossetti's case, textual and visual works made for such purposes. Hence came our recurrent set of difficulties. First, neither the SGML markup structure nor the lhypermedia design were able to integrate the textual and visual materials 'beyond elementary connecting, sorting, and gathering operations. 12 Second, the archive's principal objects of study-Rossetti's works-were not being interpretively exposed by the computational tools in very interesting ways. Computerization made much more information (and much more varied information) available-vast amounts of data in forms, relational as well as facsimile, that were previously unimaginable. As a tool for rethinking these materials, however, whether through structured or randomized searches of the data, the computer continually disappointed the high hopes it had raised. The archive includes a great deal of critical and reflexive materials in itself, but these materials are simply linked to the primary materials in an elementary, if also elaborate and complex, hypertext organization. Nothing illustrates the practico-theoretical weakness of this situation more dramatically than the brave new world of hyperfiction. Armed as they are with remarkable technical resources, the works of this new genre pale in complexity before their paper ancestors: early works like The Metamorphoses, The Arabian Nights, The Saragossa Manuscript, or recent ones by Joyce, Riding Jackson, Borges, and of course the whole OULIPO contingent and its numerous contemporary offipring. The example of hyperfiction may well locate a temporary condition.

Beginning Again: Humanities and Digital Culture, 1993-2000

17

We have no reason to doubt-indeed, we have every reason to expectthat remarkable imaginative works will appear in digital forms. Traditional imaginative texts developed and mutated over a long period of time and in many different environments. Hyperfiction and video games are early explorations and experiments, and if they seem primitive next to analogous works we inherit through predigital traditions, the same cannot be said of digital art, which has already developed sophisticated forms. Success in this case comes, of course, because digital imaging procedures feed upon the rich fund of electronic media that has emerged over the past hundred and more years. Traditional textualities have not been in a position to such media until very recently. I bring up these matters not to pass out digital merits and demerits to people working in different areas of the arts and humanities but to locate that part of the field where we have advanced hardly at all-indeed, where we have made few serious efforts to advance. More than anything else, the making of The Rossetti Archive has exposed the gulf that stands between digital tools and media, on one hand, and the regular practices of traditional philosophy, "theory," hermeneutics, and arts/literary/cultural criticism, on the other. Digital culture is virtually (!) an obsessional topic in all these fields, but it is a topic addressed from a distance, as a kind of fascinating and/or threatening alien form. That distance gets marked with unmistakable clarity in one way: the discursive procedures in all of these fields remain to date resolutely Works like The Rossetti Archive or The or Ti,e Dickens J.1Ieb are fundamentally archival and editorial. 1 They gather, sort, and make things accessible, and they link these things to related things. Unlike works imagined and organized in bibliographical forms, however, these new textual environments have yet to develop operational structures that integrate their archiving and editorial mechanisms with their critical and reflective functions at the foundational level of their material form, that is, at the digital/computational level. Although structural coding in SGML (or XML) mitigates this deficiency to a certain degree, it is not only difficult and time-consuming to implement, but its principles and other design characteristics set permanent and unacceptable limits on its usefulness with arts and humanities materials. Thus, however primitive hyperfiction and video games may seem, we recognize their functional relation to their underlying digital processes. In this respect they are more advanced in a practico-theoretical point of view than any of the IT-based scholarly works mentioned above. This is particularly the case with video games. The difference implicitly traced in this discussion, between the

18

Radiant Textuality

scholar/ editor, on one hand, and the critic/philosopher, on the other, was once far less sharply drawn than it is in our day. In ages and circumstances when hardly any distinction pertained between works of criticism/reflection and works of art/imagination-cultural conditions that produced the Bible, The &ok of Odes, Mahabarata, and the works of Sophocles, Aeschylus, Lucretius, and Dante--the work of scholarship and learning was also much more integrated. Jerome, Augustine, Aquinas, Politian: All were figures of immense cultural authority. But then came the worlds of J. G. Eichhorn and G. W. F. Hegel and, later, Karl Lachmann and Friedrich Nietzsche. Two of those four names are forgotten except among circles of textual specialists; two maintain their cultural celebrity. That difference marks a notable shift in social and historical circumstances that has occurred during the past 200 years. Our digital culture is likely to reverse that difference. A hundred years from now, which of the following two names is likely to remain pertinent to traditions of critical thinking and which will seem merely quaint, if it is recalled at all outside pedantic circles: Vannevar Bush, Harold Bloom? For historical scholars of any kind, figures like Bloom index a serious disciplinary and cultural crisis. The digital revolution has pushed us to the brink of a great age of editorial and archival scholarship. This is plain to see--if one cares to look at all. For the past 200 years, however, the central work of cultural reflection and criticism has grown increasingly divorced from that kind of editorial scholarship. Nietzsche's critique of philology and historicist method marks the point at which the original rapprochement between what philologists called the "Lower Criticism" and the "Higher Criticism" was destroyed. In our day the authority of this Nietzschean break has greatly diminished. Modern computational tools are extremely apt to execute one of the two permanent functions of scholarly criticism-the editorial and archival function, the remembrance of things past. So great is their aptitude in this foundational area that we stand on the edge of a period that will see the complete editorial transformation of our inherited cultural archive. That event is neither a possibility nor a likelihood; it is a certainty. As it emerges around us, it exposes our need for critical tools of the same material and formal order that can execute our other permanent scholarly function: to imagine what we don't know in a disciplined and deliberated fashion. How can digital tools be made into prosthetic extensions of that demand for critical reflection? This is not a question to be addressed in speculative or conceptual terms. To count as adequate today, in this culture, responses to the question-most especially theoretical responses-require the deploy-

Begitming Again: Humanities atld Digital Culture, 1993-2000

19

ment of computational instruments. Paper-based forms like this book can now, I think, only come to assist in a process of exploration and study that will henceforth be determined by digital forms. The next generation of literary and aesthetic theorists who will most matter are people who will be at least as involved with making things as with writing text. These kinds of issues won't be usefully engaged without reconsidering certain fundamental problems of texts and textuality. The critical possibilities of digital environments require that we revisit what we know, or what we think we know, about the formal and material properties of the codex. We shall see that the advent of digital tools promotes this kind of critical reflection and leads to a view of books and of language itself that breaks with many common and widely held ideas. We shall see how, in a pragmatic as well as a theoretical perspective, the normative form of language is most usefully approached not as informational and expository but as poetic and polyvalent. Though informational and expository models of language have been taken as normative for more than three centuries, they are in fact specialized models, sophisticated derivatives. They were installed to facilitate certain instrumental tasks. We shall also see how texts deploy complex visible codes-how printed pages function both in seman tical and imagistic ways-and how the executable codes (algorithms) of computational devices have much to tell us about the functioning structures of traditional textual devices. Finally, we shall trace in these investigations the discovery of a graduated series of critical moves that were generating unapparent consequences. These mutate under different topical conditions and then get reinvested as new critical opportunities. The completed form of this essentially stochastic critical process comes in the final section of this book. At that point we layout a model for a procedure of critical thinking that calls for digital implementation. This model appears in "The Ivanhoe Game," whose origins lie in the concealed pressures that drive and sustain the more immediate reflexive goals of this book's purely textual investigations. The game, which is procedural and structured for random turns of event, will be formally described, its prehistory will be documented, and its digital existence forecast.

Chapter 5

Rethinking Textuality

There are some ':features" of the real world that are better lost in translation. Perhaps the idea of a document having a single location is one of them. -Steven Johnson, Interface Culture

A

s we have seen over and over again, complex problems emerge when you try to think about digital media through our inherited codex paradigms or vice versa. The collision of these two marking systems-for that is ultimately what they are-came up repeatedly in our conversations about one of Johanna Drucker's central concerns-what she calls "The Metalogics of the Book."l That subject shifted into useful focus when Drucker and I undertook a simple experiment with an OCR scanner. The point of the experiment was to use computer hardware to demonstrate what our thought experiments kept suggesting to us: that the rationale of a textualized document is an ordered ambivalence and that this ambivalence can be seen functioning at the document's fundamental graphic levels. By "rationale" we mean the dynamic structure of a document as it is realized in determinate (artisanal) and determinable (reflective) ways. By "ordered ambivalence" we mean the signifying differentials set in play when a rhetoric of intention is applied to a document. Textual differentials at any level are a function of the effort to control or even eliminate them. 2 The implications of this demonstration are, we believe, considerable. For our own special field of interest-the study of literary, cultural, and aesthetic works, and especially those deploying textual elements-the demonstration brings a strong argument for the following ideas:

138

Radiant Textuality

(l)a. That what we call "a text" should be understood as a document composed of both seman tical and graphical signifying parts. J b. That the heuristic distinction between bibliographic and semantic elements obscures the field of textual meaning by identifying the signifying field (its "rationale") with the semantic field. (2)a. That there is no such thing as an unmarked text, and the markup systems laid upon documents to facilitate computerized analyses are marking orders laid upon already marked up material. (Thus all texts implicidy record a cultural history of their artifactuality.) b. That while we recognize how the semantic elements of any given document encode a record of those elements' historical passage, the same is true of the text's bibliographical elements. (3)a. That marked text, a document, is interpreted text. b. That text documents, while coded bibliographically and semantically, are all marked graphically. c. That the distinction between graphic and bibliographic textual features stands as an index of the text's archaeological condition in both its semantic and its bibliographical orders. d. That marking any textual distinction-for example, between bibliographic and semantic elements-is a high order interpretive operation. e. That distinctions may be marked for attention or for inattention. (So the interpretive procedure, now prevalent, that does not "reveal the codes" of its bibliographical rationale has marked them for inattention.) (4)a. That texted documents are not containers of meaning or data but sets of rules (algorithms) for generating themselves: for discovering, organizing, and utilizing meanings and data. b. That since the demise of classical rhetorical theory and the emergence of advertising, bibliographical codes constitute a text's clearest display of its generative rules. c. That these rules-the rationale of the texted document--are necessarily ambiguous because the rules are being repeatedly reread (i.e .· executed), whether the reader is conscious of this or not. d. That the marked text, as a record of its historical passage. is ipso facto a record of its previous readings, that is to say, its generative rules. e. That the rules of marked text-the descriptive/performative protocolscan be made apparent (rendered visible) as such through another marking program. (But many of these rules, now so historically remote, will have become too obscure to recover.) (5). That a certain class of texts-poetical texts, so called--are normative for

Rethinking Textuality

139

all textual documents because their generic rationale is to maximize atten-

tion to the structure and interplay of the textual orders.

Understand that as we undertook our experiment this sequence of ideas had reached only the loosest state of articulation. Several were altogether unformulated at the beginning of our work, and at this point some are hypotheses for guiding our planned work. They emerged in a determinate form only after some months of preliminary theoretical conversations and initial experiments. A dialectical/experimental process came to clarify and unpack these inchoate ideas and direct the process of exploration. Nevertheless, I place them here so that you can follow and assess the adequacy of what we're doing and why we came to these first conclusions.

The Initial Experimental Context
The project began out of a general dissatisfaction with two approaches to textuality and text interpretation that have great authority in the community of literary and linguistic scholars. One, a recent power, gained its position with the emergence of humanities computing. The logical system of text markup developed for computers, SGML, fully represents this view. This hypergrammar treats its documentary materials as organized information, and it chooses to determine the system of organization as a hierarchy of nested elements; or, in the now well-known formulation: "text is an ordered hierarchy of content objects" (the so-called OHCO thesis). This approach to textuality only became problematic when it was undertaken and then implemented by the Text Encoding Initiative (TEl), as several of its principal advocates pointed out in 1993: "the experience of the text encoding community, as represented and codified by the TEl Guidelines, has raised difficulties for the [OHCO] thesis."4 As we know, TEl set about formulating a special subset of SGML that would be useful, in its view, for encoding cultural documents (as opposed to business and administrative documents) for computerized search and analysis. TEl is now a standard for humanities encoding practices. Because it treats the humanities corpus-typically, works of imagination-as informational structures, it ipso facto violates some of the most basic reading practices of the humanities community, scholarly as well as popular. The revulsion that many humanists express for the emergence of digital technology reflects this sense that computerized models are alien things:

140

, Radiant Textua"i!y

practice as such, then certainly alien to the received inheritance of and art. This traditional community of readers comprises the second group to which our project is critically addressed. For this group textual interpretation (as opposed to text management and organization) is the central concern. In this community of readers, the very idea of a "standard generalized markup," which is to say a standard generalized interpretation, is either problematic or preposterous. The issue hangs upon the centrality of the poetical or imaginative text for cultural scholars, though it applies equally well to students of art, history, anthropology, politics, law, and any discipline in which procedural rules of interpretation are perceived as more or less context-based, flexible, manipulable: "reader" or" community" organizeddialectical-rather than structurally fixed. But while our specific project consciously addressed this duplex audience-each at once a set of adversaries and a set of comrades-it became a practical focus only after a set of (so to speak) prehistorical events had unfolded. Here I speak only for myself, not for Johanna Drucker, who at the time (1992-1993) was working elsewhere and whom I did not know, though I knew her work pretty well. The determinate matter here is the project of The Rossetti Archive. As we have seen, this came about in late 1993, about seven years after I had first encountered computerized technology and hypermedia methodology during my tenure at Caltech. At that time I knew I would undertake a project like the archive should the chance arrive. This was a clear decision based on the idea that a hypermedia "edition" or "archive" would make it possible to study literary and aesthetic works in entirely new ways. The innovative possibilities were, in my view, not just a function of the computational resource of these new tools. Two other matters interested me more. First, digital imaging resources offered hopes that students would be able to carry out their studies in a more direct relation to primary documentary materials. This was important to me because my previous work as an editor of such materials had shown me that what traditional interpretation sought as "meaning" in a text was always deeply funded in a text's material features-its "bibliographical codes:' now so called. Second, I was struck by the mterpretlve opporfumdes released in those relational organizations fostered by computerization: hypertext and hypermedia. When we began building The Rossetti Archive in 1993, I was introduced to SGML and TEl as tools for enhancing the analytic power of the archive's resources. The usefulness of these tools became apparent fairly quickly and so with the help of some people who had a deep understand-

if not alien to

Rethinking Textuality

141

ing of logical markup forms, we created a specialized version of SGML to organize the data in the archive. Then followed seven years of practical implementations of our initial plans and ideas. These were years filled with those splendid, even ravishing enlightenments that only come when your plans and ideas are thwarted and overthrown. "In a dark time," as Theodore Roethke famously wrote, "the eye begins to see." So, as we proceeded with the practical construction of the archive we began to see the hidden fault lines of its design structure. As I've already pointed out, what began as a project to put out a certain product-an image-based design for electronic editing that would have wide applicability-bifurcated. Our initial purpose acquired a new one: to use the archive's process of construction as a laboratory for reflecting on the project itself. That second purpose led inevitably to a regular set of critical inquiries into the basic organizing ideas of the archive and its procedures. This new set of interests inevitably delayed the appearance of the archive itself-a frustrating event in certain respects but immensely fruitful in others. One result of these new interests is Drucker's "Metalogics of the Book" project. In my case the project emerged directly from a reflection on three types of problem that the building of the archive exposed. The first, which I've already noted, involved the weaknesses in "the OHCO thesis" of textuallty that we found when implementing the The second centered in the way we were handling digital images. That is to say, the archive's logical design had no means for integratmg these objects into an anarather, the failure to consider interface in lytic structure. Finally, a serious way-constituted yet a third problem. This last case was in certain respects the most interesting as well as the most surprising. For when we worked out the archive's original design, we deliberately chose to focus on the logical structure and to set aside any thought about the Interface for delivering the archive to its users. We made this decision in order to avoid committing ourselves prematurely to a delivery mechanism. The volatile character of interface software appeared so extreme that we determined to proceed in such a way that, when we were ready to deliver the work, we would have a product that could be accommodated to whatever software seemed best.
The Rewards of Failure

'-1

A great virtue of computerized tools is that they are simple. Consequently, to get them to perform their operations you have to make your instructions explicit and unambiguous. To do that means you have to be very clear

142

Radiant Textuality

in your own mind about what you're thinking, meaning, intending. The simplicity of the computer is merciless. It will expose every jot and tittle of your thought's imprecisions. What I've just said is news to no one. It is a banality. But we want to remember that time when we thought differendy about computers and about cognitive precision. The recollection is important here because it will help to clarify our project. Here is my recollection circa 1983. Introduced to UNIX multitasking and to the possibilities of digital hypermedia, I was drawn to my initial imagining of something like The Rossetti Archive-a critical and scholarly environment for studying aesthetic works in novel and hitherto impossible ways. When the chance came some ten years later actually to build such a work, I thought I had come to the Promised Land. It was Middlearth after all, I would soon realize. But in fact also a land of promise, although promising in a way I had not expected. The simplifying rigors characteristic of digital systems have not been prized by humanities scholars for a long time. They are associated with disambiguated scientific-or at least scientistic-thinking. Humanities scholars pledge their allegiance to a different kind of rigor and precision. Or so we have always said. But what is that kind of precision, precisely? Responding to that question is a primary, and in all likelihood longterm, goal of the "Metalogics of the Book" experiment. Our general goal is to study how digital tools fail to render or realize complex forms of imaginative works (the works of Rossetti, for instance). The purpose, however, is not to "correct" these "failures" but to try to understand their significance and meaning. So we're trying to use computational operations not to realize our purposes and ideas but to derealize them, as it were. Why do we want this? Because our subjects of interest are works that realize themselves not in standardized and disambiguated forms but through their active relation to such forms. This relation shows why computerization can only realize imperfecdy and imprecisely the projects qlost dear to scholars who study imaginative works. The problem does not lie "in" the computers but in the strategies of those who design them. This inevitable dysfunction, however, is no reason at all to dismiss computerization from the principal research interests of humanities scholars. On the contrary, these new tools offer an unprecedented opportunity for clarifying our thinking processes. Indeed, they license us to implement in more rigorous ways della Volpe's dialectical model of interpretation (see chapter 3). This project means to use computerized resources to clarify-to define

Rethinking Textuality

143

precisely-what we imagine we know about books and texts. Because our computer tools are models of what we imagine we know-they're built to our specifications-when they show us what they know they are reporting ourselves back to us.

The Experiment
Ask this question: "Can a computer be taught to read a poem?" The answer is "yes." TEl is a grammar that computers can understand and manipulate. When you mark up text you are ipso facto reading and interpreting it. A poetical text marked up in TEl has been subjected to a certain kind of interpretation. But of course sophisticated readers of poetical works recoil when such a model of reading is recommended to them. Poems are rich with nuances that regularly and, it seems, inevitably transcend TEl protocols. But suppose one were to step away from complex forms like poetry. Suppose one were to try to begin a computerized analysis of texted documents at a primitive level. The first move in this case would be to choose to "read" the document at a presemantic level. The focus would be on the document's graphical design, the latter being understood as a set of markup features comprising a reading of the document, that is, a set of protocols for negotiating the textual scene. The idea would be to construct an initial set of elementary text descriptors that would be fed to a computer. The computer would use these to parse the document and then deliver an output of what it read. Our hypothesis was that it would deliver multiple readings. This initial model for the experiment did not survive a series of critical interrogations. Conversations we had with Worthy Martin (one of our colleagues at lATH and a specialist in computer vision) exposed the difficulty of constraining the text descriptors so that we would get usable results. To do this was theoretically feasible but would take a great deal of mathematical analysis. These conversations brought another important realization: that the text primitives we were trying to articulate would comprise an elementary set of markup codes. And that understanding brought out a crucial further understanding about textuality in general: that all texts are marked texts. At this point let me quote Johanna's notes on our investigation as it then stood:

DM said that his idea of automated mark-up was not simply to insert tags identifying semantic or syntactic or content features, but to show that texts

144

Radiant Textuality

t

were already "marked" in their written form. This suggested to me the idea of the "reveal codes" command in a digital document, since it would make evident what is usually unacknowledged and unseen: the commands and protocols according to which the file is encoded. Though "reveal codes" was the first term that galvanized discussion, it was quickly evident that Jerry and [ came at it from two different directions and with two different, but curiously complementary agendas. Jerry saw "reveal codes" as an aspect of "deformance" and [ saw it as a first step in a "metalogics of the book." Thus we split from the outset between interpretation and analytic description. between a desire to create a demonstration of deformance as a mode of reading and an interrogation of book form and format as interface. In both instances. the point of commonality that links our project into one is the conviction that the graphic format of a text participates in the production of textual signification in ways that are generally unacknowledged. Our shared aim is to demonstrate this-DM leading us through experiments in computer misreading of graphic features and me trying to push the analysis of graphic form by developing a critical vocabulary for it. (Notes 1)

As we tried to relate and define more precisely our two lines of inquiry, it occurred to us that we might take advantage of the elementary reading operations carried out by Optical Character Recognition (OCR) programs. Even the best of these programs, as we knew, produced deformed readings of the documents they scanned. We therefore decided to see what could be discovered from the deformations generated by a good scanning program. We used Omnipage 10.0. We also decided to work with prose texts rather than with poetry. But the prose texts would be of two different kinds: first, a document with some complex display features, and second a relatively straightforward piece of prose formatted margin-to-margin in standard block form. We report here only on the first document.
We chose an advert page from the 20 August 1870 issue of the Victorian periodical The Athenaeum (page 256). We set the scanner at True Page/Greyscale. The plan was to run the page through several successive scannings, in this order: (1). An initial scanning; (2). A reprocessing (NOT a rescanning) of the document at exacdy the same settings and without moving the document; (3). A repetition of operation 1, that is, a rescanning of the document keeping all original settings and with the document unmoved; (4-6). A repetition of operations 1,2, and 3 but at a black and white setting; (7-12). A repetition of operations 1-6 except we would lift

Rethinking Textuality

145

the document and replace it in as nearly the same position as we could. We had other similar repetition/variation scans in mind as well, but before planning them we judged it best to assess the results of these first 12 operations. As it turned out, the results we obtained in the first two operations led us to modify these initial plans. We performed instead a second repetition of the initial scan and then went on to perform only a selection of the other operations. Operation 1. The first scanning pass produced the usual double output: a rough image highlighting the page sectors, and a standard output of the alphanumeric text. The scan produced a text divided into 21 zones plus an alphanumeric text with a series of misreadings and error messages. Operation 2. The reprocessing of the first scan produced a startling double result: the output this time divided the document into 20 zones and displayed slighdy different alphanumeric text. Operation 3. The res canning of the document produced yet further variances in both the sectoring and the alphanumeric output. This time 17 zones were distinguished and new variances appeared in the alphanumeric text. The results of operations 2 and 3 convinced us to repeat operation 3, which produced this time 18 sectors and new variances in the alphanumeric text. At this point we had results that were significant for our purposes, so we curtailed the rest of the planned experiment. We did two more rescannings: a scan at black and white settings, which reproduced the sectoring of operation 1 but output a new set of alphanumeric variances; and a rescanning after the document had been lifted and then replaced on the scanner. This operation yielded 22 sectors plus new alphanumeric variances. Several important consequences flowed from these experiments. First, we now possessed a powerful physical argument for a key principle of "textual deformance" and its founding premise: that no text is self-identical. 5 Whatever the physical "causes" of the variant readings, and however severely one sought to maintain the integrity of the physical operation, it appeared that variance would remain a possibility. Second, the OCR experiments showed that textual ambivalence can be located and revealed at graphical, presemantic levels. This demonstration is important if one Wishes to explore the signifying value of the bibliographical codes of a textual document. For it is a commonplace in both the SGML/TEI and the hermeneutic communities that these codes do not signify in the way that semantic codes do. Third, the experiments strongly suggested that while every text possesses, as it were, a self-parsing markup, the reading of that markup can only be executed by another parsing agent. That is to say, there can be no such

146

Radiant Textuality

thing as an "unread" text. (And while the experiments did nothing to argue for the following conviction, it remains strong with both of us: that every text" contains within itself," so to speak, a more or less obscured history of the readings/parsings, both semantic and bibliographical, that transmit the document to any immediate moment of reading/parsing.)
The Present Situation

Out of these experiments emerged the theses I set out at the beginning of this chapter, and the issues raised through the theses have set Drucker and myself on a pair of new courses. First, here are Drucker's notes for developing 4D interface design models.
Rather than visualizing the thematic/semantic contents of a text in abstract form I now want to be sure to map it into a visualized spatialization of the book. Thinking of the book as a space, one that also unfolds along the vA'emporal axis (or axes) of reading, I can envision a 4-d model of the book. In this model every graphic element is actually a structuring element. Thus, for instance, a table of contents is not a simple notation lying on a thin sheet in the front matter, but is means of dividing the sculptural form of the book into a set of discrete spaces, each demarcated in relation to that original point of spatial reference, and located relationally within the whole. This sounds terribly empirical, I know, and insofar as I am interested in describing an object in material, schematic, and logical terms, I intend for it to suggest a faith in what Worthy calls "the properties of things themselves." I would stop short of any suggestion that these are "self-identical" properties, or that a specific signification inheres in these properties, or that there might a lexicon of values attached to such properties. Instead, I suggest that as organizing schemata, these format features function as an integral portion of t e text because they function as an interface. In a several stage process, I want to make a visualized model of a book, a wire-frame image of its format features down to their specifics and particularities, and then flow the text through that so that semantically/syntactically tagged features can be displayed (why? first to see what patterns figure forth from such a demonstration, and second, to be able to morph this displayas another act of deformance). ]M suggests a second form in this model, a second "book" that would emerge as an image of the discourse of reading, the trace of intercourse of reader and text. This begins to suggest the holographic projection of my original graph of deformance as a space between discourse and reference.

Rethinking Textuality

147

Now I see that that space is in fact the space of reading-with reading defined as deformance. (Notes 7)

This new project intends to deepen the exploration of the "nature" of paper-based documents. And while my work with The Rossetti Archive "as a theoretical pursuit" has been directed almost exclusively in that direction for the past five years, 6 1 am beginning to see a need to clarify the critical possibilities of digital environments and tools at the user end. So the following questions begin to pose themselves. First: what practical difference does it make to understand documents as "difference engines"? At one time we thought (I think) that a person might usefully engage in "endless play" with "the text;' but the tediousness of such a thought is now apparent to (nearly) everyone. It is the dead-end of our 150o-year experiment with the game of silent reading. The game will probably never cease to have its charms, but it is a game now often spinning the wheels of its own conventions. (That is the "meaning" of a work like If on a Winter's Niglu a Traveller [Calvino], though I should add that since Poe the meaning has been as available as the purloined letter.) Second: what good are cybertools for elucidating these difference engines? Those two questions will be addressed in a practical way by asking two other questions: What use-functions distinguish cybertext from docutext? And (how) might any of those functions promote our appreciation of texts as difference engines?

A Brief Digression
Some of the most reliable promoters of cybertext-whether critical (like Espen Aarseth) or inspirational (like Janet Murray)-have themselves, 1 think, obscured the issues. Murray, for example, distinguishes four central properties of digital environments: two interactive properties (procedural, participatory); and two immersive properties (spatial, encyclopedic) (Murray 71-90). It can be shown, however, that none of these properties are peculiar to digital environments. They are even essential properties of the docutexts that control the way Murray thinks about digital tools. Her interest is in fictional narrative, and if one thinks seriously about such narratives one easily sees that these four properties characterize their operational status. This fact is most apparent from Murray's own book, for when she introduces her view of the digital environment, she uses a number of paper-based works she calls "Harbingers of the Holodeck." All of her harbingers are recent, but the truth is that these harbingers go far back-the

148

Radiant Textuality

I

Bible being one of the most apparent. Murray chooses recent ones in order to seduce us into thinking these environments are recent phenomena. But they aren't, as book scholars have often pointed out to cyberiots. Aarseth has proposed an elaborate taxonomy for texts in general in order to construct a distinctive set of criteria for understanding what he wants to call "cybertexts." Unlike Murray, Aarseth recognizes that books have "dynamic" functions and hence that "new [cybernetic] media do not appear in opposition to the old [paper media] but as emulators of features and functions that are already invented" (Aarseth 74). Despite this remark, however, Aarseth makes a sharp distinction between what he calls "linear" and "ergodic" texts and he locates "ordinary text"-including "hyper"ordinary texts---on the linear side of the distinction. Cybertexts, by contrast, are "ergodic" in that they have dynamic user-function(s) beyond the purely interpretive function common to all texts (Aarseth 62-67). (He distinguishes three other user functions: explorative, configurative, and textonic, the last signifying the user's ability to add permanent traversal functions to the text.) Useful as Aarseth's study is, however, he too,like Murray, misconstrues . "ordinary text" as "linear." One does not have to recall Mesoamerican quipu or any number of ideographical texts to recognize the nonlinear character of various kinds of precybertexts. 7 Every poem comprised in our inherited Western corpus could fairly be described as a nonlinear game played (largely) with linear forms and design conventions, but often with nonlinear forms as well. Nonverbal texts are useful to consider in this context because they highlight the sociohistorical nature of "linear textuality." Epitomized by documents constructed from alphanumeric characters and by a "clockwork" temporality, even the most abstract linear texts contain residues of nonlinear semiotic functions and relations. The residues appear when textual spaces are treated as maps, when algorithms of traversal are deployed (as with glosses, footnotes, and such), or when the form taken by scripts and typefaces functions rhetorically (operates beyond an abstract and transparent informational function). C. S. Peirce's turn-of-the-century effort to replace the alphanumeric text with what he called existential graphs in order to achieve a greater range and clarity of logical exposition is an extremely important event in the history of Western textuality. The graphs were an effort to develop a language for nonlinear relations. 8

J

Material Messages

Aarseth's and Murray's views about the differences between traditional and cyber textualities are common and widely accepted. That fact underscores

Rethinking Textuality

149

the need for a thoroughgoing retheorization of our ideas about books and traditional textuality in general. Since our immediate purpose is to "rethink textuality" in relation to digital resources, we will settle for something much more modest at this point. Three points are especially important. First, we want to recover a clear sense of the rhetorical character of traditional text. Rhetoric exposes what texts are doing in saying what they say. It is fundamentally "algorithmic." Secondly, we want to remember that textual rhetoric operates at the material level of the text-that is to say, through its bibliographical codes. Both matters are crucial if we are to build digital tools that can exploit the resources of traditional texts. Finally, we have to preserve a clear sense of the relation of those two features of traditional text and the inherendy differential, underdetermined character of textual works. Texts are not self-identical, but their ambiguous character-this is especially clear in poetical texts-functions in very precise and determinate ways. Of first importance is the analysis of the elemental formality that makes text possible: textspace, or the confmes that invite the appearance of text in any form at all. We think of this as some kind of pagespace or its equivalent, but in fact text can be entertained in spaces whose elements are distributed in linear or nonlinear arrangements, or both. In the case of nonlinear, the topology may be open or closed (a cave wall, say, versus a bowl, a vase, a knife, etc.). Those spaces represent different executable programs for the deployment of text. If we think in terms of pagespace exclusively, the formal options for deploying text (or text plus shapes, or text plus shapes and/or images) are both complex and determinate. Any given pagespace could be analyzed for its textual deployment rules, and one could also go on to layout a higher order set of rules for pagespace in general-a set that would be generalized from an inductive study of a comprehensive body of documentary instances. The atomic documentary unit of any pagespace would be a textual mark, that is to say, an alphanumeric or a syllabic (or perhaps an ideographic) mark along with the space that has interpellated the mark. That unit of marked space can then generate, through rule-governed procedures, the higher-order textual constructions that are available within pagespace. is only pardy distinguished in such documentary terms, however. For one thing, those terms are purely formal ones and say nothing about the generative capabilities of a textual document considered with respect to its materialities (whether semantic or linguistic). For another, although the formal structure of pagespace is planar and two-dimensional, the forms of pagespace can be made to generate themselves through n-dimensional orders (see chapter 7). Furthermore, beyond the documentary state of the text stand many other rule-governed orders of textuality-rules of grammar, rhetoric,

150

Radiant Textuality

genre, and their subsets of rules. And informational as opposed to poetic texts illustrate a crucial generic distinction, as everyone knows. Those two great genres proliferate through numerous subgenres that are framed by coded instructions. Any given textual form represents a complex set of transactional codes written for simultaneous and/or sequential execution. In all these cases we are considering what texts are doing in saying what they say. I have deliberately kept the analysis from considering any more complex, higher-order types of textual performatives-for instance, from considering the ideological dimension of textuality, which is perhaps the one area of textuality where injunctive processes have drawn regular-if not especially rigorous-critical attention. The point is to keep the form of the analysis as sharply defined as possible so that we can get a clear view of the rhetorical scene at a general level, because the complexity of textspace and its coding options are very great. Let's shift, then, from these spatial grammatologies to consider textuality in terms of the lexicon. Any textspace can, in the abstract, deploy any lexicon. But in fact any text coded into any textspace brings with it certain discursive instructions, that is, certain rules that delimit the discourse(s) being deployed in the textspace. Bakhtin's celebrated discussions of textual heteroglossia reference in a general way this discursive structure for fictional texts. Or consider the brief opening chapter of G. Spencer Brown's lAws of Form (1969), where a discourse drawn from the (closely related) lexicons of logic and mathematics is being deployed. 9 What is important to rememberWittgenstein forced us to this recollection, remember?-is that semantic materials are not units of atomized meaning. They are parts of a language game-more than that, they are instantiated instructions for playing a certain language game in a certain time and place for certain particular purposes. That truth about text was first brought home to me years ago in Chicago. I and some of my students interested in poetry used to playa game with brief texts of verse. One person would choose a short and more or less obscure poem (unknown to the others), break it down into an alphabetized list of its constituent words (including the number of times any word appeared in the poem), and then give the list to other persons in the group. The game was to construct a poem from the list of words. Playing the game we soon discovered that our reconstructed poems were often uncannily similar to each other; more, they seemed borne along by a kind of fate back towards the original poem. We then changed the game and made our object the reconstruction of the original (unknown) poem from the given word lists. We gave up the game when it became obvious that we could do these reconstructions-not because we were especially clever, but because (a) we

Rethitlkitlg Textuality

151

could access the discourse of poetry in various dialects, and (b) the word lists represented brief excerpts of a complete language game. The apparent randomness of the lists comprised a second order illusion that threw into relief the coded character of the original texts. We weren't dealing with code and output. Everything was code and output simultaneously. Implicit in this discussion is the (theoretical) possibility, long cherished by structural linguists, of deducing the rules for writing the codes that generate traditional text output. In my view such an effort would be most usefully pursued toward broadly semiotic, rather than more restricted linguistic, codes because the latter depend for their operation on a Oogically prior) semiotic space. Besides, that space, being more primitive, displays textual laws of form in structural terms that are simpler to see and read. The complexity of such a project of deduction, known to all and lamented by many, is a function of our different ways of parsing semiotic and linguistic space. To structure or analyze semiotic or linguistic space we need, we believe, some standard code for description and measurement. But such a standard code flees us for exactly the reason set out by Spencer Brown: you cannot obtain a standardized system of measurement when your acts of analysis are drawn from and make up a part of your subject of attention and inquiry. We get fixed standards-rules of naming and rules of relation--only when we make an impermeable distinction between subject and object. In a textual condition, while such a distinction can be drawn, it can only be maintained arbitrarily. Even structural linguistics is riven by difference, and its best practitioners, conscious of those differentials, have always tried to make them as explicit as possible. What has this to do with the possibility of using digital technology to improve the study of traditional text? The discussion may perhaps appear simply to have spun to a dismal conclusion about that possibility. But my argument is that we (necessarily) reach dismaying conclusions in these matters when we discount or miss the character of traditional text. Text generates text, it is a code of signals elaborating itself within decisive frames of reference. A text is a display and a record of itself, a fulfillment of its own instructions. Every subsequent re-presentation in whatever form--editorial or interpretive-ramifies the output of the instructional inheritance. are Jjke mcta) derivations. From an analytic perspective, what is especially interesting about those re-presentations is their unmistakable errancy with respect to the "original text." Whereas the latter appears self-identical, the former seem alien interventions brought-paradoxically if not ludicrously-to reveal the truth

152

Radiant Textuality

about an original object that-paradoxically if not mystically-seems difficult to access despite its clear integritas. But because text is a field of dynamically unfolding elements and relations, every "state" of a text represents an arbitrary form "taken out of The Form," as Spencer Brown might put the matter (see chapter 7). These forms are de-forms and their usefulness for text analysis lies exactly in the set of differential possibilities they call to attention. Every text is a network of roads taken and not taken. Some of the roads have never been taken, so far as we know, and of the roads known to have been taken, some are well traveled and some hardly traveled at all. Who traveled which roads, and when, and where, are matters of consequence to anyone studying the texts. Roads identical in one respect or another may be seen as very different roads if viewed from a different vantage-and of those different points of view, many will be possible. -"But those are mere figures of speech," you will say, "how do you translate them into digital instructions?" -"Look for the protocols of figuration. Here the graphic text may serve as an instructive figure-a clear instance-of an elementary set of instructional options (not 'figures of speech' but typographical figures-so to speak). The instructions are always instructions to mark and hence to deform/transform. So take any given poem and make a schedule of its transformations. Readers do this as a matter of course as they move through a text and make themselves the measure of a process of transformation. Scholars searching the text develop those transformations to exponential degrees. Let the transformations be marked and let their forms be stored in a network of related forms. Let that network of forms search itself and generate further forms out of itself, and let the process replicate. Ask the network to display the forms realized or not realized in any given text or set of texts." We don't want to discover what the texts mean but what they might be imagined to mean or to have meant. Those meanings are a function of what texts might or might not do, given their rules of engagement; and those rules are determined from what they have and have not done, as well as what they might have done or might be made to do given their historical descent. The initial search should, I believe, be undertaken at the elemental level of figuration-that is to say, in the metaphoric field where (so to speak) graphemes are carried across to phonemes and vice versa. The phoneme is a metaphor for language expending itself without record or trace other than the evidence implicit in what Shakespeare called lust in action. Blake saw

RethitJkitJg Textuality

153

this as the language of eternity when he wrote, in The [First] Book ojUrizetJ, that before there was any "Earth ... or Globes of Attraction," "Eternal Life sprung" (plate 3). By contrast or differential, the grapheme is, as Derrida showed, the trace, or a metaphor for language not as an "expense of spirit" but as a reflexive record of itself. Both grapheme and phoneme are forms of thought and not facts-not character data but parsed character data, or "data" that already functions within an instructional field. The elemental scene where those metaphoric transformations expose themselves is the marked field, the graphical or auditional record. Because this will be a record of rule-governed differences, one can extract from that field a dataset of (hypothetical and arbitrary) rules that could replicate analogous differences in comparable fields (including the original record as it might be augmented and transformed by replicant operations). The output of such operations would be collated as a calculus of variants and delivered to us for study. Throughout, the operation assumes that the representation of knowledge involves the construction and display of difference. That assumption appears in every final "output" as a confrontation between a digital display of knowledge and a human reflection of/on the display.
Instructional Examples from The Garden of Forking Paths

A. Byroll's "To the Po" / "StatJzas to the Po"
There are two holograph manuscripts, a draft (MS. M, 1819, Morgan Library) and a fair copy (MS. B, 1820, New York Public Library). Both are titled "To the Po. June 1819,"10 both are in continuous units rhyming like abab quatrains. Because no space breaks come between those rhyming units, the rhyme scheme could as well be described ababcdcdefef ... 11 Several copies were made from manuscript M during Byron's lifetime by other persons, all arranging the poem in distinctly spaced quatrain units; all are titled, however, as the holograph manuscripts. The poem was first printed after Byron's death (1824) without a title from one of those dependent nonholograph copies, and it was first printed in an authoritative collected edition in 1831 from a modified version of the 1824 text. The 1831 text is in quatrains and is titled "Stanzas to the Po." All subsequent editions follow the 1831 version until 1986, when the text was printed as it was left by Byron 12-that is to say, tJot in space-marked stanza units but as a continuously unfolding text built from quatrainlike units that are sometimes end-stopped and sometimes enjambed.

154

Radiant Textuality

These two basic textual forms represent different sets of reading instructions for the work. Choosing one or the other radically affects both how one physically negotiates the work and how one interprets it for meaning. Furthermore, the existence of both of these forms points toward a general set of rule-governed options available for textspace scripts. 13 Let us look at the opening 20 lines of the poem in diplomatic transcriptions of Byron's manuscript text, on one hand, and the influential 1831 printing, on the other. First, then, the manuscript text of 1819:
To the Po. June 2nd 1819. River! That roUest by the ancient walls Where dwells the lady of love, when she Walks by thy brink, and there perchance recalls A faint and fleeting memory of me, What if thy deep and ample stream should be A mirror of my heart, where she may read The thousand thoughts I now betray to thee Wild as thy wave and headlong as thy speed? What do I say? "a mirror of my heart"? Are not thy waters sweeping, dark, and strong, Such as my feelings were and are, thou art, And such as thou art were my passions long. Time may have somewhat tamed them, not forever Thou overflow'st thy banks, and not for aye The bosom overboils, congenial River! Thy floods subside, and mine have sunk away, But left long wrecks behind us, yet again Borne our old career unchanged we move, Thou tendest wildly to the wilder main And I to loving one I should not love

5

10

15

20

Now here is the text first printed in 1831 and subsequendy followed by all of Byron's editors until 1986:
Stanzas to the Po. River, that roUest by the ancient walls, Where dwells the lady of love, when she Walks by thy brink, and there perchance recalls A faint and fleeting memory of me;

Ret/,inki"g Textuality

155

What if thy deep and ample stream should be A mirror of my heart, where she may read The thousand thoughts I now betray to thee, Wild as thy wave, and headlong as thy speed! What do I say-a mirror of my heart? Are not thy waters sweeping, dark, and strong? Such as my feelings were and are, thou art; And such as thou art were my passions long. Time may have somewhat tamed them,-not for ever; Thou overflow'st thy banks, and not for aye Thy bosom overboils, congenial river Thy floods subside, and mine have sunk awayBut left long wrecks behind: and now again, Borne our old unchanged career we move, Thou tendest wildly onwards to the main, And I-to loving one I should not love.

5

10

15

20

The linguistic changes that come into the 1831 text shall not detain us, interesting as they are. We shall concentrate on bibliographical details only, and especially on the general graphical transformation of the shape of the poem as Byron originally wrote it. Crucial to note is that Byron wrote the poem in metrical quatrains rhyming abab. By long-standing typographical convention such a verse form is normally arranged as in the 1831 printing, that is, in separate fourline stanzas. But when he wrote the poem Byron departed from that convention, nor is it difficult to see why: Running the quatrains without stanza breaks forces an approach to the affective pace of the poem that turns the graphic form into a figure of the dominant linguistic figure (headlong passion and headlong river). The eye registers the presence of the quatrains and the refusal of the stanza convention and then "reads" the relation between the two. Much can and should be said about the implications of what we see here. At the most general level we remark the signifying character of even the most elementary typographical conventions of verse presentation. What we observe here of this four line stanzaic form should be extrapo-

156

Radiant Textuality

lated to the presentational character of all stanzaic forms, as well as to the visual segmentations executed at the text's lower-if no less significantlevels. Punctuation is the most crucial visible form in this case. We want to recall that this highJy evolved set of marks represent signs that were originally introduced as notations both for oral articulation and syntactic differentiation, and that they function in both registers to this day. As a set of oral cues-whether in silent or in articulated reading--punctuation is a foundational element in the affective (as opposed to the conceptual) ordering of the poem. As a set of syntactic cues it is also a signifying system foregrounding dominant sets of conceptual relations in the text. We see some striking illustrations of these matters in this Byron poem. Compare, for example, the terminal punctuation of lines 4,8, and 13 in the two versions. In line 8, what a difference between the exclamation point and the question mark. Or consider lines 4 and 13. In the 1831 text the editors introduce an end-stopped punctuation that isn't present in Byron's manuscript, and they further alter the movement of the text by breaking Byron's word "forever" into two words. The two texts signal very different pacings-what Keats famously called "unheard melodies"-for the reader. Furthermore, the t 83 t punctuation of line t 3 also directs the reader to a different conceptual understanding of the poem. In Byron's manuscript the enjambment at the end of line t 3 places the phrase "not forever" in a zeugmatic structure such that we are asked to read it simultaneously in two different syntaxes (as modifying "tamed" and as modifying "overflow'st"). The 1831 punctuation is a visible signal to read the phrase in only one syntax. In this respect it replicates the meaning signaled in the t 831 decision to print the poem in four-line stanzas: to do that is not only to tell the reader that the poem's basic metrical unit is a quatrain, it is also to say that the quatrain and its textual presentation stand to each other in a relation of symmetry rather than a relation of tension. And that "statement" imbedded in the text's visible form plainly goes to the very core of this poem's intellectual and affective meaning. We want to point out that in this example we have consciously chosen a poet who is not known for any special interest in exploiting graphical forms for poetical effect. We leave aside Blake, Rossetti, Dickinson, and pattern and concrete poetry precisely because we wish to show that the standard presentational and graphic features of text are signifying features as such. In the effort to articulate meaning, affective as well as conceptual, one necessarily installs the visible resources of language. For every text is comprised simultaneously of a beauty of inflections as well as a beauty of innuendoes.

Rethinking Textuality

157

Here are two further examples. I shall not elaborate explanations in these two cases but leave those exercises in particularities to the reader.

B. Byron's "Men

Two Parted"

In this case we are dealing with equivalent texts that are migrating authoritatively through different textspaces. A simple example of this kind of situation in Byron's works involves the document he published separately in 1816 under the tide "The [ncantation" and the equivalent text that appeared in 1817 as part of Byron's play Manfred. Text is a spacetime manifold where these kinds of translations and reinvestments take place on a regular basis. "When We Two Parted" is remarkable only because it illustrates how complex these relationships can become. Between 1812 and 1823 some or all of the text published by Byron in 1816 as "When We Two Parted" was incorporated by him in at least eight completely different kinds of textual document-as printed and manuscript documents; as singly authorized and collaborative documents; as integral poetic manuscripts and as parts of manuscript letters where the formal poetic status of equivalent texts are completely transformed by incorporation in new prose expositions; and finally, as integral poetic forms that exhibit completely different metrical organizations.
C. Byron's "Fare Thee

This is another case of migrating texts. [n this case the textspaces differ because only some of the translations and reinvestments are deliberately authorized by Byron. The most common instance of this situation unfolds as the general transmission history of any particular work or author during and then after his or her lifetime. Because that history commonly appears to us as more or less integrated and continuous, eventuating at any point in a largely transparent and apparendy self-identical text, "Fare Thee Well!" is a useful case. Although the lexical form of the poem differs hardly at all moment by moment across its process of transmission, this work is marked by many contradictions. tensions. and discontinuities generated through the bibliographical coding. Those contradictions are most apparent in the sequence of the poem's transmissions in 1816. Byron distributed copies in manuscript to various persons. and he had the poem printed twice. In the interstices of those acts of authorial transmission appear other acts of textual transmission-in manuscript as well as print--executed by other par-

158

Radiant Textuality

ties, many of these persons Byron's enemies who clearly "read" his poem in antithetical ways. This network of relations is highly dynamic and interdependent, with different texts emerging as responses or consequences of other texts. In all cases, while the linguistic level of the texts remains fairly stable, the material/transmissional forms stand as eloquent witnesses of radical changes in the poem's "meanings."
Knowing Games

What then does distinguish cybertext from traditional docutext? Without pretending to answer that question, I would call attention to the special kinds of simulation that can be realized in cybernetic environments. While both Aarseth and Murray discuss computerized simulations, their critical taxonomies permit the subject to come forward only at the interspaces of their studies. Of course all traditional texts construct simulations, but with docutexts we engage these simulations as "readers." A project like Michael Joyce's celebrated hyperfiction Afternoon or my own Rossetti Archive are paradigms of the "humanities" cybertexts we see all around us now. Both were conceived and designed as high-order reading environments. The Rossetti Archive was imagined as a simulated syndesis of a critical edition of Rossetti's textual works with a complete collection of facsimile editions of those works and a complete set of illustrated catalogues of all his pictorial works, including the reproductions of those works. The whole, however, remains a study environment embedded in a reading environment. In this context it helps to remember that Plato disapproved of these kinds of textual simulations as instruments of study, thought, and reflection. For Plato, the optimal scene for thinking had to be living and dialectical. Texts are inadequate because they do not converse: When we interrogate them, Plato observed, they maintain a majestic silence. But in MUDS (Multi User Domains) and with various kinds of cybergames like ELIZA, one enters simulated environments where the user's interaction is no longer a readerly one. This result comes from the construction of a textual scene that simulates in real time an n-dimensional spatial field. One thinks of the Chorus's speech to the audience at the opening of Hetlry V, except in cyberspace the "wooden A" of the Shakespearean stage has been extended to include the audience as characters in the action. 14 Computer games exploit this new dynamic space of textuality by inviting the user to play a role in the gamespace. These are well-known role

Rethinking Textuality

t 59

types like warrior, hero, explorer-adventurer, creator-nurturer, problemsolver, and so forth. And while players may well have to read at various points, their participation in the game is not readerly. When cybertext enthusiasts speak of the "passive" docutext and the "active-participatory" cybertext, they are calling attention to this differential. Traditional readers rightly point out that reading is a highly participatory activity and one that is commonly quite as "nonlinear" as any cybertext. When the examples from Byron show us that multiple coding operates at the elementary material level of textspace, they are simultaneously demonstrating the interactive nature of that traditional textual condition. As a traditional literary text enters (or is translated into) a cyberspace, then, it will be laid open to "participations" that mayor may not be readerly participations. Indeed, paperspace is a far more effective medium for reading than cyberspace. From the point of view of someone wanting to create imaginative works, however-narrative or otherwise-cyberspace is replete with inviting opportunities. But from the point of view of the scholar, or someone wanting to reflect upon and study our imaginative inheritance, the resources of cybernetic simulation remain underutilized. The difficulty is conceptual not technical. Even when we work with cybernetic tools, our criticism and scholarship have not escaped the critical models brought to fruition in the nineteenth century: empirical and statistical analysis, on one hand, and hermeneutical reading o'n the other. What critical equivalents might ' we develop for MUDS, LARPS, and other computer-driven simulation programs? How would one playa game oLcritical analysis and reflection? That quesDon brought imu view the idea for what we would eventually call "The Ivanhoe Game." It would be a multi-user game designed to expose the structures of imaginative works like Scott's famous romancewhich is also to say the structures that any literary work like Ivanhoe makes possible through the double helix of its genetic (social) codes: its production history and its reception history. These are the content fields of the game of Ivanhoe--discourse fields, as their scholars call them. The game would be played in either of two available multi-user domains: a real-time environment and a list-serve environment. Players would enter one or both as they like, and they would engage with others either as themselves or under consciously adopted roles. The game is to rethink Ivanhoe by rewriting any part(s) of its codes. Two procedural rules pertain: First, all recastings of the codes must be done in an open-text environment such that those recastings can be themselves

160

Radiant Textuality

immediately rewritten or modified (or unwritten) by others; second, codes can only be recast by identifiable game-players, digital or human, who have specifically assumed a role in the game. Any number of roles might be played. There are the roles of the fictional characters first imagined by Scott for his romance and for its surrounding materials. But to these we add other possible roles: persons involved in the book's material production; Scott's precursors, contemporaries, and inheritors Oiterary and otherwise); early reviewers and any of its later readers, reviewers, critics, illustrators, redactors, translators, or scholarly commentators; in general, persons in the book or persons who might have been in it, real or imaginary, as well as persons who read the book or who might be imagined reading it, for whatever reason. The roles may be played in various forms: in conversation or dialogue, through critical commentary and appreciation, by rewriting any received text, primary or secondary, seen to pertain to Scott's work. The goal is to rethink the work's textuality by consciously simulating its social reconstruction.
VOICE OF AN ANGEL. But this is implicitly to propose that the works of our cultural inheritance have no meaning or identity an sicll-that their meanings are whatever we choose to make of them. It is to make a mere game of the acts of imagination. VOICE OF THE DEVIL. Are we then to make a business or religion of those acts? If we see it as a business then we propose to make something of our inheritance and not simply bury it in the ground, lest it be lost. If it is a religion we propose to recreate the world anew exactly as did the demiurge of the Book of Genesis when he refashioned his pagan inheritance by pretending there were no strange gods before him and then making a rule forbidding any later ones as well.

Chapter 7

Dialogue and Interpretation at the Interface of Man and Machine

The electric things have their lives, too. Paltry as those lives are. -Philip K. Dick, Do Androids Dream of Electric Sheep
rying to think about texts, documents, and their possible electronic transformations, I am going to begin at a severely oblique angle. I do this to set a visibly disorienting figure-obliquity-into the rhetorical structure of the exposition. We all know too much about texts and textuaIity. We need to think about them in different ways. Knowing this move is being made, you may, I trust, willingly suspend your belief in it and instead work through its evident premeditation. My hope is that the exposed rhetorical illusion will set a clarifying frame around the issues, the way Brecht sets a clarifying frame around his theatrical investigations by exposing his use of them. The discussion will begin through a return to G. Spencer Brown's remarkable 1969 book LAws of Form. t In his Introduction Brown notes that in writing his book "I found it easier to acquire an access to the laws [deliberated through the work) than to determine a satisfactory way of communicating them" (xxii). The comment seems at once, and paradoxically, both modest and outrageous, particularly for a book that takes as its point of departure and central subject "self-referential paradoxes." That modest/ outrageous statement is as much a self-referential paradox as the famous one Brown cites in the preface to the American edition of his book: "This statement is false." Brown's observation about his expressive difficulty cuts to the center of the laws of form his book seeks to elucidate, as I hope to demonstrate later in this chapter. Laws of form, it turns out, are expressive (trans)forms and

T

194

Radiant Textllality

are reflected-and reflexive-as such. And certain kinds of text-Brown calls these "injunctive" texts-reveal why it's difficult to communicate the laws they themselves realize. This chapter will thus reconsider the issues taken up in LAws of Form. My purposes are, however, more narrow and more practical. I want to elucidate some key but neglected formalities of textual documents and to meditate satisfactory ways of communicating those formalities. I am particularly interested in documents that have been indispensable for traditional humanities disciplines: language and literature, history, philosophy, art. 2 Realizing the need to develop a reliable system for representing such documents in a form that lays them open to the power of digital analysis, humanities computing during the past 20 years has worked hard to develop a model for text markup with general applicability. Despite the problems its own implementation exposed, the model of TEl was developed and, as we've seen, has become a disciplinary standard. 3 But with increasing numbers of humanities scholars using digital tools in their research work, the realization is growing that TEl's problems are not technical but systemic. To address them properly we have to step back and think not about TEl but about "text" itself. 4 What is text? I am not so naive as to imagine that question could ever be finally settled. Asking such a question is like asking "How long is the coast of England?" But now we have to ask it again because when the question was re-posed by our digital culture, the humanities response proved inadequate: on one hand a reactionary refusal to admit that this new culture had any right to ask such a question (Sven Birkerts); on the other, the emergence of TEl and the proposal that its view of text would serve the interests of humanities scholars for digital culture. Much is at stake here. Even now we are beginning the process of re-editing--of representing-in digital form the entirety of our received textual and documentary \ archive. How successful this effort will be depends on how clearly we understand the materials we have to work with. On one hand, digital tools often appear strange and wondrous, especially because they spawn and mutate so quickly into rich and surprising possibilities. Books and documents, on the other hand, seem stable and familiar. They are tools we have learned how to use, they are reliable. The question is, how well do we understand them? Too well, I believe. They have become familiar to us and consequently have grown much more obscure. Brown's unusual approach to questions of form can help us think our way back into the problems of textual forms. But his work will itself benefit by a tangential move. To begin thinking

Dialogue and Interpretation at tile Interface of Man and Machine

195

about textuality with Brown, then, lees begin again further back, by thinking about textuality with Dante, whose grasp of the subject was acute. His way of thinking is especially useful in this case exacdy because it is a premodern way. Inner Infinities In the Vita Nuova, Dante regularly attaches explanatory prose descriptions to the poems he imbeds in his famous autobiography.5 These "divisions," as he calls them, are "made to open the meaning of the thing divided." Some of the divisions are so brief as to seem perfunctory. Others appear so simple and transparent that we wonder what use they might serve. Then again, in the case of one sonnet-an especially cryptic one as Dante himself acknowledges-no division is supplied because, Dante says, "a division is only made to open the meaning of the thing divided; and this, as it is sufficiendy manifest through the reasons given has no need of division." The "reasons given" are the words in the text that explain "the occasion of this sonnet."6 I shall have to pass without remark much in these "divisions" that could be usefully taken up, even in the present context. My focus will be on a pair of related matters: the fact that dividing the poems into parts should seem to Dante a way of opening up their meaning; and the fact that the divisions Dante makes seem arbitrary. For example, after quoting the sonnet "Tutti li miei penser parlan d' Amore" Dante says that "This sonnet may be divided into four parts" ["Questo sonetto in quattro parte si puo dividere "1 and he then proceeds to do two things: First, he restates in schematic terms the prose sense of each of the designated parts; second, he then indicates where in the poem each part falls. Although the exegesis may in any particular case be more or less elaborate, this is the general double form that it always takes. Note that in dividing this sonnet Dante chooses to distinguish four parts-that is to say, his partitioning represents a judgment Dante makes about what would be useful for the reader to know. The arbitrariness of his divisions leap to one's mind as soon as we see where the four parts separate from each other. In the case of this sonnet, part 1 comprises only line 1, part 2 includes lines 2--6, part 3,lines 7-8, and the last part covers the sestet. Looking at his divisions for the other poems in the text, we find that they too have little relation to their highly formalized metrical structures. The divisions cut across those structures 10 apparently random ways, as we

196

Radiant Textuality

see in the case of this sonnet. The randomness is all the more clear because Dante does not disguise from us the fact that the divisions have as muchperhaps more-to do with his purposes toward his readers as they do with the formal structure of the poems themselves. We notice as well that the exegeses, even the elaborated ones, are spare to a degree. Unlike for instance in the Convivio, Dante gives no thematic or symbolic interpretations. Indeed, wherever the poems seem most obscure he typically retreats even further from explanations, even from the schematic divisioning process that marks his method here. So in the case of the sonnet that he doesn't divide at all, Dante says something remarkable. Though obscure and ambiguous words spring up, like tares among the wheat, in the passages "whereby is shown the occasion of this sonnet" in a clear way, Dante chooses not to provide a divisioning, for the difficulties, he says, can't be solved by anyone who doesn't have a deep insight into the issues in the first place:" And therefore it were not well for me to expound this difficulty, inasmuch as my speaking would be either fruitless or else superfluous." Dante's method is perhaps most fully revealed in his prose division of the great canzone "Donne ch'avete intelletto d' Amore," the Vita Nuova's central text. "That it may be better understood," Dante says, "I will divide it more subtly than the others." Then, having laid out an intricate set of divisions and subdivisions, he finishes his analysis with a remarkable set of statements:
I say, indeed, that the further to open the meaning of this poem, more minute divisions ought to be used; but nevertheless he who is not of wit enough to understand it by these which have been already made is welcome to leave it alone; for certes, I fear I have communicated its sense to too many by these present divisions ....

From all this several things of importance seem clear. For Dante a process of divisioning could be carried on indefinitely, with a partitioning analysis moving to isolate further ranges of subdivision at any and all levels. Furthermore, if the divisions represent "objective" characteristics of the poetical work, they come to expose only a certain range or set of the poem's formalities-one reconstructed as a certain perspective on the poem taken by Dante. These divisions are, as we've already noticed. incommensurate with any metrical structure; they transcend sentence grammar; and they represent only what Dante himself thinks might or might not be useful for the reader. He seems to produce them as models

Dialogue and ItJterpretation at tire Interface of Man and Machine

197

or stimuli that might provoke readers with "wit enough to understand" and to search out meanings (and divisions) for themselves. Finally, Dante's divisions do little more than mark off places in the poems, as if each were a kind of field or area to be mapped rather than a complex linguistic event to be paraphrased or "interpreted" in the manner of the Convivio or of the "readings" we have cultivated in our twentieth-century exegetical traditions. In setting out these divisions for his poems, Dante recalls us to a crucial primitive level of his work's textuality. The significance of Dante's divisionings can now be exposed further by considering them in relation to Brown's Laws c?f Form. These laws draw out the structural dynamic implicit in form as such. The elemental condition or manifestation of form is the appearance of a mark in an otherwise unmarked space. Brown calls this mark a "distinction" so that the elemental law of form is: a distinction can be drawn. Every conceivable formal world may be traced or tracked back to that elementary law. Dante's textual divisions illustrate his understanding that the same law underwrites the making of poems. Each poem is a kind of world or universe to itself, and any set of poems-for instance, the set chosen for the Vita Nuova-may be conceived and fashioned into a meta-universe. So we might say-might show, as critics have done for centuries-that the Vita Nuova is usefully seen as part of other networks and textual universes. Or we might turn the direction of our tracing in the opposite direction, back into the subuniverses concealed, as it were, within the apparitions of the specific poems placed in Dante's autobiography. The latter is Dante's own procedure when he offers us his divisions and their arbitrariness points to the infinities of order that may be tracked inward of the poem, through the looking glass of its surface(s).

Argument by Bibliographic Code
Drawing on the ancient tradition of the Arts of Memory, Dante's textual divisiones point toward the inherently spatial conception he has of his textual field. The Vita Nuova is a "book of memory" shaped by visible rubrications so as to give a mirror image of the events it aims to recall. Indeed, it is for Dante only one section or division of a larger book clearly visible to his mind's eye as he undertakes its (re)construction: "In that part of the book of my memory before the which little can be read, there is a rubric, saying, Indpit Vita Nova. Under such rubric I find written many things; and among them the words which I purpose to copy into this little book."

198

Radiant Textuality

Time itself for Dante occupies a space of events mapped on a grid of mathematical and astrological relations. Movement, textual as well as human, occurs within a fixed space where the relations of things is unimaginably deep and complex. One divides this space in order to mark a way into those complex relations. Mark, space, direction: If we think of language in linguistic terms, these words as I've been using them appear to us as metaphors, figures of speech. But if we think of language in semiotic terms the same words take on a literality that can be extremely helpful for understanding how language works, and particularly-for my present purposes-how the language of poetic forms works. From that altered scale of attention we may then start to re-imagine, in the graphematic terms realized through digitization, analytic tools for our most complex semiotic devices. It is useful to remember that the text we are transacting here and now on this page is not just a vehicle for communicating certain ideas. It is simultaneously a reflection of and on itself, an analysis of itself-to borrow from Dante, a divisioning of itself. The analysis is executed semiotically, that is, through the deployment of the forms of what might be called, after Robert Horne, its visual language. This language Horne describes as a composition of words and shapes and/or images. Useful as this definition is, it is not strictly accurate, and the loose feature of Horne's view is important since it pervades, as an assumption, nearly all approaches to textuality. 7 The point is important and must be pressed. Horne sees visual language when he sees wordtext combined with shapes or wordtext combined with images or wordtext combined with both together. But graphically transmitted wordtext is always ab initio "visual" since there can be no wordtext without the presence of what Horne calls "shapes." According to Horne, images are optional to graphic text but shape is not. But in fact whether or not shape enters the pagespace as explicitly drawn boxes or arrows or circles or whatever-these are the forms Horne sees-shape is ever present. Graphically transmitted texts, by elementary "laws of form," automatically generate-perhaps "incarnate" would be the better termshape, which emerges along with the primal "mark" that shapes Brown's demonstration. So far as our common (visual) language is concerned, then, the elementary marks are an alphabet of letters, plus an accompanying set of signs-explicit or implicit-for reorganizing the letter marks into different scales and sets of relation. One can distinguish ("si puC> distinguere") the most common elementary textspace as the page itself (in contradistinction,

Dialogue and Illterpretation at the Interface

of Man

and Machine

199

for instance, to a scroll's textspace or to the nonlinear textspace of, say, a cave wall). This pagespace is elemental because it replicates at a different scalar level the same kind of distinction marked within the page space by the elementary letter and graphic marks. The relation between the elemc.:I!!at¥ §APl:Jk marks and the elemental page space sets the parameters for all types of graphemic directionality. In the spatial conventions of the page regulated at two dimensions, the normative directions are horizontal left to right and vertical top to bottom (along with a normative line of directionality from upper left to lower right). So for the paperspace at two dimensions, a range of complex spatial options can be manipulated at the borders of the page space, between and within the lines of text, and as blocks of space and/or text formed within the main dynamical textspace area arbitrarily established by the margins, header, and footer. Variant forms of sequence and direction are developed through rule-governed deviations from these norms. In bookspace, pagespace variances emerge as a set of higher order conventions of three-dimensional relations: between page rectos and versos; between the single page and the page opening; and between sequences of pages gathered together. As more explicit shapes and/ or images are introduced into the paperspace, that space will be pushed toward a space governed by rules of collage rather than by rules of textuali ty. B A page of i ted text should thus be understood as a certain kind 0 hic interfac The complexity of the interface varies from a minimal use of the bibliographical codes open to a given paperspace-the text you are now reading is a good instance of such simplicity-to highly elaborated interfaces like those determined as poetic texts. Some of the latter exploit the bibliographical resources of paperspace to an extreme degree--Pound's Cantos, for example, or Dickinson's various writingswhile others are satisfied to work within a set of basic and commonly used conventions. Whatever the specific differentials, however, a broad heuristic distinction separates informational from imaginative texts. The former aspire to transparency, the latter to noise, redundancy, repetition. One is vehicular, the other, iconic. Exploiting bibliographical codes does not per se signal that poetic motives are governing a particular text, as the work of Tufte and Horne-as their own books-indicate. Both participate in a long tradition of "knowledge representation" that mixes textual and graphic forms (a tradition importantly advanced in the late work, the existential graphs, of Charles Sanders Peirce). It is crucial to realize that every text may choose, as it were, to engage that self-conscious tradition. We see this in the layout

200

Radiant Textuality

and general book design of Brown's LAws if Fom,. These apparitional featUfes of the work show Brown's effort to meet the problem of effectively communicating himself. In the event, the rhetorical and design move would elucidate the laws of form at a much deeper level. Figures 7.1 and 7.2 reproduce the first edition of chapter 1, "The Form," which begins with a statement of assumptions, an initial presentation of The Form's elemental definition, and a declaration of The Form's two primary axioms, the law of calling and the law of crossing. Brown's root concept of "distinction" is thoroughly replicated in this text's bibliographical codes, in which various key differentials are developed through the manipulation of pagespace, changes of font, and the deployment of one simple, explicit shape (a line).9 The layout constructs a graphic scene composed of related planes, colors, and textures. The simplicity of the elements can easily disguise the sophistication, even the elegance, of the graphical representations that transact every moment of Brown's conceptual exposition. In terms of the latter, this text's bibliographical codes exemplify-instantiate-The Form's elementary definition as well as axiom 1. The law of calling. The text of chapter 1 does not instantiate axiom 2, the law of crossing, nor is that law exemplified at the bibliographical level in Brown's book anywhere in the first 11 chapters. 1o Of course Brown invokes and applies axiom 2 at a conceptual level throughout, but the axiom is never explicitly marked as such. We realize this remarkable fact about the book only in the culminant chapter 12, "Re-entry into the Form," where for the first time axiom 2 is instantiated. It gets marked in the text, however, not at the twodimensional convention of the page but at the three-dimensional convention of the chapter, as the chapter title indicates. At the page level of the chapter, where a series of experiments are conducted, none of the experiments even invoke axiom 2, much less mark it. That structure in Brown's book is extremely significant for understanding how laws of form operate in a textual horizon. The law of crossing governs reflexive functions, and in Brown's book reflexivity does not explicitly begin until the final chapter, whence it continues through the set of chapter notes and appendices that follow. How scrupulous Brown has been in constructing his text along the lines of his laws is underscored by the way those late chapter notes are (not) marked: Brown provides no links to them from within the chapter texts. The chapters draw and then draw out further and further distinctions until, at the conclusion of chapter 11, the process itself reveals "that the account may be continued endlessly" (68). At that point an implicit injunction becomes explicit in and as chap-

1
THE FORM

We take as given the idea of distinction and the idea of indication, and that we cannot malee an indication without drawing a distinction. We take, therefore, the form of distinction for the form.

De8nition

Distinction is perfect continence.
That is to say, a distinction is drawn by arranging a boundary with separate sides so that a point on one side cannot reach the other side without crossing the boundary. For example, in a plane space a circle draws a distinction. Once a distinction is drawn, the spaces, states, or contents on each side of the boundary, being distinct, can be indicated. There can be no distinction without motive, and there can be no motive unless contents are seen to differ in value.

H a content is of value, a name can be taken to indicate this value.
Thus the calling of the name can be identified with the value of the content.

Axiom 1. The law of calling The value of a call made again is the value of the call.
That is to say, if a name is called and then is called again, the value indicated by the two calls taken together is the value indicated by one of them. That is to say, for any name, to recall is to call.

G. Spencer Brown, LAws of Form (1969, first edition), page 1.

THe FORM

Equally. if the content -is of value. a motive or an intention or instruction to cross the boundary into the content can be taken to indicate this value. Thus. also, the crossing of the boundary can be identified with the value of the content.

Axiom 2. The law or crossing
The value of a crossing made again ;s not the value of the crossing.
That is to say, if it is intended to cross a boundary and then intentions taken together is the value indicated by none of them. That is to say, for any boundary, to recross is not to cross.

it is intended to cross it again, the value indicated by the two

2

Figure 7.2: G. Spencer Brown, LAws of Form (1969, first edition), page 2.

Dialogue and Interpretation at the Interface

of Man

and Machine

203

ter 12, where we are called to reflection, that is to say, where we are called to cross back to chapter 1: "we return for a last look at the agreement with which the account was opened." In point of textual fact, it is the book's first last look back. In the reflexive note to chapter 2 of his book Brown explains that "the primary form of mathematical communication is not description, but injunction. In this respect it is comparable to practical art forms like cookery [and] music" (77). Although Brown does not include text production among these practical art forms, he might and indeed should have done so, as his own book admirably demonstrates. Clearly Dante regards writing, including poetical writing, as injunctive. Dante's word would not be injunction, however, it would be-it was-rhetoric. 11 As Brown's book shows, a primary textual injunction is to make and elaborate distinctions. If these distinctions are rigorously pursued they produce the realization "that the world we know is constructed in order (and thus in such a way as to be able) to see itself": "and so on, and so on you will eventually construct the universe, in every detail and potentiality, as you know it now; but then, again, what you construct will not be all, for by the time you have reached what now is, the universe will have expanded into a new order to contain what will then be" (106). In a splendid act of wit Brown encodes this passage in the typographical convention of indented block quotation. But the passage is not a quotation in the ordinary sense; it is all Brown's own words. Setting it off as he does, however, Brown not only marks the text reflexively, he names himself one of those godlike "universal representatives [who] can record universal law far enough to say" what the passage says. These are altogether (and literally) lowercase gods, as Brown's text shows at a textual level that supervenes the typographical font. After rising to quote Brown's "universal" self in the block quotation, the text descends to its common margins to gloss that universal law as follows: "In this sense, in respect of its own information, the universe must expand to escape the telescopes through which we, who are it, are trying to capture it, which is us. The snake eats itself, the dog chases its tail" (106).
Injunctive Forms

That Laws of Form should display this kind of wit is perhaps not surprising. G. Spencer Brown-mathematician and philosopher-is after all also an imaginative writer-the latter under the pseudonym James Keys, whose strange autobiographical text Only Two Can Play this Game was conceived

204

Radiant Textuality

in the wonderlands of Borges, Abbott, and Carroll. Laws of Form does not resort to an imaginative genre. Nonetheless, an SGML or TEl markup that would adequately open such a work to digital analysis seems as unimaginable as it would be for the Alice books. The conceptual structure-the demonstration-of Laws of Form explains, demonstrates, why this dysfunction must occur in any case where laws of form operate-for instance, in language or any of its instances. Unbeknownst to itself until the moment when it turns reflexively back upon itself-and then it is too late-every form of thought is incommensurate with itsel£ Certain texts-and certain kinds of text-make that contradiction a primary focus of attention. Not many works of philosophy demonstrate that paradox as elegandy as Laws of Form-perhaps Nagarjuna's treatise on "Emptiness" makes an apt comparison. 12 Some do it with greater force-Montaigne, Kierkegaard, Wittgenstein (especially in that relendess work known to us as Philosophical Investigations). How to "edit" such writers presents a constant challenge and adventure, and hence a recurrent opportunity to address anew the unanswerable questions they raise. Works of imagination, hpwever-Iet us say henceforth "poetrytt-make the discourse of paradox.,Jnd contradiction the ground of their semiosis. In terms of Brown's Laws of Form, this means that distinctions in poetic texts are elaborated in a space that (so to speak) has no intention of maintaining original integrity. As textual boundaries are defined and crossed, the marks of distinction constituting the boundaries are canceled or threatened with erasure, because new marks of distinction turn out to be phenomenal illusions, closely akin to mathematical transformations. New distinctions conceal algorithms-hidden injunctions-to cancel the same distinctions by recrossing the boundary initiated when the distinction first appeared to view. Whereas everyone knows this about poetical texts, we are less clear about how and why this network of recursions unfolds. Yet clarity on the matter is particularly important in a digital horizon if we are to have any hope of building adequate electronic re-presentations of our received textual archive. Modern linguistic analysis from Saussure to Hjelmslev to Segre develops a four-part analysis of the sign.13 The signifier and the signified-the elemental dismanding and reconstruction of the ancient distinction of form from content-are each shown to replicate in themselves the form/content distinction because in any case both signifier and signified, in order to be recognized as such, have to be separately marked. The marking transaction creates, by the law of form, a new distinction--signifier

Dialogue and Interpretation at the Intetjace

of Man and Machine

205

versus signified-that dissolves a mistaken implication drawn from the earlier distinction of form/content. In the reconstructed sign both signifier and signified are not only "content constitutive tU they are so precisely because of their "form function tUbecause they have been marked. What this otherwise useful analysis does not indicate-what it positively obscureSt in fact-is the injunctive or rhetorical character of textuality. The structure of any text overgoes its own internal (signifier/signified) coherencies and/or contradictions. In Jakobsonian terms, this overplus is comprehended under the concept of "reference:' The concept functions reasonably well in analyses focused on informational and nonpoetic texts t but its analytic force dissipates when directed toward poetry. This happens because a modern aesthetic understanding shapes our thought about "the literary text.u Since poetical works are conceived as "communication sui generis u (or "language [oriented] toward the message in itself" [Segre t 28-29])t neither affirming nor denying anything beyond their internal relations, "reference u in the literary text turns (virtually) virtual. They are t as manasts used to say, "not among the ideologies.u Texts seen in this light turn dark and passive. They seem not to address us but rather to lie down and await examination t like corpses under autopsy or treasured secrets. That textual structure was fashioned in the rhetoric of romanticism t where a textuality was sought that would not appear to have "a palpable design upon" the reader. But all texts are generated through algorithms; romantic texts are coded with special instructions to obscure the design codes, or rather to make those codes appear not as reading instructions-not as marked text-but as pure character data. The signifier/signified/referent structure implicidy poses two (related) questions to a text: "What is it saying?U and "What is it doing in saying what it sayst' This second question points toward the injunctive feature that is open in every text and in every part of every text. Ultimately one would want to be able to describe and analyze what literary texts are doing, sui generis, in saying what they say-how they function in society and culture at large as they are literary works. To construct that kind of comprehensive analysis t however, we will have to undertake a thorough re-examination of their rhetorical and injunctive character at micro levels. The history of any text's emergence is both a record and an index of how it has been used t what it "meant." Those records should be recovered for a programmatic analysis of their injunctive features, which is to say for their Dantean "divisiones u and their Brownian "laws of form.U

206

Radiant Textuality

(In)Conciusion
But the historical record can only be "recoveredu through acts that cover it yet again, by agencies of markup. Every text descending to us is not only marked text, it is multiply and ambiguously marked. The analytic usefulness of aesthetic texts lies exactly in their generic inertia to pursue and exploit multiple and overlapping formalities and divisions in explicit ways. In such circumstances what is needed is a dynamic engagement with text and not a program aimed at discovering the objectively constitutive features of what a text "is." That dynamic requirement follows from the laws of form themselves, as Brown's work shows. But what equally follows is that the analysis must be applied to the text as it is perfimnative. We begin with an understanding that text is always the marked or materially distinguished text-the text as image and/or audition-and that the textual analysis is itself part of the marking processes that governs the object of study. The problem at this point becomes at once more clear and more difficult to address. One is perhaps reminded of Brown's observation that "I found it easier to acquire an access to the laws [deliberated through the work] than to determine a satisfactory way of communicating them." This difficulty arises because the act of communication promotes and reenergizes the original (historical) ambiguity of the textual signifiers. They are arbitrary forms, open to an indefinite range of significations. The more complex the form of the signifier the more deeply run the ambiguous options of meaning. In the visible state of language they scale up from letters and diacriticals to wondrous scriptural and bibliographical creatures. In this situation the limitations of determinately marked forms can be exploited for more dynamic operations. The proposal I imagine here is directed at visible text only and involves approaching the text not in terms of its semantic "content" but as a physically shaped construction. We do this on the assumption that the physical arrangement of the text amounts to a reflection or interpretation--a marking--of its semantic meaning. The reflection will inevitably introduce a "deformance" of the work, and thus will appear in one or another perspective as what Joyce once called (in the opening chapter of Ulysses) "a cracked mirror": because even at the purely bibliographical level the semiosis of any specific set of signifiers will lie open to different possible readings. The text's non-self-identity extends itself through all marked levels precisely because it is the operation of marking that divides the text from itsel£ The project imagined here attends only to the text's bibliographical

f

Dialogue and Interpretation at the Interface

of MatI

and Machine

207

codes in order to begin with a relatively simple set of rules for marking or interpreting textuality. We want to teach the computer a set of rules for reading texts. Trying to teach it higher order rules presents enormous difficulties. It seems possible, however, to develop an initial set of rules for bibliographical coding options and forms. Part of the programmatic operation is to implement these rules in order to expose and generate a more complex set of rules extending to higher orders of textual form. The ultimate event in this program will be a dialogue between the computer and the human beings who are teaching it how to read. We want to study the bibliographical formations that appear out of the computerized readings. These readings will, we believe, inevitably constitute a set of (de)formations full of surprises for the rule-givers. What those surprising readings will be cannot be predicted, but that they will come is, We think, as certain as the fact that no text is commensurate with itself. The more sophisticated we are the more we normalize textual incommensurates. We have internalized an immensely complicated, many-leveled set of semiotic rules and signs, and we control the contradictions of actual textual circumstances by various normalizing operations. We can hope to expose these normalizations-which are themselves deformative acts-by opening the conversation here being proposed between analogue and digital readers. We begin by implementing what we think we know about the rules of bibliographical codes. The conversation should force us to see-finally, to imagine--what we don't know that we know about texts and textuality. At that point, perhaps, we may begin setting philology-"the knowledge of what is known," as it used to be called--on a new footing.

