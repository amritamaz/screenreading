128.1

]
the changing profession

Redefining Reading: The Impact of Digital Communication Media
It's not a book. It doesn't have a smell, you don't touch it . . . , you're plugged into the internet, you can't concentrate, it hurts your eyes, and you lose the beauty of the words behind this screen. Life itself is in hard copy. . . . Not this treacherous digitalism which has permeated our lives and our reality. --Respondent to survey comparing on-screen with hard-copy reading

naomi s. baron

EACH NEW TECHNOLOGY MAY BE JANUS-FACED, POTENTIALLY IMPROVING AND DEGRADING THE HUMAN CONDITION. THE STEAM ENGINE made industrial products cheaper and more diverse but contributed to the exploitation of child labor and proliferation of squalid urban living conditions. he automobile makes transportation more convenient but pollutes and leads to countless highway deaths. Calculators let anyone perform feats of math but have weakened basic arithmetic skills. A related conundrum holds true for technologies of the written word. he printing press helped spread literacy but shook the foundations of the Catholic Church. Word processing enabled the Japanese to generate text without producing each kanji stroke by stroke, but now many Japanese ind themselves forgetting the stroke order. he spelling checkers in word-processing programs monitor typographical errors but dampen motivation to master spelling. Information and communication technologies have generated new platforms on which to read. he list includes desktop and laptop computers, e-readers (such as the Kindle and Nook), tablet computers (e.g., the iPad), and handheld devices (e.g., the iPod Touch, mobile phones). But does reading on these devices difer from reading in hard copy? If so, does our growing dependence on reading onscreen contribute to a redeinition of what it means to read? What Does It Mean to Read? Just as text can designate many things (Paradise Lost, a soup can label, a traic sign), so can reading. First, what kind of reading materials do we have in mind? Here I will be referring to extended, connected texts like
© 2013 Naomi S. Baron
PMLA 128.1 (2013), published by the Modern Language Association of America

NAOMI S. BARON, professor of linguistics and executive director of the Center for Teaching, Research, and Learning at American University in Washington, DC, is a former Guggenheim fellow and Swedish Fulbright fellow. Her seven books include Alphabet to Email: How Written English Evolved and Where It's Heading (Routledge, 2000) and Always On: Language in an Online and Mobile World (Oxford UP, 2008), which won the EnglishSpeaking Union's Duke of Edinburgh English Language Book Award for 2008. She is studying the relationship between reading in print and reading on-screen.

193

194
the changing profession

Redefining Reading: The Impact of Digital Communication Media

[

PM L A

newspaper articles, novels, and religious works (rather than, e.g., news headlines or tweets). Next, how do we approach text? Do we read linearly (from start to inish), or do we seek out snippets (using a table of contents, an index, or the "Find" function for searching an online text)? Do we skim or engage in "deep" reading (Wolf and Barzillai)? Do we read quickly or slowly? Answers to these questions are oten shaped by the character of the text. Is the material familiar or new to the reader? How complex are the words, the syntax, and the concepts? Functionality is also a consideration in deining reading (reading for information, for conceptual understanding, for enjoyment, or to kill time), as is the physical medium (a scroll, a paperback, an iPhone). Reading is, of course, just half of literacy. he other half is writing. Historically, ability to read has not always implied ability to write--or vice versa (Baron, Alphabet 81­82). However, in modern times, when we speak of people being literate, we generally mean readers who possess at least some writing skills. hese skills may be used not only in producing original text but also in annotating what others have written or in copying out passages of existing texts. Writing has commonly accompanied the act of reading. Reader markings appear in all sorts of texts (Caxton Club; Jackson), from marginalia in the Gutenberg Bible to underlining in a literature professor's well-worn copy of a Jane Austen novel. Written accompaniment to this act of reading can ill various roles. he simplest is to make parts of a text prominent (by underlining, highlighting, or adding asterisks, lines, or squiggles). More-reflective responses are notes written in the margins or in an external location--a notebook or a computer file. For many contemporary readers, textual annotation remains integral to reading. User Perspectives on Reading A burgeoning literature is considering how using the Internet may be afecting cognitive

activity (e.g., Carr; Cull; Greenield; Small and Vorgan). he work draws evidence from a variety of sources, including neurological imaging and objective performance on cognitive tasks. Some studies directly address reading on-screen. Jakob Nielsen has reported that "people rarely read Web pages word by word." Investigators at University College London, examining how users interact with online research sites, concluded that "users are not reading online in the traditional sense; indeed there are signs that new forms of `reading' are emerging as users `power browse' horizontally through titles, contents pages and abstracts going for quick wins. It almost seems that they go online to avoid reading in the traditional sense" (Information Behaviour 10). While the graduate students studied by Ziming Liu turned irst to a library's onlineinformation resources or the Web for help in completing assignments, 80% of them "always" or "frequently" printed out electronic documents (586, 587). Professional writers such as Nicholson Baker, Alan Jacobs, and David Ulin have weighed in on how reading on digital media compares with reading in hard copy (the general consensus being that e-readers are somewhat clunky but may provide a platform for uninterrupted reading of longer texts). However, it is also important to hear the voices of ordinary readers, especially those of university students--the irst generation of "digital natives." herefore, in fall 2010 and fall 2011, I investigated users' perspectives on the difference between reading in hard copy and reading on digital platforms. Study 1 he initial study involved 82 undergraduates (aged eighteen to twenty-four) at a midsize private university in the United States. Using a questionnaire mounted on SurveyMonkey (a commercial online survey instrument), participants were asked such questions as

128.1

]

Naomi S. Baron

195
the changing profession

how oten ("most of the time," "sometimes," "occasionally," "never") they annotated their textbooks by highlighting or underlining words or making notes in the books. In addition, open- ended questions probed what students liked most (and least) about reading on-screen and in hard copy. Finally, subjects were invited to ofer further comments. We look here at the results most relevant to the present discussion.1
OWNERSHIP AND ANNOTATION OF TEXTBOOKS

then printed it out. When asked whether they were more likely to read an assigned article if it was available online or if they were handed a copy, 6% said "available online," while 56% chose "handed a copy." (The other 38% declared it made no diference.)
COGNITIVE AND USABILITY ISSUES

Several questions addressed students' relationships with their textbooks. We found that 61% of the students sold their textbooks at the end of the semester "most of the time," 51% were renting hard copies of some of their textbooks, and 48% indicated that they annotated their textbooks only "occasionally" or "never." Given that more than half the students were disposing of their books at the end of the semester, there is a certain logic to not annotating them.
PREFERRED MODE FOR READING

Another series of questions examined students' preferred platforms for reading diferent genres (such as serious noniction, light iction, newspapers), either for academic purposes or for pleasure. Except for academic journals and newspapers, students overwhelmingly preferred to read texts in hard copy rather than online. (Of relevance is that the university had recently moved its boundjournal collection of-site, necessitating reliance on electronic journals.)
DOING ASSIGNED READING

Two questions probed how students approached reading assignments. When asked which platform they preferred if academic reading materials were available online, 55% favored reading the assignment online. However, 39% indicated they usually printed out the material and then read it, while another 6% noted they read the material online and

he survey explored students' level of engagement with reading materials, along with cognitive outcomes. When asked whether they reread academic materials, 49% said "occasionally" and 10% said "never." These selfreports are consistent with the earlier inding that 48% only "occasionally" or "never" wrote in their textbooks, since annotations typically facilitate reviewing a text that has already been read. However, when asked if they were more likely to reread academic materials available in hard copy or on-screen, 66% selected hard copy, compared with only 24% who chose on-screen. (The other 10% indicated they didn't reread.) When students were asked how their memory of what they read on-screen compared with their memory of what they read in hard copy, 46% indicated the medium didn't matter, 51% reported they had better memory for material read in hard copy, and only 2% said they retained more of what they read on-screen. Even more telling were subjects' responses regarding multitasking: 90% replied they were likely to multitask when reading on-screen, compared with only 1% when reading in hard copy. (he other 9% said they were equally likely to multitask when reading in either mode.) Cognitive and pedagogical issues also surfaced in the open-ended questions about what subjects liked most (and least) about reading in each medium. When asked what they liked least about reading on-screen, 91% complained about something cognitive ("I get distracted," "I don't absorb as much"). Similarly, when asked what they liked most about reading in hard copy, 78% of the responses

196
the changing profession

Redefining Reading: The Impact of Digital Communication Media

[

PM L A

involved cognition or usability (it is "necessary for focus," "I can write in it").
RESOURCE ISSUES

In their "like most"/"like least" responses, some students volunteered sentiments relating either to ecological resources (what they liked least about reading in hard copy was that it "kills trees") or to money (by reading on-screen, you "don't have to pay to print stuf out"). However, in their comments at the end of the survey, several students acknowledged personal conlicts in choosing between reading platforms ("I prefer hard copies, but think they're bad for the environment," "I know it's a waste of paper, but I really prefer reading a physical book or article to reading it online").
PHYSICALITY OF THE BOOK

While many survey questions explored the physical venues in which respondents read on diferent platforms (standing at a bus stop, sitting in a cofee shop), I focus here on responses relating to cognition and to emotion, aesthetics, and physicality.
COGNITION

Subjects were asked which platform in the following pairs made it easier to concentrate on reading. he responses of the 203 students show clear trends, although only 26 students, or 13%, owned tablets, and 43 students, or 22%, owned e-readers: Hard copy Tablet computer Hard copy E-reader Tablet computer Desktop or laptop computer E-reader Desktop or laptop computer E-reader Tablet computer 90% 10% 67% 33% 68% 32% 78% 22% 64% 36%

Finally, 10% of the subjects indicated that what they liked most about reading in hard copy involved the physicality of books. Among their responses were "having a tangible copy of the text" and "tactile interaction with reading material." he desire for a tangible, tactile relationship with reading material is relected in the fact that many subjects printed out online materials, before or ater reading them. Study 2 The second study compared reading on a variety of digital devices, but it also asked about reading in hard copy by way of comparison. he study was intended to survey a broad spectrum of users (college age and up, from a variety of occupations), though most responses (203 of 296) were from eighteento-twenty-four-year-olds, 192 of whom were students. Ten other students were between twenty-five and thirty-five and 1 between thirty-six and fifty, making a total student population of 203. Using e-mail and Facebook, we recruited subjects to complete a SurveyMonkey questionnaire.2

The perceived advantage of hard copy (over tablet or e-reader) for concentration is not surprising, given results from the irst study indicating preference for reading in hard copy. he students' perceptions that they concentrate better when they read on a tablet than when they read on a desktop or laptop computer likely relect their risk of succumbing to easily accessible online temptations when they use traditional networked computers. (Two-thirds of the student tablet owners indicated they purchased the device primarily for reading.) E-readers, which at the time typically lacked general-purpose Internet connectivity, were seen as posing fewer challenges to concentration than traditional computers or tablets. Cognitive issues were also on the minds of subjects in the second study as they responded to the open-ended questions: what they liked

128.1

]

Naomi S. Baron

197
the changing profession

most (and least) about reading on mobile devices. Of the 231 subjects (from the larger pool of 296) who responded to the open- ended questions (most of whom were students), 10% indicated that what they liked least related to cognitive focus, critical thinking, or retention. For example, one woman wrote, "[It's] harder to focus on a computer screen"; another female explained, "With my iPad, I can get distracted very easily by my ability to switch back and forth between the internet and the reading." A man wrote that "it is hard to remember where u read things"; and another male maintained that when using an iPhone "I . . . am rushed and multitasking (reading, moving about, interpreting my surroundings), so I don't think I'm retaining content/narrative from my reading as well as when I'm better situated." With one exception, no one indicated a cognitive advantage to reading on-screen.
EMOTION, AESTHETICS, AND PHYSICALITY

tal platforms (particularly computers, iPod Touches, and mobile phones) but judged it easier to concentrate when reading on platforms that lacked Internet connectivity (hard copy, e-readers of the time the study was conducted). Like subjects in the irst group, many voiced preference for the physical aspects of books. his research captures only some of the issues afecting today's readers. However, by combining indings from the two studies with other discussions of reading, we can begin to identify factors that may be shaping a deinition of what it means to read in the digital age. Factors Contributing to Redefining Reading
COGNITIVE AND PEDAGOGICAL ISSUES DERIVING FROM READING ON-SCREEN

his is the domain in which we would anticipate inding digital technology afecting what we mean by reading. Issues include
· Loss of concentration due to distraction (especially by other functions available through digital devices). he exception here may be e-readers that do not have general Internet connection (Jacobs, Pleasures 81, 82). · Exchanging linear reading for searching or skimming. In the memorable words of Joe O'Shea, a newly selected Rhodes scholar, "I don't read books per se. I go to Google and I can absorb relevant information quickly. Some of this comes from books. But sitting down and going through a book from cover to cover doesn't make sense. It's not a good use of my time as I can get all the information I need faster through the web" (qtd. in Jacobs, Pleasures 72). he "Find" function in online reading has created a new culture of what elsewhere I have called "snippet literacy" (Always On 204­06). · Shrinking expectations about reasonable text length. he proliferation of short textual formats (instant messages, text messages, Twitter, mobile apps for news outlets, and even mobile-phone novels [Goodyear]) correlates with perceptions that we lack the leisure time for longer reading (Griswold,

Another 10% of the open-ended responses regarding what subjects liked least about reading on-screen related to emotional or aesthetic issues or to the physicality of books. One man wrote, "You feel disconnected from the work, and they do not smell of rich leather binding"; another cited "lack of tangibility"; and a female said, "I just like the feel of books." Similarly, the female respondent quoted in this essay's epigraph complained that a mobile reading device is "not a book" and that since it's plugged into the Internet, "you can't concentrate." The Future of Reading Our two studies of user perspectives regarding reading platforms present a nuanced portrait. he irst study revealed that while many students are not keeping their books (or doing much annotating), most nonetheless prefer reading in hard copy and perceive such reading to have cognitive advantages over reading on-screen. In the second study, students reported doing substantial reading on digi-

198
the changing profession

Redefining Reading: The Impact of Digital Communication Media

[

PM L A

McDonnell, and Wright) and with university faculty members' growing tendency to assign short online readings (chapters, articles) in lieu of entire books. · Assumption that reading should include instant access to other resources. Platforms with Internet access enable users, in the act of reading, to draw relevant information from other sources. As a subject in the second study reported, what she liked most about reading on a mobile device was being able to "toggle between my book and my foreign language dictionaries easily."
LOSING THE PHYSICALITY OF THE BOOK

A second--and less anticipated--impact of digital technology on what it means to read involves the physicality of the printed book. Readers expressed a variety of concerns leading us to infer that physicality has been part of their notion of reading:
· Emotions and aesthetics. Subjects in the second study complained that with digital media "[y]ou lose the sense of being inside the book when you read anything but a hard copy." hey also spoke of the tactile and olfactory relationship they had with books ("I like turning pages. It's satisfying"; "he disadvantage [of a Kindle or iPhone] is that it doesn't have the feel and smell of a book"). · Navigation in a physical book. Several research participants noted enjoying the process of physically navigating in a book. In the second study, one commented that reading on a mobile device was "not as satisfying as getting a chunk of a book done," while another objected that with mobile readers it was not possible to see how far along in the book you were. A third subject mentioned the importance of seeing the book's cover: "[I] don't remember the book title/author as well [with a Kindle] because [I] don't see as oten . . . [the] cover of [the] book." And a fourth missed being able to easily lip back and forth in the text to ind a particular passage.

· Reading by eye and hand. Annotation of digital texts is still fairly cumbersome, though sure to improve over time. Many readers commented that (at least at the time of the studies) only physical books permitted convenient annotation. · he social function of physical books. Printed books (and what we write in them) can serve as the basis for social interaction. Undoubtedly, networked computing has enabled us to easily exchange newspaper articles, Web site URLs, and the like with one another. But, as one subject commented, you can't lend e-books. With digital materials, you also can't share in the reading experience of someone who annotated a text you come upon in a library or used bookstore or to whom you lent a book in which the borrower then wrote. (Samuel Coleridge was notorious--and oten appreciated--for annotating borrowed books.)3 · Physical jog to memory. Browsing the shelves of a physical library (personal or public) can prod one's memory--of books enjoyed in the past, of forgotten titles that now prove relevant. While one can scan titles in an electronic ile, this is a very diferent physical (and perhaps cognitive) experience.

Conclusions The meaning of reading has not remained static. Some changes have involved shits in individual or social practice, such as the transition from reading aloud to reading silently (Saenger, Space) or from reading to others to reading by oneself (Manguel). Other changes relect the emergence of navigational tools, including tables of contents and indexes. Printing encouraged standardization of abbreviations and replacement of hard-to-read cursive hand with more-legible print (both of which made for faster reading), along with the proliferation of page numbers, increasingly used for cross-referencing within a text (Saenger, "Impact"). Other changes in reading relected technological developments unrelated to print. In

128.1

]

Naomi S. Baron

199
the changing profession

the eighteenth century the increased afordability of windows in middle-class residences made reading physically easier (Watt). The growth of railroads in the nineteenth century resulted in long journeys on which reading was a good pastime (Altick). Literacy rates in the West soared in modern times. So, too, did the amount people read. he nineteenthcentury British triple-decker novel epitomized the "long read." However, a chorus of voices is now concluding that long-form reading, slow and careful reading, and even regular opportunities to read for enjoyment are not reasonable expectations for a wide swath of educated people (Griswold, McDonnell, and Wright; Jacobs, "We"; Steiner). Rather, it is more appropriate to think in terms of a "self-perpetuating minority that we shall call the reading class" (Griswold, McDonnell, and Wright 138). Mass education has led many people to read but not necessarily to the cognitively focused, physically grounded reading with which dedicated readers (including many subjects in the studies cited above) tend to identify. Perhaps, as Alan Jacobs argues, we should recognize that "the whole environment of school is simply alien to what longform reading has been for almost all of its history. . . . Education is and should be primarily about intellectual navigation, about . . . skimming well, and reading carefully for information in order to upload content. Slow and patient reading, by contrast, properly belongs to our leisure hours" ("We"). Given the paucity of leisure, we might anticipate that the number of people engaging in "[s]low and patient reading" will be small. In evaluating the impact of digital technologies on what it means to read, we need to be mindful that just as the notion of reading is complex, so are the ways in which we must think about reading platforms. First, we must not mistake nostalgia for substance. While a number of subjects in my studies saw the feel and smell of books as vital to the reading experience, people a century ago may have dis-

played parallel sentiments about their horses and travel. Moreover, some might argue that young adults are not suiciently "digitally native," given their extensive experience with print media, or that the studies did not use random sampling. Second, digital-reading technologies are still new. It took more than a decade for e-readers to penetrate the market, partly because earlier technologies were not user-friendly. Given the rapid appearance of new functionalities on mobile media, it is foolhardy to assume that future digital-reading platforms cannot overcome some of the shortcomings that current readers identiied. hird, each information and communication technology has its own afordances--that is, functions for which it is particularly well suited (Sellen and Harper). Besides praising mobile devices for being lightweight and able to contain dozens of books, our subjects commented on the ease with which they could access materials related to their reading (e.g., dictionaries or Web pages), as well as on the fact that nonnetworked devices such as e-readers of the time facilitated concentrating on just reading. These issues notwithstanding, the fact remains that digital-reading platforms support some functions of reading more than others. Such platforms are useful when one is reading for information and to ill in small gaps of free time. In an earlier cross-cultural study of mobile-phone use, I found university students oten used their phones to kill time ("Attitudes"). With the explosive development of apps on iPhones and iPads, killing time by reading snippets on mobile media has become yet more pervasive. It is less clear that reading on digital platforms is useful in reading for deeper understanding. he odds seem stacked against this function, given diiculties in annotation, a reader's tendency not to reread or remember digital text (compared with hard copy), and the overwhelming likelihood that people reading on digital devices will be multitasking, thereby dividing their attention.

200
the changing profession

Redefining Reading: The Impact of Digital Communication Media

[

PM L A

Taken together, all the factors we have been talking about point to a notion of reading that structurally privileges locating information over deciphering and analyzing more- complex text. This structural bent becomes increasingly important in planning educational curricula as the number of online courses (along with online readings) skyrockets and as readers flock to e-books because they are nearly always less expensive than their print counterparts. We must not let pedagogical and economic pressures cause us to lose sight of the question of whether a new notion of reading is emerging, in which deep and sustained reading (for work or pleasure) runs second to information gathering and short-term distraction.

NOTES
1. For more details on the study, see Baron, "Reading." I thank Michal Panner for assistance in data analysis. 2. I am grateful to the students in my fall 2011 university honors colloquium for recruiting subjects and to Assen Assenov and his staf for assisting in data analysis. 3. Since the time this paper was completed, a number of e-book manufacturers and distributors have introduced functions for lending e-books and for sharing annotations.

WORKS CITED
Altick, Richard D. he English Common Reader: A Social History of the Mass Reading Public, 1800­1900. Chicago: U of Chicago P, 1957. Print. Baker, Nicholson. "A New Page: Can the Kindle Really Improve on the Book?" he New Yorker. Condé Nast, 3 Aug. 2009. Web. 27 Dec. 2011. Baron, Naomi S. Alphabet to Email: How Written English Evolved and Where It's Heading. London: Routledge, 2000. Print. ------. Always On: Language in an Online and Mobile World. New York: Oxford UP, 2008. Print. ------. "Attitudes towards Mobile Phones: A CrossCultural Comparison." Cultures of Participation. Ed. Hajo Greif, Larissa Hjorth, Amparo Lasen, and Claire Lobet. Frankfurt: Lang, 2011. 77­94. Print. -- -- --. "Reading in Print versus Onscreen: Better, Worse, or About the Same?" Discourse 2.0: Language and

New Media. Ed. Deborah Tannen and Anna Marie Trester. Washington: Georgetown UP, forthcoming. Carr, Nicholas. he Shallows: What the Internet Is Doing to Our Brains. New York: Norton, 2010. Print. Caxton Club. Other People's Books: Association Copies and the Stories hey Tell. Introd. G. homas Tanselle. New Castle: Oak Knoll, 2011. Print. Cull, Barry W. "Reading Revolutions: Online Digital Text and Implications for Reading in Academe." First Monday 16.6 (2011): n. pag. Web. 26 Dec. 2011. Goodyear, Dana. "Letters from Japan: I  Novels." New Yorker 22 Dec. 2008: 22­29. Print. Greenield, Susan. "Does the Mind Have a Future?" Oxford Internet Institute. U of Oxford, 7 Apr. 2011. Web. 26 Dec. 2011. Griswold, Wendy, Terry McDonnell, and Nathan Wright. "Readers and Reading in the Twenty-First Century." Annual Review of Sociology 31 (2005): 127­41. Print. Information Behaviour of the Researcher of the Future: A Ciber Brieing Paper. JISC. Joint Information Systems Committee, 11 Jan. 2008. Web. 26 Dec. 2011. Jackson, H. J. Marginalia: Readers Writing in Books. New Haven: Yale UP, 2002. Print. Jacobs, Alan. he Pleasures of Reading in an Age of Distraction. New York: Oxford UP, 2011. Print. ------. "We Can't Teach Students to Love Reading." he Chronicle Review. Chronicle of Higher Educ., 31 July 2011. Web. 26 Dec. 2011. Liu, Ziming. "Print vs. Electronic Resources: A Study of User Perceptions, Preferences, and Use." Information Processing and Management 42.2 (2006): 583­92. Print. Manguel, Alberto. A History of Reading. New York: Viking, 1996. Print. Nielsen, Jakob. "How Users Read on the Web." Useit.com. N.p., 1 Oct. 1997. Web. 26 Dec. 2011. Saenger, Paul. "he Impact of the Early Printed Page on the History of Reading." he History of the Book in the West, 1455­1700. Ed. Ian Gadd. Vol. 2. Surrey: Ashgate, 2010. 385­449. Print. -- -- --. Space between Words: he Origin of Silent Reading. Stanford: Stanford UP, 1997. Print. Sellen, Abigail, and Richard Harper. he Myth of the Paperless Oice. Cambridge: MIT P, 2002. Print. Small, Gary W., and Gigi Vorgan. iBrain: Surviving the Technological Alteration of the Modern Mind . New York: Harper, 2008. Print. Steiner, George. "Ater the Book?" Visible Language 6.3 (1972): 197­210. Print. Ulin, David L. he Lost Art of Reading: Why Books Matter in a Distracted Time. Seattle: Sasquatch, 2010. Print. Watt, Ian. he Rise of the Novel: Studies in Defoe, Richardson, and Fielding. Berkeley: U of California P, 1957. Print. Wolf, Maryanne, and Mirit Barzillai. "he Importance of Deep Reading." Educational Leadership 66.6 (2009): 32­37. Print.

· SUBSCRIBE · RENEW · GIVE  A  GIFT · DIGITAL  EDITION Print | Close

Is  Google  Making  Us  Stupid?
W H AT T H E INT E R NE T IS DO ING T O O UR B R AINS

By  Nicholas  Carr Illustration by Guy Billout

"Dave, stop. Stop, will you? Stop, Dave. Will you stop, Dave?" So the supercomputer HAL pleads with the implacable astronaut Dave Bowman in a famous and weirdly poignant scene toward the end of Stanley Kubrick's 2001:  A  Space  Odyssey. Bowman, having nearly been sent to a deep-space death by the malfunctioning machine, is calmly, coldly disconnecting the memory circuits that control its artificial " brain. "Dave, my mind is going," HAL says, forlornly. "I can feel it. I can feel it." I can feel it, too. Over the past few years I've had an uncomfortable sense that someone, or something, has been tinkering with my brain, remapping the neural circuitry, reprogramming the memory. My mind isn't going--so far as I can tell--but it's changing. I'm not thinking the way I used to think. I can feel it most strongly when I'm reading. Immersing myself in a book or a lengthy article used to be easy. My mind would get caught up in the narrative or the turns of the argument, and I'd spend hours strolling through long stretches of prose. That's rarely the case anymore. Now my concentration often starts to drift after two or three pages. I get fidgety, lose the thread, begin looking for something else to do. I feel as if I'm always dragging my wayward brain back to the text. The deep reading that used to come naturally has become a struggle. I think I know what's going on. For more than a decade now, I've been spending a lot of time online, searching and surfing and sometimes adding to the great databases of the Internet. The Web has been

a godsend to me as a writer. Research that once required days in the stacks or periodical rooms of libraries can now be done in minutes. A few Google searches, some quick clicks on hyperlinks, and I've got the telltale fact or pithy quote I was after. Even when I'm not working, I'm as likely as not to be foraging in the Web's info-thickets'reading and writing e-mails, scanning headlines and blog posts, watching videos and listening to podcasts, or just tripping from link to link to link. (Unlike footnotes, to which they're sometimes likened, hyperlinks don't merely point to related works; they propel you toward them.) For me, as for others, the Net is becoming a universal medium, the conduit for most of the information that flows through my eyes and ears and into my mind. The advantages of having immediate access to such an incredibly rich store of information are many, and they've been widely described and duly applauded. "The perfect recall of silicon memory," Wired's Clive Thompson has written, "can be an enormous boon to thinking." But that boon comes at a price. As the media theorist Marshall McLuhan pointed out in the 1960s, media are not just passive channels of information. They supply the stuff of thought, but they also shape the process of thought. And what the Net seems to be doing is chipping away my capacity for concentration and contemplation. My mind now expects to take in information the way the Net distributes it: in a swiftly moving stream of particles. Once I was a scuba diver in the sea of words. Now I zip along the surface like a guy on a Jet Ski. I'm not the only one. When I mention my troubles with reading to friends and acquaintances--literary types, most of them--many say they're having similar experiences. The more they use the Web, the more they have to fight to stay focused on long pieces of writing. Some of the bloggers I follow have also begun mentioning the phenomenon. Scott Karp, who writes a blog about online media, recently confessed that he has stopped reading books altogether. "I was a lit major in college, and used to be [a] voracious book reader," he wrote. "What happened?" He speculates on the answer: "What if I do all my reading on the web not so much because the way I read has changed, i.e. I'm just seeking convenience, but because the way I THINK has changed?" Bruce Friedman, who blogs regularly about the use of computers in medicine, also has described how the Internet has altered his mental habits. "I now have almost totally lost the ability to read and absorb a longish article on the web or in print," he wrote earlier this year. A pathologist who has long been on the faculty of the University of Michigan Medical School, Friedman elaborated on his comment in a telephone conversation with me. His thinking, he said, has taken on a "staccato" quality, reflecting the way he quickly scans short passages of text from many sources online. "I can't read War  and  Peace   anymore," he admitted. "I've lost the ability to do that. Even a blog post of more than three or four paragraphs is too much to absorb. I skim it." Anecdotes alone don't prove much. And we still await the long-term neurological and psychological experiments that will provide a definitive picture of how Internet use affects cognition. But a recently published study of online research habits , conducted by scholars from University College London, suggests that we may well be in the midst of a sea change in the way we read and think. As part of the five-year research program, the scholars examined computer logs documenting the behavior of visitors to two popular research sites, one operated by the British Library and one by a U.K. educational consortium, that provide access to journal articles, e-books, and other sources of written information. They found that people using the sites exhibited "a form of skimming activity," hopping

from one source to another and rarely returning to any source they'd already visited. They typically read no more than one or two pages of an article or book before they would "bounce" out to another site. Sometimes they'd save a long article, but there's no evidence that they ever went back and actually read it. The authors of the study report: It is clear that users are not reading online in the traditional sense; indeed there are signs that new forms of "reading" are emerging as users "power browse" horizontally through titles, contents pages and abstracts going for quick wins. It almost seems that they go online to avoid reading in the traditional sense. Thanks to the ubiquity of text on the Internet, not to mention the popularity of text-messaging on cell phones, we may well be reading more today than we did in the 1970s or 1980s, when television was our medium of choice. But it's a different kind of reading, and behind it lies a different kind of thinking--perhaps even a new sense of the self. "We are not only what we read," says Maryanne Wolf, a developmental psychologist at Tufts University and the author of Proust  and  the  Squid:  The  Story and  Science  of  the  Reading  Brain. "We are how we read." Wolf worries that the style of reading promoted by the Net, a style that puts "efficiency" and "immediacy" above all else, may be weakening our capacity for the kind of deep reading that emerged when an earlier technology, the printing press, made long and complex works of prose commonplace. When we read online, she says, we tend to become "mere decoders of information." Our ability to interpret text, to make the rich mental connections that form when we read deeply and without distraction, remains largely disengaged. Reading, explains Wolf, is not an instinctive skill for human beings. It's not etched into our genes the way speech is. We have to teach our minds how to translate the symbolic characters we see into the language we understand. And the media or other technologies we use in learning and practicing the craft of reading play an important part in shaping the neural circuits inside our brains. Experiments demonstrate that readers of ideograms, such as the Chinese, develop a mental circuitry for reading that is very different from the circuitry found in those of us whose written language employs an alphabet. The variations extend across many regions of the brain, including those that govern such essential cognitive functions as memory and the interpretation of visual and auditory stimuli. We can expect as well that the circuits woven by our use of the Net will be different from those woven by our reading of books and other printed works. Sometime in 1882, Friedrich Nietzsche bought a typewriter--a Malling-Hansen Writing Ball, to be precise. His vision was failing, and keeping his eyes focused on a page had become exhausting and painful, often bringing on crushing headaches. He had been forced to curtail his writing, and he feared that he would soon have to give it up. The typewriter rescued him, at least for a time. Once he had mastered touch-typing, he was able to write with his eyes closed, using only the tips of his fingers. Words could once again flow from his mind to the page. But the machine had a subtler effect on his work. One of Nietzsche's friends, a composer, noticed a change in the style of his writing. His already terse prose had become even tighter, more telegraphic. "Perhaps you will through this instrument even take to a new idiom," the friend wrote in a letter, noting that, in his own work, his "`thoughts' in music and language often depend on the quality of pen and paper."

Also  see: Living With a Computer (July 1982) "The process works this way. When I sit down to write a letter or start the first draft of an article, I simply type on the keyboard and the words appear on the screen..." By James Fallows "You are right," Nietzsche replied, "our writing equipment takes part in the forming of our thoughts." Under the sway of the machine, writes the German media scholar Friedrich A. Kittler , Nietzsche's prose "changed from arguments to aphorisms, from thoughts to puns, from rhetoric to telegram style." The human brain is almost infinitely malleable. People used to think that our mental meshwork, the dense connections formed among the 100 billion or so neurons inside our skulls, was largely fixed by the time we reached adulthood. But brain researchers have discovered that that's not the case. James Olds, a professor of neuroscience who directs the Krasnow Institute for Advanced Study at George Mason University, says that even the adult mind "is very plastic." Nerve cells routinely break old connections and form new ones. "The brain," according to Olds, "has the ability to reprogram itself on the fly, altering the way it functions." As we use what the sociologist Daniel Bell has called our "intellectual technologies"--the tools that extend our mental rather than our physical capacities--we inevitably begin to take on the qualities of those technologies. The mechanical clock, which came into common use in the 14th century, provides a compelling example. In Technics  and  Civilization, the historian and cultural critic Lewis Mumford described how the clock "disassociated time from human events and helped create the belief in an independent world of mathematically measurable sequences." The "abstract framework of divided time" became "the point of reference for both action and thought." The clock's methodical ticking helped bring into being the scientific mind and the scientific man. But it also took something away. As the late MIT computer scientist Joseph Weizenbaum observed in his 1976 book, Computer  Power  and  Human  Reason:  From  Judgment  to  Calculation, the conception of the world that emerged from the widespread use of timekeeping instruments "remains an impoverished version of the older one, for it rests on a rejection of those direct experiences that formed the basis for, and indeed constituted, the old reality." In deciding when to eat, to work, to sleep, to rise, we stopped listening to our senses and started obeying the clock. The process of adapting to new intellectual technologies is reflected in the changing metaphors we use to explain ourselves to ourselves. When the mechanical clock arrived, people began thinking of their brains as operating "like clockwork." Today, in the age of software, we have come to think of them as operating "like computers." But the changes, neuroscience tells us, go much deeper than metaphor. Thanks to our brain's plasticity, the adaptation occurs also at a biological level. The Internet promises to have particularly far-reaching effects on cognition. In a paper published in 1936, the British mathematician Alan Turing proved that a digital computer, which at the time existed only as a theoretical machine, could be programmed to perform the function of any other information-processing device. And that's what we're seeing today. The Internet, an immeasurably powerful computing system, is subsuming most of our other intellectual technologies. It's becoming our map and our clock, our printing press and our typewriter, our calculator and our telephone, and

our radio and TV. When the Net absorbs a medium, that medium is re-created in the Net's image. It injects the medium's content with hyperlinks, blinking ads, and other digital gewgaws, and it surrounds the content with the content of all the other media it has absorbed. A new e-mail message, for instance, may announce its arrival as we're glancing over the latest headlines at a newspaper's site. The result is to scatter our attention and diffuse our concentration. The Net's influence doesn't end at the edges of a computer screen, either. As people's minds become attuned to the crazy quilt of Internet media, traditional media have to adapt to the audience's new expectations. Television programs add text crawls and pop-up ads, and magazines and newspapers shorten their articles, introduce capsule summaries, and crowd their pages with easy-to-browse infosnippets. When, in March of this year, TheNew  York  Times decided to devote the second and third pages of every edition to article abstracts , its design director, Tom Bodkin, explained that the "shortcuts" would give harried readers a quick "taste" of the day's news, sparing them the "less efficient" method of actually turning the pages and reading the articles. Old media have little choice but to play by the new-media rules. Never has a communications system played so many roles in our lives--or exerted such broad influence over our thoughts--as the Internet does today. Yet, for all that's been written about the Net, there's been little consideration of how, exactly, it's reprogramming us. The Net's intellectual ethic remains obscure. About the same time that Nietzsche started using his typewriter, an earnest young man named Frederick Winslow Taylor carried a stopwatch into the Midvale Steel plant in Philadelphia and began a historic series of experiments aimed at improving the efficiency of the plant's machinists. With the approval of Midvale's owners, he recruited a group of factory hands, set them to work on various metalworking machines, and recorded and timed their every movement as well as the operations of the machines. By breaking down every job into a sequence of small, discrete steps and then testing different ways of performing each one, Taylor created a set of precise instructions--an "algorithm," we might say today--for how each worker should work. Midvale's employees grumbled about the strict new regime, claiming that it turned them into little more than automatons, but the factory's productivity soared. More than a hundred years after the invention of the steam engine, the Industrial Revolution had at last found its philosophy and its philosopher. Taylor's tight industrial choreography--his "system," as he liked to call it--was embraced by manufacturers throughout the country and, in time, around the world. Seeking maximum speed, maximum efficiency, and maximum output, factory owners used time-and-motion studies to organize their work and configure the jobs of their workers. The goal, as Taylor defined it in his celebrated 1911 treatise, The  Principles  of  Scientific  Management, was to identify and adopt, for every job, the "one best method" of work and thereby to effect "the gradual substitution of science for rule of thumb throughout the mechanic arts." Once his system was applied to all acts of manual labor, Taylor assured his followers, it would bring about a restructuring not only of industry but of society, creating a utopia of perfect efficiency. "In the past the man has been first," he declared; "in the future the system must be first."

Taylor's system is still very much with us; it remains the ethic of industrial manufacturing. And now, thanks to the growing power that computer engineers and software coders wield over our intellectual lives, Taylor's ethic is beginning to govern the realm of the mind as well. The Internet is a machine designed for the efficient and automated collection, transmission, and manipulation of information, and its legions of programmers are intent on finding the "one best method"--the perfect algorithm-- to carry out every mental movement of what we've come to describe as "knowledge work." Google's headquarters, in Mountain View, California--the Googleplex--is the Internet's high church, and the religion practiced inside its walls is Taylorism. Google, says its chief executive, Eric Schmidt, is "a company that's founded around the science of measurement," and it is striving to "systematize everything" it does. Drawing on the terabytes of behavioral data it collects through its search engine and other sites, it carries out thousands of experiments a day, according to the Harvard  Business Review, and it uses the results to refine the algorithms that increasingly control how people find information and extract meaning from it. What Taylor did for the work of the hand, Google is doing for the work of the mind. The company has declared that its mission is "to organize the world's information and make it universally accessible and useful." It seeks to develop "the perfect search engine," which it defines as something that "understands exactly what you mean and gives you back exactly what you want." In Google's view, information is a kind of commodity, a utilitarian resource that can be mined and processed with industrial efficiency. The more pieces of information we can "access" and the faster we can extract their gist, the more productive we become as thinkers. Where does it end? Sergey Brin and Larry Page, the gifted young men who founded Google while pursuing doctoral degrees in computer science at Stanford, speak frequently of their desire to turn their search engine into an artificial intelligence, a HAL-like machine that might be connected directly to our brains. "The ultimate search engine is something as smart as people--or smarter," Page said in a speech a few years back. "For us, working on search is a way to work on artificial intelligence." In a 2004 interview with Newsweek, Brin said, "Certainly if you had all the world's information directly attached to your brain, or an artificial brain that was smarter than your brain, you'd be better off." Last year, Page told a convention of scientists that Google is "really trying to build artificial intelligence and to do it on a large scale." Such an ambition is a natural one, even an admirable one, for a pair of math whizzes with vast quantities of cash at their disposal and a small army of computer scientists in their employ. A fundamentally scientific enterprise, Google is motivated by a desire to use technology, in Eric Schmidt's words, "to solve problems that have never been solved before," and artificial intelligence is the hardest problem out there. Why wouldn't Brin and Page want to be the ones to crack it? Still, their easy assumption that we'd all "be better off" if our brains were supplemented, or even replaced, by an artificial intelligence is unsettling. It suggests a belief that intelligence is the output of a mechanical process, a series of discrete steps that can be isolated, measured, and optimized. In Google's world, the world we enter when we go online, there's little place for the fuzziness of contemplation. Ambiguity is not an opening for insight but a bug to be fixed. The human brain is just an outdated computer that needs a faster processor and a bigger hard drive.

The idea that our minds should operate as high-speed data-processing machines is not only built into the workings of the Internet, it is the network's reigning business model as well. The faster we surf across the Web--the more links we click and pages we view--the more opportunities Google and other companies gain to collect information about us and to feed us advertisements. Most of the proprietors of the commercial Internet have a financial stake in collecting the crumbs of data we leave behind as we flit from link to link--the more crumbs, the better. The last thing these companies want is to encourage leisurely reading or slow, concentrated thought. It's in their economic interest to drive us to distraction. Maybe I'm just a worrywart. Just as there's a tendency to glorify technological progress, there's a countertendency to expect the worst of every new tool or machine. In Plato's Phaedrus, Socrates bemoaned the development of writing. He feared that, as people came to rely on the written word as a substitute for the knowledge they used to carry inside their heads, they would, in the words of one of the dialogue's characters, "cease to exercise their memory and become forgetful." And because they would be able to "receive a quantity of information without proper instruction," they would "be thought very knowledgeable when they are for the most part quite ignorant." They would be "filled with the conceit of wisdom instead of real wisdom." Socrates wasn't wrong--the new technology did often have the effects he feared--but he was shortsighted. He couldn't foresee the many ways that writing and reading would serve to spread information, spur fresh ideas, and expand human knowledge (if not wisdom). The arrival of Gutenberg's printing press, in the 15th century, set off another round of teeth gnashing. The Italian humanist Hieronimo Squarciafico worried that the easy availability of books would lead to intellectual laziness, making men "less studious" and weakening their minds. Others argued that cheaply printed books and broadsheets would undermine religious authority, demean the work of scholars and scribes, and spread sedition and debauchery. As New York University professor Clay Shirky notes, "Most of the arguments made against the printing press were correct, even prescient." But, again, the doomsayers were unable to imagine the myriad blessings that the printed word would deliver. So, yes, you should be skeptical of my skepticism. Perhaps those who dismiss critics of the Internet as Luddites or nostalgists will be proved correct, and from our hyperactive, data-stoked minds will spring a golden age of intellectual discovery and universal wisdom. Then again, the Net isn't the alphabet, and although it may replace the printing press, it produces something altogether different. The kind of deep reading that a sequence of printed pages promotes is valuable not just for the knowledge we acquire from the author's words but for the intellectual vibrations those words set off within our own minds. In the quiet spaces opened up by the sustained, undistracted reading of a book, or by any other act of contemplation, for that matter, we make our own associations, draw our own inferences and analogies, foster our own ideas. Deep reading, as Maryanne Wolf argues, is indistinguishable from deep thinking. If we lose those quiet spaces, or fill them up with "content," we will sacrifice something important not only in our selves but in our culture. In a recent essay, the playwright Richard Foreman eloquently described what's at stake:

I come from a tradition of Western culture, in which the ideal (my ideal) was the complex, dense and "cathedral-like" structure of the highly educated and articulate personality--a man or woman who carried inside themselves a personally constructed and unique version of the entire heritage of the West. [But now] I see within us all (myself included) the replacement of complex inner density with a new kind of self--evolving under the pressure of information overload and the technology of the "instantly available." As we are drained of our "inner repertory of dense cultural inheritance," Foreman concluded, we risk turning into "`pancake people'--spread wide and thin as we connect with that vast network of information accessed by the mere touch of a button." I'm haunted by that scene in 2001. What makes it so poignant, and so weird, is the computer's emotional response to the disassembly of its mind: its despair as one circuit after another goes dark, its childlike pleading with the astronaut--"I can feel it. I can feel it. I'm afraid"--and its final reversion to what can only be called a state of innocence. HAL's outpouring of feeling contrasts with the emotionlessness that characterizes the human figures in the film, who go about their business with an almost robotic efficiency. Their thoughts and actions feel scripted, as if they're following the steps of an algorithm. In the world of 2001, people have become so machinelike that the most human character turns out to be a machine. That's the essence of Kubrick's dark prophecy: as we come to rely on computers to mediate our understanding of the world, it is our own intelligence that flattens into artificial intelligence. This article available online at: http://www.theatlantic.com/magazine/archive/2008/07/is-google-making-us-stupid/306868/ Copyright © 2014 by The Atlantic Monthly Group. All Rights Reserved.

· SUBSCRIBE · RENEW · GIVE  A  GIFT · DIGITAL  EDITION Print | Close

Books  on  Paper  Fight  Analog Distractions
By  Alexis  C.  Madrigal We're  worried.  With  all  of  the  things  in  the  physical  world  --  parks  and  baseball,  cars  and  cats,  food and  drink,  duvet  covers  and  lamps  --  how  will  anyone  get  any  reading  done?

Can you concentrate on Flaubert when your cute cat is only a few feet away, or give your true devotion to Mr. Darcy when people are swimming in a pool nearby? People who read books on paper are realizing that while they really want to be reading Dostoyevsky, the real world around them is pretty distracting with all of its opportunities for interacting with people, buying things in stores, and drinking coffee. The telephone lurks tantalizingly in reach. Looking up a tricky word or unknown fact in the book is easily accomplished through yelling loudly across the room to someone who might know the answer. And some of the millions of people who have ever picked up a book only to put it back down again a few minutes have come away with the conclusion: It's hard to sit down and focus on reading. If these paragraphs strike you as silly, I agree with you. Yet, they are a nearly word for word substitution of the latest anti-e-book story in today's New York Times. "Can you concentrate on

Flaubert when Facebook is only a swipe away, or give your true devotion to Mr. Darcy while Twitter beckons?" goes the lede. This story, like many of its ilk, suffers from a baseline problem: how often were people getting distracted before and how do we know things are worse now? We notice when we are distracted by some newfangled app on a Kindle Fire. We do not notice when we are distracted by vacuuming the rug or going to watch television or daydreaming. Having read thousands of books in my day -- on paper -- I can assure you that the phenomenon of getting distracted while reading a book is not limited to e-books. So, then the question becomes: does the user interface of the book matter so much? That is to say, does the paper book provide such a different reading experience as to tilt people ineluctably towards distraction? First, the article provides no evidence aside from some people saying so: "It's like trying to cook when there are little children around," said David Myers, 53, a systems administrator in Atlanta, who got a Kindle Fire tablet in December. "A child might do something silly and you've got to stop cooking and fix the problem and then return to cooking." ... Or it's like trying to read when there are children around, regardless of format! Second, the Times does all of its analysis on what's happening in your hand, but reading is an embodied experience. We're *in the world* while we read, and there have always, always, always been distractions in physical space. If the e-reader engages you more with the thing in your hand, even though the gadget itself is more distracting, that could be a net distraction win. Third, let's assume the worst, that tablets are "temptresses" as one Times source puts it. OK. But don't we think that people will develop strategies to resist these temptations? What do we think going to the library when you need to study is? Humans respond to the novel technologies they encounter to reshape their experiences of them. If distraction is really bothering all these people, and they really want to read books, then they will find a way to do so. Image:  Shutterstock/Zoom  Team This article available online at: http://www.theatlantic.com/technology/archive/2012/03/books-on-paper-fight-analogdistractions/254049/ Copyright © 2014 by The Atlantic Monthly Group. All Rights Reserved.

Google Effects on Memory: Cognitive Consequences of Having Information at Our Fingertips Betsy Sparrow et al. Science 333, 776 (2011); DOI: 10.1126/science.1207745

This copy is for your personal, non-commercial use only.

Permission to republish or repurpose articles or portions of articles can be obtained by following the guidelines here. The following resources related to this article are available online at www.sciencemag.org (this information is current as of January 15, 2014 ): Updated information and services, including high-resolution figures, can be found in the online version of this article at: http://www.sciencemag.org/content/333/6043/776.full.html Supporting Online Material can be found at: http://www.sciencemag.org/content/suppl/2011/07/13/science.1207745.DC1.html http://www.sciencemag.org/content/suppl/2011/07/14/science.1207745.DC2.html A list of selected additional articles on the Science Web sites related to this article can be found at: http://www.sciencemag.org/content/333/6043/776.full.html#related This article has been cited by 7 articles hosted by HighWire Press; see: http://www.sciencemag.org/content/333/6043/776.full.html#related-urls This article appears in the following subject collections: Psychology http://www.sciencemag.org/cgi/collection/psychology

Science (print ISSN 0036-8075; online ISSN 1095-9203) is published weekly, except the last week in December, by the American Association for the Advancement of Science, 1200 New York Avenue NW, Washington, DC 20005. Copyright 2011 by the American Association for the Advancement of Science; all rights reserved. The title Science is a registered trademark of AAAS.

Downloaded from www.sciencemag.org on January 15, 2014

If you wish to distribute this article to others, you can order high-quality copies for your colleagues, clients, or customers by clicking here.

REPORTS
Acknowledgments: The present study was funded by NIH grants RO1 MH086563 to W.A.S. and Y.N. and RO1 MH058847 to W.A.S. We thank E. Wang, A. Shang, and N. Nystrom for expert animal care and E. Hargreaves, M. Yanike, and M. Shapiro for helpful comments. The authors declare no competing financial interests. Y.N. and W.A.S. designed the experiments and wrote the manuscript. Y.N. performed the experiment and analyzed the data. SOM Text Figs. S1 to S7 Tables S1 to S5 References (24­35) 11 April 2011; accepted 21 June 2011 10.1126/science.1206773

Supporting Online Material
www.sciencemag.org/cgi/content/full/333/6043/773/DC1 Materials and Methods

Google Effects on Memory: Cognitive Consequences of Having Information at Our Fingertips
Betsy Sparrow,1* Jenny Liu,2 Daniel M. Wegner3 The advent of the Internet, with sophisticated algorithmic search engines, has made accessing information as easy as lifting a finger. No longer do we have to make costly efforts to find the things we want. We can "Google" the old classmate, find articles online, or look up the actor who was on the tip of our tongue. The results of four studies suggest that when faced with difficult questions, people are primed to think about computers and that when people expect to have future access to information, they have lower rates of recall of the information itself and enhanced recall instead for where to access it. The Internet has become a primary form of external or transactive memory, where information is stored collectively outside ourselves. n a development that would have seemed extraordinary just over a decade ago, many of us have constant access to information. If we need to find out the score of a ball game, learn how to perform a complicated statistical test, or simply remember the name of the actress in the classic movie we are viewing, we need only turn to our laptops, tablets, or smartphones and we can find the answers immediately. It has become so commonplace to look up the answer to any question the moment it occurs that it can feel like going through withdrawal when we can't find out something immediately. We are seldom offline unless by choice, and it is hard to remember how we found information before the Internet became a ubiquitous presence in our lives. The Internet, with its search engines such as Google and databases such as IMDB and the information stored there, has become an external memory source that we can access at any time. Storing information externally is nothing particularly novel, even before the advent of computers. In any long-term relationship, a team work environment, or other ongoing group, people typically develop a group or transactive memory (1), a combination of memory stores held directly by individuals and the memory stores they can access because they know someone who knows that information. Like linked computers that can address each other's memories,

I

1 Department of Psychology, Columbia University, 1190 Amsterdam Avenue, New York, NY 10027, USA. 2Department of Psychology, University of Wisconsin­Madison, 1202 West Johnson Street, Madison, WI 53706, USA. 3Department of Psychology, Harvard University, 33 Kirkland Street, Cambridge, MA 02138, USA.

*To whom correspondence should be addressed. E-mail: sparrow@psych.columbia.edu

people in dyads or groups form transactive memory systems (2, 3). The present research explores whether having online access to search engines, databases, and the like, has become a primary transactive memory source in itself. We investigate whether the Internet has become an external memory system that is primed by the need to acquire information. If asked the question whether there are any countries with only one color in their flag, for example, do we think about flags or immediately think to go online to find out? Our research then tested whether, once information has been accessed, our internal encoding is increased for where the information is to be found rather than for the information itself. In experiment 1, participants were tested in two within-subject conditions (4). Participants answered either easy or hard yes/no trivia questions in two blocks. Each block was followed by a modified Stroop task (a colornaming task with words presented in either blue or red) to test reaction times to matched computer and noncomputer terms (including general and brand names for both word groups). People who have been disposed to think about a certain topic typically show slowed reaction times (RTs) for naming the color of the word when the word itself is of interest and is more accessible, because the word captures attention and interferes with the fastest possible color naming. Paired within-subject t tests were conducted on color-naming reaction times to computer and general words after the easy and difficult question blocks. Confirming our hypothesis, computer words were more accessible [color-naming RT mean (M) = 712 ms, SD = 413 ms] than general words (M = 591 ms, SD = 204 ms) after VOL 333 SCIENCE

participants had encountered a series of questions to which they did not know the answers, t(68) = 3.26, P < 0.003, two-tailed. It seems that when we are faced with a gap in our knowledge, we are primed to turn to the computer to rectify the situation. Computer terms also interfered somewhat more with color naming (M = 603 ms, SD = 193 ms) than general terms (M = 559 ms, SD = 182 ms) after easy questions, t (68) = 2.98, P < 0.005, suggesting that the computer may be primed when the concept of knowledge in general is activated. Comparison using a repeated measures analysis of variance (ANOVA) of specific search engines (Google/Yahoo) and general consumergood brand names (Target/Nike) revealed an interaction with easy versus hard question blocks, F(1,66) = 5.02, P < 0.03, such that search engine brands after both easy questions (M = 638 ms, SD = 260 ms) and hard questions (M = 818 ms, SD = 517 ms) created more interference than general brands after easy questions (M = 584 ms, SD = 220 ms) and hard questions (M = 614 ms, SD = 226 ms) (Fig. 1). Simple effects tests showed that the interaction was driven by a significant increase in RT for the two search engine terms after the hard question block, F(1,66) = 4.44, P < 0.04 (Fig. 1). Although the concept of knowledge in general seems to prime thoughts of computers, even when answers are known, not knowing the answer to general-knowledge questions primes the need to search for the answer, and subsequently computer interference is particularly acute. In experiment 2, we tested whether people remembered information that they expected to have later access to--as they might with information they could look up online (4). Participants were tested in a 2 by 2 between-subject experiment by reading 40 memorable trivia statements of the type that one would look up online (both of the new information variety, e.g., "An ostrich's eye is bigger than its brain," and information that may be remembered generally, but not in specific detail, e.g., "The space shuttle Columbia disintegrated during re-entry over Texas in Feb. 2003."). They then typed them into the computer to ensure attention (and also to provide a more generous test of memory). Half the participants believed the computer would save what was typed; half believed the item would be erased. In addition, half of the participants in each of the saved and erased conditions were asked explicitly to try to remember the information. After the reading and typing task, participants wrote down as many of the statements as they could remember.

776

5 AUGUST 2011

www.sciencemag.org

REPORTS
A between-subjects 2 (saved or erased) by 2 (explicit memory instructions versus none) ANOVA revealed a significant main effect for only the saved/erased manipulation, as those who believed that the computer erased what they typed had the best recall, omnibus F(3, 56) = 2.80, P < 0.05 [Erase M = 0.31, SD = 0.04, and Erase Remember M = 0.29, SD = 0.07, paired comparisons of erased conditions not significant (ns)] compared with those who believed the computer would be their memory source (Save M = 0.22, SD = 0.07 and Save Remember M = 0.19, SD = 0.09, paired comparisons of saved conditions ns). This finding corresponds to previous work on directed forgetting, showing that when people don't believe they will need information for a later exam, they do not recall it at the same rate as when they do believe they will need it (5). Participants apparently did not make the effort to remember when they thought they could later look up the trivia statements they had read. Because search engines are continually available to us, we may often be in a state of not feeling we need to encode the information internally. When we need it, we will look it up. The main effect of the instruction to explicitly remember or not was not significant, which is similar to findings in the learning literature on intentional versus incidental studying of material, which generally finds that there is no difference of explicit instruction (6, 7). Participants were more affected by the cue that information would or would not be available to them later, regardless of whether they thought they would be tested on it. Fig. 1. Accessibility of brand names (as measured by color-naming reaction time) after blocks of easy or hard test items. Error bars, mean T SEM. In Experiment 3, we tested memory for where to find information that one might look up online. Participants again read and typed in items of memorable trivia, this time in three withinsubject conditions (4). For one-third of the questions, participants were shown "Your entry has been saved." For another one-third of the questions, participants were shown "Your entry has been saved into the folder X" (where X is one of six folders named FACTS, DATA, INFO, NAMES, ITEMS, or POINTS--generic interchangeable names to which the statements had previously been randomly assigned). For the final one-third of the questions, participants were shown "Your entry has been erased." Participants were given the expectation that they would have access to what they saved through a supposed "practice" trial in which they had access to the file folders during a "recall" task. Thus, generically saved, saved in a specific folder, and erased trials were created for all participants. Participants were then given a recognition task. They saw all 30 statements, half of which had been altered slightly (names or dates altered). Participants had to judge yes or no whether the statement they were now shown was exactly what they had read, whether the statement had been saved or erased, and finally, if the statement had been saved to a folder, which folder it had been saved into (they were given the folder names, and also had "no specific folder" and "erased" as answer options to this last question). Overall, in answer to the question "Was this statement exactly what you read?" participants recognized the accuracy of a large proportion of statements. But for those statements they believed had been erased, participants had the best memory (erase M = 0.93, SD = 0.09, pairwise comparisons to both saved conditions P < 0.05) compared with the statements participants believed they would continue to have access to (saved generically M = 0.88, SD = 0.12, and saved specifically to a folder M = 0.85, SD = 0.12, pairwise ns), repeated measures omnibus F (1, 27) = 4.01, P < 0.03. However, the opposite pattern was found for the question, "Was this statement saved or erased?" Participants accurately remembered what they had saved (saved generically M = 0.61, SD = 0.21, and saved into a folder M = 0.66, SD = 0.20, pairwise ns) more than they accurately remembered what they had erased (M = 0.51, SD = 0.19, pairwise comparisons with both saved conditions, P < 0.04), repeated measures ANOVA omnibus F (1, 27) = 5.34, P < 0.03. Thus, it appears that believing that one won't have access to the information in the future enhances memory for the information itself, whereas believing the information was saved externally enhances memory for the fact that the information could be accessed, at least in general. In this recognition task, when asked "If the information was saved, what folder was it saved into?" participants did remember more that the information was erased (M = 0.54, SD = 0.19, pairwise comparisons with both saved conditions, P < 0.001) than specifically whether the information was generically saved or which folder it was saved into (saved generically M = 0.30, SD = 0.20, and saved into a specific folder M = 0.23, SD = 0.14, pairwise comparisons ns), repeated-measures ANOVA omnibus F (1,27) = 21.67, P < 0.001. This result is a reminder of the experience of remembering something you have read online that you would like to see again or share but no longer remembering where you saw it or what steps you took to find it in the first place, or even knowing that a file is saved onto your hard drive but having to use the search feature to find it. The fact that some of the statements were saved in a general folder was important to include to rule out increased memory demands in the two saved conditions but does not parallel the continuous access to information we experience with current technology, in that there is no nameless depository of leftover information we would check after searching the obvious places. In addition, recognition is not usually the task we are charged with when answering someone's question. We need to recall the information we have gathered. Experiment 4 was conducted to see if people would recall where to find information more than the information itself. All participants expected trivia statements that they read and then typed to be saved to a specific folder with a generic name ("FACTS," etc., as in the previous experiment, although in this case there were no practice trials and the names and number of

Fig. 2. An if/then analysis of memory for what the information is and where to find it. Scale is measured in proportion recalled. Error bars, mean T SEM.

www.sciencemag.org

SCIENCE

VOL 333

5 AUGUST 2011

777

REPORTS
folders was never explicitly called to the participants' attention) (4). Participants were then given a recall task, in which they were given 10 min to write down as many of the statements as they could remember. Participants finally were given an identifying feature of the statement that they read (and that had been saved), and they had to answer with the folder name in which it was saved. For example, for the statement "An ostrich's eye is bigger than its brain," the question would be "What folder was the statement about the ostrich saved in?" Participants had to type into a dialog box called "Items" to recall this particular folder correctly. Folder names were not mentioned again, past the original typing period, and participants were never explicitly told there were six folder names where the items were saved. Overall, participants recalled the places where the statements were kept (M = 0.49, SD = 0.26) better than they recalled the statements themselves (M = 0.23, SD = 0.14), between-subject t(31) = 6.70, P < 0.001 two-tailed. These results seem unexpected on the surface, given the memorable nature of the statements and the unmemorable nature of the folder names. Also, these recall results are notable in comparison with the dismal level of recognition of which folder the statement was saved into in Experiment 3. However, several caveats need to be mentioned. Participants did have a cue to memory (a word from the trivia statement) with the folder recall that the statements themselves did not have. We were not able to counterbalance the trivia and the folders trials such that the folders were as numerous as the statements, which would be necessary to counterbalance the uncued and cued recall tasks. However, if we look at the pattern of what was remembered, the results do suggest that "where" was prioritized in memory, with the advantage going to "where" when "what" was forgotten. You might expect that, with the advantages of cued recall, participants would most remember the folder where statements were saved if they were cued both by our question and by their recalling the statement in the first place. To examine this, an if/then analysis was conducted giving participants separate scores for whether they (i) recalled both the statement and the folder where it was saved, (ii) recalled the statement but not the folder, (iii) didn't recall the statement but recalled the folder, or (iv) recalled neither the statement nor the folder. Participants were particularly poor at recalling both statement and folder (M = 0.17, SD = 0.16) and recalling the statement but not the folder (M = 0.11, SD = 0.08, pairwise comparison, ns). They were significantly more likely to recall nothing (M = 0.38, SD = 0.24), but surprisingly equally likely to recall the folder, when they didn't recall the statement (M = 0.30, SD = 0.16, pairwise ns), repeated measures ANOVA omnibus F (1, 31) = 11.57, P < 0.003 (Fig. 2). It would seem from this pattern that people don't remember "where" when they know "what" but do remember where to find the information when they don't recall it. This is preliminary evidence that when people expect information to remain continuously available (such as we expect with Internet access), they are more likely to remember where to find it than to remember the details of the item. One could argue that this is an adaptive use of memory--to include the computer and online search engines as an external memory system that can be accessed at will. Relying on our computers and the information stored on the Internet for memory depends on several of the same transactive memory processes that underlie social information-sharing in general. These studies suggest that people share information easily because they rapidly think of computers when they find they need knowledge (experiment 1). The social form of information storage is also reflected in the findings that people forget items they think will be available externally and remember items they think will not be available (experiments 2 and 3). Transactive memory is also evident when people seem better able to remember where an item has been stored than the identity of the item itself (experiment 4). These results suggest that processes of human memory are adapting to the advent of new computing and communication technology. Just as we learn through transactive memory who knows what in our families and offices, we are learning what the computer "knows" and when we should attend to where we have stored information in our computerbased memories. We are becoming symbiotic with our computer tools (8), growing into interconnected systems that remember less by knowing information than by knowing where the information can be found. This gives us the advantage of access to a vast range of information, although the disadvantages of being constantly "wired" are still being debated (9). It may be no more that nostalgia at this point, however, to wish we were less dependent on our gadgets. We have become dependent on them to the same degree we are dependent on all the knowledge we gain from our friends and co-workers--and lose if they are out of touch. The experience of losing our Internet connection becomes more and more like losing a friend. We must remain plugged in to know what Google knows.
References and Notes
1. D. M. Wegner, in Theories of Group Behavior, B. Mullen, G. R. Goethals, Eds. (Springer-Verlag, New York, 1986), pp. 185­208. 2. D. M. Wegner, Soc. Cogn. 13, 319 (1995). 3. V. Peltokorpi, Rev. Gen. Psychol. 12, 378 (2008). 4. Materials and methods are available as supporting material on Science Online. 5. R. A. Bjork, in Coding Processes in Human Memory, A.W. Melton, E. Martin, Eds. (Winston, Washington, DC, 1972), pp. 217­235. 6. F. I. Craik, E. Tulving, J. Exp. Psychol. Gen. 104, 268 (1975). 7. T. S. Hyde, J. J. Jenkins, J. Exp. Psychol. 82, 472 (1969). 8. A. Clark, Natural-Born Cyborgs: Minds, Technologies, and the Future of Human Intelligence (Oxford Univ. Press, New York, 2003). 9. N. Carr, Atlantic 302, 56 (2008). Acknowledgments: We acknowledge the financial support of Columbia University to B.S. and NSF grant BNS-0841746 to D.M.W. Raw data are available from B.S.

Supporting Online Material
www.sciencemag.org/cgi/content/full/science.1207745/DC1 Materials and Methods SOM Text References (10­13) 2 May 2011; accepted 27 June 2011 Published online 14 July 2011; 10.1126/science.1207745

778

5 AUGUST 2011

VOL 333

SCIENCE

www.sciencemag.org

+ transactive memory - there have always been sources of external memory, and categories of information, and you might not know what the exact information in the external memory is, but you know the kind or category of this information
+ how does media shape primary sources and public perception 
+ the internet is just expanding our transactive memories
+ people self-reporting is not accurate
+ digital natives - people who grew up with certain technologies
+ appearances and the actual truth differ - 
+ how did you decide to use certain technology things as symbols for transactive memory?
+ increasing control may make you more critical
+ objective self-awareness - thinking about you in the world and focusing in rather than focusing on the world
+ working memory
+ 