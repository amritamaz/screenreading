<!DOCTYPE html>
<html>

<head>
    <title>Amrita Mazumdar - Screen Reading Final Project</title>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="Amrita Mazumdar">
    <link rel="stylesheet" href="lib/normalize.min.css">
    <link rel="stylesheet" href="lib/styles.css">

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-29924178-1', 'amritamaz.me');
  ga('send', 'pageview');

</script>


</head>

<body>
    <div class="content">
    <header>
	<h1> screen reading</h1><br><h2>a text analysis <br>by amrita mazumdar</h2>
    </header>
    <section id="intro">
	<h1>Introduction</h1>
	<p> This project was born out of a desire to find our bearings within the texts. After a semester of learning, understanding, questioning what it means to read and write in the digital age, where are we? What have we taken from cultures and technologies past and what kinds of open problems are we not yet solving? Lofty goals for a 30% assignment, I know, but a more approachable synthesis of these questions still stands: what can we learn with new tools and a bunch of texts? what can we learn from them, from the tools and the books and the amalgamation of the two?</p>

	<p> What follows is a brief exploration of our semester, applying screen reading techniques to a syllabus of seminal texts on screen reading. We first explore the low-hanging (but still enlightening!) fruit of frequency counting, apply this analysis to our notion of keywords, and then consider the dichotomy between print and screen texts and its impact on this analysis. We also cover a brief overview of the tools used in this analysis, and their potential side efects on this discussion.</p>
    </section>

    <section id="freqs">
	<h1>Word Frequencies</h1>
	<p> When tasked with the question: what did we actually learn?, my first thought was to see what ideas appeared most frequently in the texts. The assumption was that by finding most common words, we could uncover the most common ideas.</p>

	<p>After compiling the database of texts in the syllabus, counting common words was simple enough. The result, however, was a list of words and their frequencies - not the most useful way to understand the results. I decided to filter out all stopwords (common words such as 'a', 'the', 'etc', etc.) and any parsing errors resulting in one- or two-letter words, then plot them as a percentage of the total word frequency (sum of all frequencies of all words). For clarity and sanity's sake, I only plotted the top 30 words.</p>
<div class="pics">
<img src="img/freq1.png"><br />
</div>
<p> Hmm.</p>
<p> Well, it appears 'new' is the most frequently-used word with 56% of the popular vote! This makes a decent amount of sense. In a semester of readings about the history and trends of book publishing and writing and reading, it would be logical to see the word 'new' used frequently. </p>
<p> More expected and understandable is the word 'book' coming in at second place. Considering most of the texts concern reading, writing, and everything in between, the book is certainly a logical second choice.</p>
<p class="aside">Aside: It is eerie how much frequency-ranked lists of words resemble minimalist sentences. "New book like also even read text first world." It would be an entertaining exercise to try to make a summary-generator by just filtering out stopwords and sorting by word frequency. Maybe a Twitter bot.</p>
<p> This analysis leans closer to disappointing with regards to its results (I probably could have told you 'also' was a popular word, and wonder why it is not considered a stopword...), but is also indicative of a larger frustration in digital analysis - not being stupid enough for a computer. The human act of skimming, picking out the important words and phrases and immediately gleaning the bits most salient and words most relevent is a task a computer cannot easily do, as my 5 hours and many broken programs can assure you. Moreover, the patterns you may eventually get may tell you less about your problem than you could have expected - what does it mean that 'world' is such a popular word? How does that fit into my preconceived understanding of the focus of the semester's readings? Was the class secretly focused on global texts and I missed it? Or is the use of 'world' meant to evoke how dramatically the state of the book is changing, that it digital technologies are increasingly universal in scope? Questions beget questions.

<div class="pics">
<img src="img/freq2.png" width="30%"> 
<img src="img/freq3.png"width="30%"> 
<img src="img/freq4.png"width="30%"></div> 

<p>
To confirm my process was correct, and to do a more micro analysis on some of the my perceived most definitive texts of the semester, I also graphed the word frequencies of Marshall McLuhan's 'Understanding Media', Jerome McGann's 'Radiant Textuality', and Franco Moretti's 'Graphs, Maps, and Trees'. On a smaller body of work, the most popular words turn out to be more indicative of a text than on the larger scale. What also stuck out to me was how infrequently a word needs to appear in a text to be considered frequent. Out of a 2,000 word essay on media, one only needs to mention the word 'media' 40 times to beat McLuhan for presence.  </p>

    </section>


    <section id="incommon">
	<h1>Keywords</h1>

<p>
A more fruitful analysis might be to use humna-generated words to conduct my analysis with. Conveniently, us students were put to task identifying and explicating a set of 'keywords' from the semester, and I used these pre-defined words to track how the keywords manifested themselves and interacted across the weeks' readings.
</p>

<div class="pics"><img src="img/keywords.png"></div>

<p> First, a discussion on how this graph was developed is in order. This was a topic of contention within myself - I envisioned it as a set of bar graphs with best fit lines squiggling around, showing the rise and fall of 'print' as 'interface' surges forward. <br /><br /> This was not the case.</p>

<div class="pics"><img src="img/keyword_bad.png"><br>(this is unhelpful)</div>

<p> So, another approach had to be taken. After experimenting with all the graph types available, I settled on a stacked area chart. According to Apple's Keynote documentation, "stacked area charts show the magnitude of change over time, and they display both individual values and the sum of all values in the chart." Perfect. On my charting tool, it insisted on adding up the percentages to 1000%, which I maintain is impossible. In any case, the y-axis indicates how popular the keyword was as a fraction of its most popular week. Weeks where the graph veers downward in all colors indicate that no keywords had a strong showing that week.</p>

<p> While the chart is fairly straightforward, there are a few limitations to this visualization. Because the areas represent fractions of a whole, the word 'colophon', for instance, barely appeared 6 times in its most frequent sources. This presence appears in as large of an area as 'book''s 60-word showing in its less popular texts. This proportionality problem proved too difficult to solve at this level of analysis, but is something to consider when understanding how words appear and are presented in visualizations.</p> <p> I also wondered about the peak and fall-off, and where that arose from. The answer, it turns out, has less to do with the content of the text but its manifestation (see: next section). </p>

    </section>


    <section id="keywords">
	<h1>Writing for Print and Screen</h1>

<p> What this chart had elucidated to me was a key assumption I had hidden from myself, even throughout and involved text parsing process that had me digging around through UNIX shell scripts and Python libraries to solve it for myself. 
</p>

<p style="text-align: center;font-style:italic;"><u>Fallacy</u><br>Texts produced for <b>screens</b> and texts produced for <b>print and then converted for screens</b> are <s>lexically equivalent</s>. <br /> <br><br>They're not.</p>

<div class="pics"><img src="img/print.png"></div>

<p>
As it turns out, my beloved OCR had failed me. It hadn't failed me, per se, but it had produced output I was not prepared to deal with. To comprehensively understand this problem, let me give a high-level overview of how Python does text parsing.
</p>
<p> Imagine you give OCR an input text, usually on an image or PDF page:</p>
<pre><code>Hello! My name is Wally.</code></pre>
<p>To identify the words in that string, OCR splits up the words based on punctuation and white space, like tabs and spaces. So our previous string would be split up into:</p>
<pre><code>[Hello][My][name][is][Wally]</code></pre>

<p>which looks great! This is usually how OCR finds words. <br><br></p>

<p>But sometimes, OCR is tasked to parse some garbled scans. Or the scanner had some dots on the screen which came up on your picture. Or just the image has text pretty close together, and the OCR doesn't know what to do with it. Then you might have some output that looks like:</p>


<pre><code>[Hello!.][Myname][i][s][W a l l y]</code></pre>

<p>Which isn't right, but that's what happens. It's unfortunately much more difficult to find and clear up these errors automatically, which is why accurate text detection in images is still an open area of research. It turns out these types of parsing issues are very common with scanned texts, and why electronically-produced PDFs are preferable for this type of analysis.  </p>

<p> Consequently, this led me to question how much garbled text might be influencing my results. I recalled how many steps of encoding conversion I had built into my workflow, how many warnings during text parsing I ignored for the sake of faster results, and realized there must be some loss of information in my data set that would influence my keywords analysis.</p>

<p> To investigate further, I peeked into a bit of Anna Friedburg's 'Virtual Window' dataset, to see what kinds of words were sitting at the bottom of the frequency barrel:</p> 

<pre><code>['verner', 1], ['vertical', 1], ['vertically', 1], ['veteran', 1], ['vfassage.', 1], ['vhich', 1], ['vhile', 1], ["vi'hile", 1], ['vi..slon', 1], ['vi..suality.', 1], ['vi.deo', 1], ['vi/lq', 1], ['vi11yl', 1], ['vicissitudes', 1], ['victim', 1], ['vicwu', 1], ['vid', 1], ['vidcu', 1], ['vide', 1], ['video-', 1], ['video-its', 1], ['video-liveness', 1], ['videot', 1], ['videotape', 1], ['vide\xa2', 1], ['vie.', 1], ['viei1.1s', 1], ['vieniog', 1], ['vietving', 1], ['viev..-', 1], ['viewer-each', 1], ['viewer.', 1], ['viewers.', 1], ['viewport', 1], ['vile', 1], ['villa', 1], ['vindow', 1], ['vindows', 1], ['viola', 1], ['violin', 1], ['vir-', 1], ['virilio', 1], ['virl\\1', 1], ['virmal', 1], ['virrues', 1], ['virruri', 1], ['virtual-reality', 1], ['virtual.', 1], ['virtualities.', 1], ['virtuality', 1], ['visi-on', 1], ['visibly', 1], ['visit.', 1], ['visl', 1], ['viso.al', 1], ['visoal', 1], ['vist1al', 1], ['visuaj', 1], ['visual.', 1], ['visualization', 1], ['visualjze', 1], ['visually', 1], ['vitascope', 1], ['vito', 1], ['viulti-wi.udow', 1], ['vivid', 1], ['vividly', 1], ['vi~', 1], ['vjhatever', 1], ['vjne', 1], ["vlfhirlwind'sc", 1]</pre></code>

<p> A lot of words I would only use once, like 'virtuality', sure, but also a lot of clear OCR recognition errors and errors in parsing. It's not beyond the realm of possibility, for instance, for 'vindow' to be 'window', or 'vjhatever' to be 'whatever', or to even guess that 'viulti-wi.udow' might be the title of the book, 'virtual window'. But these things clearly tripped up the programs I set up, and this is just a handful of the set of low-frequency words in the letter 'v' of one text. In this way, print text pose a significant challenge in even a cursory textual analysis like the one I constructed. </p>

    </section>


    <section id="topics">
	<h1>Tools and Thoughts</h1>

<p> In writing this analysis piece, I took great efforts to document as much of my process and analysis as possible. Moreover, as we learned from Sparrow and Sculley,Pasanek, just as it is impossible for texts to exist without the medium within which they are presented, it is also impossible for a computational analysis to exist without influence from the tools used in its production. To that end, I present a comprehensive listing of all design and computational tools used. All materials, code, notes, etc. can be found on <a href="http://www.github.com/amritamaz/screenreading">the github repo</a>.</p>

<h2> Programming Languages and Programs</h2>
<ul>
<li> Python and the NLTK </li>
<li> Poppler Utils - specifically, pdftotext</li>
<li> Iconv - unix program for encodings</li>
</ul>
<h2>Visuals</h2>
<ul>
<li> Apple Keynote - For deadline reasons, I constructed all charts and graphs by hand in Keynote. I would have preferred to have a more interactive format of visualization, there are some merits to invoking a static form of visualization.</li>
<li> <a href="http://ethanschoonover.com/solarized">Solarized</a> - A notable consequence of constructing a work for consumption on screens is extended exposure to the work in its natural habitat. As a result, I strove to make this page as visually relaxing as possible. The colorscheme used, Solarized, was carefully designed to promote readability without eye fatigue. 
</ul>


    </section>


    <section id="printvscreen">
	<h1>The End</h1>
<p> Undertaking this sort of experiment was intended as a simple exercise in marrying the theory of our readings ("literature", "academia", etc.) to the practice ("digital humanities", "data mining", etc.). I felt a strong understanding of the coursework and its coverage before embarking on this task, but a number of lessons manifested regardless: </p>
<h2> 1. Clear plans are critical.</h2>
<p> My first iteration of this experiment was "Hop into the Python interpreter and see what happens!" This is a recipe for failure. While it is fun to play around with a corpus of texts you have read and analyzed to death with fellow classmates, this typically elucidates very little grand observations. The next few iterations built on research, inspiration from the readings themselves, and eventually grew into "what do simple text processing algorithms show us about texts themselves?" Even these open-ended questions are better for directing experimental analysis.</p>
<h2> 2. If you do not have a deep understanding of the content, one will be provided to you.</h2>
<p> Once the question is defined, however, its merits are tested at every step of the journey. Or, less abstractly, every small complication in a computational tool will make you question what question you are actually testing. For that reason, a very observational eye is necessary for effective experimentation. Less observant experimenter Amrita would have discarded her notes on processing out hex characters and worked with what she had; seasoned screen reader Amrita knew better and used it as a tool in analyzing how the texts were being processed during her program. </p>
<h2> 3. Yes, do judge the book by its cover.</h2>
<p> Even the shallowest reading of McLuhan will leave the reader with a strong impression of the merits of judging a book by its cover. Applying this school of thought to Franco Moretti, we see that writing can be conveyed in a number of ways, each providing more insight and visual interest than the last. Extending this under the research of Pasanek, we see that none of these works are devoid of influence in the same way a text cannot be devoid of influence. Consequently, appearances matter, and shape what your text is, how it is consumed, what the message is like. Most readers will scroll this site, see pretty colors, and smile at the cool charts. Some readers may swipe through this on their phone, and note how the colors are soothing to their eyes at night. Others may carefully read each section and wonder how their perspective or knowledge base may have changed the trajectory or results of my experiment. But all of these things are carefully crafted experiences by me, and the influence of the author in a digital work is something I have become acutely aware as a result of this text.</p>

<p> In conclusion, digital texts change how we read, but also provide a number of new forms by which to interact with a text. All these forms can influence our understanding of a work. The consequences of these forms are not limited to human understanding, however, as these differences can sometimes become more acute with an automated hand to help. By recruiting digital tools to help with humanities problems, we can isolate assumptions and identify new problems while also reflecting our understandings back on ourselves.
    </section>

    <section id="reflections">
	<h1>End Notes</h1>
<h2> Source files</h2>
<p> All code and generated files can be found at <a href="http://www.github.com/amritamaz/screenreading">the github repo</a>.
<h2> Source texts used in text analysis</h2>
<div id="bib">
<p><strong>January 23: Preliminaries</strong></p>
<ul>
<li>Sven Birkerts, <a href="http://lareviewofbooks.org/essay/the-room-and-the-elephant">&quot;The Room and the Elephant&quot;</a> (2011, web)</li>
<li>Jaron Lanier, <a href="https://github.com/gwijthoff/Screenreading/blob/master/readings/week1-preliminaries/Lanier_2010_You_Are_Not_a_Gadget_-_A_Manifesto2.pdf?raw=true">Preface to <em>You Are Not a Gadget</em></a> (2010, pdf)</li>
<li>Stephen Ramsay, <a href="http://www.playingwithhistory.com/wp-content/uploads/2010/04/hermeneutics.pdf">&quot;The Hermeneutics of Screwing Around; or, What You Do With a Million Books&quot;</a> (2010, pdf)</li>
<li>Craig Mod, <a href="http://craigmod.com/journal/ipad_and_books/">&quot;Books in the Age of the iPad&quot;</a> (2010, web)</li>
</ul>
<p><strong>January 30: The invention of writing coincides with the emergence of civilization.</strong></p>
<ul>
<li>Denise Schmandt-Besserat, <a href="https://courseworks.columbia.edu/access/content/group/ENGLW3986_001_2014_1/Schmandt-Besserat_1996_How_writing_came_about-CH1-5.pdf"><em>How Writing Came About</em></a> (1997, protected pdf), pp. 1-85</li>
<li>Lydia Liu, <a href="https://courseworks.columbia.edu/access/content/group/ENGLW3986_001_2014_1/Liu_2010_Writing.pdf">&quot;Writing&quot;</a>, in <em>Critical Terms for Media Studies</em> (2010, protected pdf)</li>
<li>Steven Roger Fischer, <a href="https://courseworks.columbia.edu/access/content/group/ENGLW3986_001_2014_1/Fischer_2003_A_History_of_Reading_CH1-2.pdf"><em>A History of Reading</em></a> (2003, protected pdf), pp. 11-98</li>
<li>Mara Mills, <a href="http://flowtv.org/2012/12/what-should-we-call-reading/">&quot;What Should We Call Reading?&quot;</a>, <em>Flow TV</em> (2012, web)</li>
<li>Barry Powell, <a href="https://courseworks.columbia.edu/access/content/group/ENGLW3986_001_2014_1/Powell_2012_Writing_-_theory_and_history_of_the_technology_of_civilization-intro-ch1.pdf"><em>Writing: History and Theory of the Technology of Civilization</em></a> (2012, protected pdf), pp. 1-18</li>
<li>Andr√© Leroi-Gourhan, <a href="https://courseworks.columbia.edu/access/content/group/ENGLW3986_001_2014_1/Leroi-Gourhan_1993_Gesture_and_Speech-CH3.pdf">&quot;The Language of Prehominids,&quot;</a> in <em>Gesture and Speech</em> (1965, protected pdf), p. 112-116</li>
</ul>
<p><strong>February 6: Western thought is the product of a paradigm shift from orality to literacy.</strong></p>
<ul>
<li>Walter Ong, <a href="https://courseworks.columbia.edu/access/content/group/ENGLW3986_001_2014_1/Ong_Hartley_2012_Orality_and_literacy_the_technologizing_of_the_word-CH4-6.pdf"><em>Orality and Literacy</em></a> (1982, protected pdf), pp. 77-152</li>
<li>Marshall McLuhan, <a href="https://courseworks.columbia.edu/access/content/group/ENGLW3986_001_2014_1/McLuhan_1964_Understanding_media_-_the_extensions_of_man-introCH1-8-9.pdf"><em>Understanding Media: The Extensions of Man</em></a> (1964, protected pdf), p. 3-21, 77-88</li>
<li>Friedrich Kittler, <a href="http://www.ctheory.net/articles.aspx?id=45">&quot;The History of Communication Media,&quot;</a> <em>CTHEORY</em> (July 1996, web)</li>
</ul>
<p><strong>February 13: Books process, record, and transmit data.</strong></p>
<ul>
<li>Lucien Febvre and Henri-Jean Martin, <a href="https://courseworks.columbia.edu/access/content/group/ENGLW3986_001_2014_1/Febvre_Wootton_1976_The_coming_of_the_book_-_the_impact_of_printing,_1450-1800_CH1-3.pdf"><em>The Coming of the Book: The Impact of Printing 1450-1800</em></a> (1976, protected pdf), p. 8-108</li>
</ul>
<p><strong>February 20: It took 200 years for libraries to shelve print &amp; manuscript as separate entities.</strong></p>
<ul>
<li>David McKitterick, <a href="https://courseworks.columbia.edu/access/content/group/ENGLW3986_001_2014_1/McKitterick_2003_Print,_manuscript,_and_the_search_for_order,_1450-1830-CH1-2.pdf"><em>Print, Manuscript, and the Search for Order, 1450-1830</em></a> (2003), p. 1-52</li>
<li>Adrian Johns, <a href="https://courseworks.columbia.edu/access/content/group/ENGLW3986_001_2014_1/Johns_1998_The_nature_of_the_book_-_print_and_knowledge_in_the_making-CH1_CH6.pdf"><em>The Nature of the Book: Print and Knowledge in the Making</em></a> (2000), pp. 1-57, 380-443</li>
</ul>
<p><strong>February 27: &quot;How greatly this page here resembles a thousand other pages.&quot;</strong></p>
<ul>
<li>Roger Chartier, <a href="https://courseworks.columbia.edu/access/content/group/ENGLW3986_001_2014_1/Chartier_1994_The_order_of_books_-_readers,_authors,_and_libraries_in_Europe_between_the.pdf"><em>The Order of Books: Readers, Authors, and Libraries in Europe between the Fourteenth and Eighteenth Centuries</em></a> (1992), 92pp</li>
<li>Roger Chartier, <a href="https://github.com/gwijthoff/Screenreading/blob/master/readings/week6%20-%20reproducibility/Chartier_1989_Texts%2C_Printings%2C_Readings.pdf/?raw=true">&quot;Texts, Printings, Readings,&quot;</a> in <em>The New Cultural History</em>, ed. Hunt (1989)</li>
<li>Anthony Grafton, <a href="https://courseworks.columbia.edu/access/content/group/ENGLW3986_001_2014_1/Grafton_2009_Codex_in_Crisis_-_The_Book_Dematerializes.pdf">&quot;Codex in Crisis: The Book Dematerializes&quot;</a>, in <em>Worlds Made By Words: Scholarship and Community in the Modern West</em> (2009), pp. 288-326.</li>
</ul>
<p><strong>March 6: &quot;They conveyed their thoughts to one another in an instant over cities or mountains.&quot;</strong></p>
<ul>
<li>Andrew Piper, <a href="https://courseworks.columbia.edu/access/content/group/ENGLW3986_001_2014_1/Piper_2009_Dreaming_in_books_-_the_making_of_the_bibliographic_imagination_in_the_Romantic.pdf"><em>Dreaming in Books: Making of the Bibliographic Imagination in the Romantic Age</em></a> (2009), pp. 1-18, 121-152, 235-245
</ul>
<p><strong>March 13:</strong></p>
<ul>
<li>Michel de Certeau, <a href="https://courseworks.columbia.edu/access/content/group/ENGLW3986_001_2014_1/de_Certeau_2011_Reading_as_Poaching.pdf">&quot;Reading as Poaching,&quot;</a> in <em>The Practice of Everyday Life</em> (1984), pp. 166-175</li>
</ul>
<p><strong>March 27: Texts cannot be said to have fixed properties; they are always in flux.</strong></p>
<ul>
<li>Jerome McGann, <a href="https://courseworks.columbia.edu/access/content/group/ENGLW3986_001_2014_1/McGann_2004_Radiant_textuality-intro-ch5-7.pdf"><em>Radiant Textuality: Literature After the World Wide Web</em></a> (2001), pp. 1-28, 137-166, 193-208</li>
</ul>
<p><strong>April 3: Media reconfigure the logic of the senses.</strong></p>
<ul>
<li>Anne Friedberg, <a href="http://quod.lib.umich.edu.ezproxy.cul.columbia.edu/cgi/t/text/text-idx?c=acls;idno=heb08244"><em>The Virtual Window: From Alberti to Microsoft</em></a> (2009)</li>
<li>Johanna Drucker, <a href="https://courseworks.columbia.edu/access/content/group/ENGLW3986_001_2014_1/Drucker_2013_Reading_Interface.pdf">&quot;Reading Interface,&quot;</a> <em>PMLA</em> (2013)</li>
</ul>
<p><strong>April 10: Multitasking threatens attention span and the ability to sustain complex thought.</strong></p>
<ul>
<li>Betsy Sparrow, Jenny Liu, and Daniel M. Wegner, <a href="https://courseworks.columbia.edu/access/content/group/ENGLW3986_001_2014_1/Sparrow_et_al_2011_Google_Effects_on_Memory_-_Cognitive_Consequences_of_Having_Information_at_Our2.pdf">&quot;Google Effects on Memory: Cognitive Consequences of Having Information at Our Fingertips,&quot;</a> <em>Science</em> (2011)</li>
<li>Alexis Madrigal, <a href="http://www.theatlantic.com/technology/archive/2012/03/books-on-paper-fight-analog-distractions/254049/">&quot;Books on Paper Fight Analog Distractions&quot;</a> (2012)</li>
<li>Nicholas Carr, <a href="http://www.theatlantic.com/magazine/archive/2008/07/is-google-making-us-stupid/306868/">&quot;Is Google Making Us Stupid?&quot;</a> (2008)</li>
<li>Naomi S. Baron, <a href="https://courseworks.columbia.edu/access/content/group/ENGLW3986_001_2014_1/Baron_2013_Redefining_Reading_-_The_Impact_of_Digital_Communication_Media.pdf">&quot;Redefining Reading: The Impact of Digital Communication Media&quot;</a> <em>PMLA</em> (2013)</li>
</ul>
<p><strong>April 17: Reading takes place within feedback loops between old and new technologies.</strong></p>
<ul>
<li>N. Katherine Hayles, <a href="https://courseworks.columbia.edu/access/content/group/ENGLW3986_001_2014_1/Hayles_2005_My_mother_was_a_computer_digital_subjects_and_literary_texts-prologue_ch4.pdf"><em>My Mother Was a Computer</em></a> (2005), pp. 1-14, 89-116</li>
<li>N. Katherine Hayles, <a href="http://nkhayles.com/how_we_read.html">&quot;How We Read: Close, Hyper, Machine&quot;</a> (2011, right click to save PDF)</li>
</ul>
<p><strong>April 24: There have been 129,864,880 books published, ever.</strong></p>
<ul>
<li>Franco Moretti, <a href="https://courseworks.columbia.edu/access/content/group/ENGLW3986_001_2014_1/Moretti_2007_Graphs%2C_maps%2C_trees_-_abstract_models_for_literary_history.pdf"><em>Graphs, Maps, Trees: Abstract Models for Literary History</em></a> (2007)</li>
<li>D. Sculley and Bradley Pasanek, <a href="https://courseworks.columbia.edu/access/content/group/ENGLW3986_001_2014_1/Sculley_Pasanek_2008_Meaning_and_mining_-_the_impact_of_implicit_assumptions_in_data_mining_for_the.pdf">&quot;Meaning and Mining: The Impact of Implicit Assumptions in Data Mining for the Humanities,&quot;</a> <em>Literary and Linguistic Computing</em> (2008)</li>
<li>Matthew K. Gold, ed., <a href="http://dhdebates.gc.cuny.edu/debates"><em>Debates in the Digital Humanities</em></a> (2012)</li>
<li>Jonathan Goodwin and John Holbo, <a href="https://github.com/gwijthoff/Screenreading/blob/master/readings/week13-machine_reading/Goodwin_et_al_2011_Reading_Graphs,_Maps,_Trees_-_Critical_Responses_to_Franco_Moretti.pdf?raw=true"><em>Reading Graphs, Maps, Trees: Critical Responses to Franco Moretti</em></a> (2011)</li>
</ul>
</div>
    </section>
</body>
</html>
